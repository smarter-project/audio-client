// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: model_config.proto

#ifndef PROTOBUF_INCLUDED_model_5fconfig_2eproto
#define PROTOBUF_INCLUDED_model_5fconfig_2eproto

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3006001
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3006001 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/inlined_string_field.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/map.h>  // IWYU pragma: export
#include <google/protobuf/map_entry.h>
#include <google/protobuf/map_field_inl.h>
#include <google/protobuf/generated_enum_reflection.h>
#include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
#define PROTOBUF_INTERNAL_EXPORT_protobuf_model_5fconfig_2eproto 

namespace protobuf_model_5fconfig_2eproto {
// Internal implementation detail -- do not use these members.
struct TableStruct {
  static const ::google::protobuf::internal::ParseTableField entries[];
  static const ::google::protobuf::internal::AuxillaryParseTableField aux[];
  static const ::google::protobuf::internal::ParseTable schema[35];
  static const ::google::protobuf::internal::FieldMetadata field_metadata[];
  static const ::google::protobuf::internal::SerializationTable serialization_table[];
  static const ::google::protobuf::uint32 offsets[];
};
void AddDescriptors();
}  // namespace protobuf_model_5fconfig_2eproto
namespace nvidia {
namespace inferenceserver {
class ModelConfig;
class ModelConfigDefaultTypeInternal;
extern ModelConfigDefaultTypeInternal _ModelConfig_default_instance_;
class ModelConfig_CcModelFilenamesEntry_DoNotUse;
class ModelConfig_CcModelFilenamesEntry_DoNotUseDefaultTypeInternal;
extern ModelConfig_CcModelFilenamesEntry_DoNotUseDefaultTypeInternal _ModelConfig_CcModelFilenamesEntry_DoNotUse_default_instance_;
class ModelConfig_MetricTagsEntry_DoNotUse;
class ModelConfig_MetricTagsEntry_DoNotUseDefaultTypeInternal;
extern ModelConfig_MetricTagsEntry_DoNotUseDefaultTypeInternal _ModelConfig_MetricTagsEntry_DoNotUse_default_instance_;
class ModelConfig_ParametersEntry_DoNotUse;
class ModelConfig_ParametersEntry_DoNotUseDefaultTypeInternal;
extern ModelConfig_ParametersEntry_DoNotUseDefaultTypeInternal _ModelConfig_ParametersEntry_DoNotUse_default_instance_;
class ModelDynamicBatching;
class ModelDynamicBatchingDefaultTypeInternal;
extern ModelDynamicBatchingDefaultTypeInternal _ModelDynamicBatching_default_instance_;
class ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse;
class ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUseDefaultTypeInternal;
extern ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUseDefaultTypeInternal _ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse_default_instance_;
class ModelEnsembling;
class ModelEnsemblingDefaultTypeInternal;
extern ModelEnsemblingDefaultTypeInternal _ModelEnsembling_default_instance_;
class ModelEnsembling_Step;
class ModelEnsembling_StepDefaultTypeInternal;
extern ModelEnsembling_StepDefaultTypeInternal _ModelEnsembling_Step_default_instance_;
class ModelEnsembling_Step_InputMapEntry_DoNotUse;
class ModelEnsembling_Step_InputMapEntry_DoNotUseDefaultTypeInternal;
extern ModelEnsembling_Step_InputMapEntry_DoNotUseDefaultTypeInternal _ModelEnsembling_Step_InputMapEntry_DoNotUse_default_instance_;
class ModelEnsembling_Step_OutputMapEntry_DoNotUse;
class ModelEnsembling_Step_OutputMapEntry_DoNotUseDefaultTypeInternal;
extern ModelEnsembling_Step_OutputMapEntry_DoNotUseDefaultTypeInternal _ModelEnsembling_Step_OutputMapEntry_DoNotUse_default_instance_;
class ModelInput;
class ModelInputDefaultTypeInternal;
extern ModelInputDefaultTypeInternal _ModelInput_default_instance_;
class ModelInstanceGroup;
class ModelInstanceGroupDefaultTypeInternal;
extern ModelInstanceGroupDefaultTypeInternal _ModelInstanceGroup_default_instance_;
class ModelOptimizationPolicy;
class ModelOptimizationPolicyDefaultTypeInternal;
extern ModelOptimizationPolicyDefaultTypeInternal _ModelOptimizationPolicy_default_instance_;
class ModelOptimizationPolicy_Cuda;
class ModelOptimizationPolicy_CudaDefaultTypeInternal;
extern ModelOptimizationPolicy_CudaDefaultTypeInternal _ModelOptimizationPolicy_Cuda_default_instance_;
class ModelOptimizationPolicy_ExecutionAccelerators;
class ModelOptimizationPolicy_ExecutionAcceleratorsDefaultTypeInternal;
extern ModelOptimizationPolicy_ExecutionAcceleratorsDefaultTypeInternal _ModelOptimizationPolicy_ExecutionAccelerators_default_instance_;
class ModelOptimizationPolicy_ExecutionAccelerators_Accelerator;
class ModelOptimizationPolicy_ExecutionAccelerators_AcceleratorDefaultTypeInternal;
extern ModelOptimizationPolicy_ExecutionAccelerators_AcceleratorDefaultTypeInternal _ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_default_instance_;
class ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse;
class ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUseDefaultTypeInternal;
extern ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUseDefaultTypeInternal _ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse_default_instance_;
class ModelOptimizationPolicy_Graph;
class ModelOptimizationPolicy_GraphDefaultTypeInternal;
extern ModelOptimizationPolicy_GraphDefaultTypeInternal _ModelOptimizationPolicy_Graph_default_instance_;
class ModelOptimizationPolicy_PinnedMemoryBuffer;
class ModelOptimizationPolicy_PinnedMemoryBufferDefaultTypeInternal;
extern ModelOptimizationPolicy_PinnedMemoryBufferDefaultTypeInternal _ModelOptimizationPolicy_PinnedMemoryBuffer_default_instance_;
class ModelOutput;
class ModelOutputDefaultTypeInternal;
extern ModelOutputDefaultTypeInternal _ModelOutput_default_instance_;
class ModelParameter;
class ModelParameterDefaultTypeInternal;
extern ModelParameterDefaultTypeInternal _ModelParameter_default_instance_;
class ModelQueuePolicy;
class ModelQueuePolicyDefaultTypeInternal;
extern ModelQueuePolicyDefaultTypeInternal _ModelQueuePolicy_default_instance_;
class ModelSequenceBatching;
class ModelSequenceBatchingDefaultTypeInternal;
extern ModelSequenceBatchingDefaultTypeInternal _ModelSequenceBatching_default_instance_;
class ModelSequenceBatching_Control;
class ModelSequenceBatching_ControlDefaultTypeInternal;
extern ModelSequenceBatching_ControlDefaultTypeInternal _ModelSequenceBatching_Control_default_instance_;
class ModelSequenceBatching_ControlInput;
class ModelSequenceBatching_ControlInputDefaultTypeInternal;
extern ModelSequenceBatching_ControlInputDefaultTypeInternal _ModelSequenceBatching_ControlInput_default_instance_;
class ModelSequenceBatching_StrategyDirect;
class ModelSequenceBatching_StrategyDirectDefaultTypeInternal;
extern ModelSequenceBatching_StrategyDirectDefaultTypeInternal _ModelSequenceBatching_StrategyDirect_default_instance_;
class ModelSequenceBatching_StrategyOldest;
class ModelSequenceBatching_StrategyOldestDefaultTypeInternal;
extern ModelSequenceBatching_StrategyOldestDefaultTypeInternal _ModelSequenceBatching_StrategyOldest_default_instance_;
class ModelTensorReshape;
class ModelTensorReshapeDefaultTypeInternal;
extern ModelTensorReshapeDefaultTypeInternal _ModelTensorReshape_default_instance_;
class ModelVersionPolicy;
class ModelVersionPolicyDefaultTypeInternal;
extern ModelVersionPolicyDefaultTypeInternal _ModelVersionPolicy_default_instance_;
class ModelVersionPolicy_All;
class ModelVersionPolicy_AllDefaultTypeInternal;
extern ModelVersionPolicy_AllDefaultTypeInternal _ModelVersionPolicy_All_default_instance_;
class ModelVersionPolicy_Latest;
class ModelVersionPolicy_LatestDefaultTypeInternal;
extern ModelVersionPolicy_LatestDefaultTypeInternal _ModelVersionPolicy_Latest_default_instance_;
class ModelVersionPolicy_Specific;
class ModelVersionPolicy_SpecificDefaultTypeInternal;
extern ModelVersionPolicy_SpecificDefaultTypeInternal _ModelVersionPolicy_Specific_default_instance_;
class ModelWarmup;
class ModelWarmupDefaultTypeInternal;
extern ModelWarmupDefaultTypeInternal _ModelWarmup_default_instance_;
class ModelWarmup_Input;
class ModelWarmup_InputDefaultTypeInternal;
extern ModelWarmup_InputDefaultTypeInternal _ModelWarmup_Input_default_instance_;
class ModelWarmup_InputsEntry_DoNotUse;
class ModelWarmup_InputsEntry_DoNotUseDefaultTypeInternal;
extern ModelWarmup_InputsEntry_DoNotUseDefaultTypeInternal _ModelWarmup_InputsEntry_DoNotUse_default_instance_;
}  // namespace inferenceserver
}  // namespace nvidia
namespace google {
namespace protobuf {
template<> ::nvidia::inferenceserver::ModelConfig* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig>(Arena*);
template<> ::nvidia::inferenceserver::ModelConfig_CcModelFilenamesEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig_CcModelFilenamesEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelConfig_MetricTagsEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig_MetricTagsEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelConfig_ParametersEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig_ParametersEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelDynamicBatching* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelDynamicBatching>(Arena*);
template<> ::nvidia::inferenceserver::ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelEnsembling* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelEnsembling>(Arena*);
template<> ::nvidia::inferenceserver::ModelEnsembling_Step* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelEnsembling_Step>(Arena*);
template<> ::nvidia::inferenceserver::ModelEnsembling_Step_InputMapEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelEnsembling_Step_InputMapEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelEnsembling_Step_OutputMapEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelEnsembling_Step_OutputMapEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelInput* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelInput>(Arena*);
template<> ::nvidia::inferenceserver::ModelInstanceGroup* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelInstanceGroup>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_Graph>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer>(Arena*);
template<> ::nvidia::inferenceserver::ModelOutput* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOutput>(Arena*);
template<> ::nvidia::inferenceserver::ModelParameter* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelParameter>(Arena*);
template<> ::nvidia::inferenceserver::ModelQueuePolicy* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelQueuePolicy>(Arena*);
template<> ::nvidia::inferenceserver::ModelSequenceBatching* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelSequenceBatching>(Arena*);
template<> ::nvidia::inferenceserver::ModelSequenceBatching_Control* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelSequenceBatching_Control>(Arena*);
template<> ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelSequenceBatching_ControlInput>(Arena*);
template<> ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect>(Arena*);
template<> ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest>(Arena*);
template<> ::nvidia::inferenceserver::ModelTensorReshape* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelTensorReshape>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy_All* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy_All>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy_Latest* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy_Latest>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy_Specific* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy_Specific>(Arena*);
template<> ::nvidia::inferenceserver::ModelWarmup* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelWarmup>(Arena*);
template<> ::nvidia::inferenceserver::ModelWarmup_Input* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelWarmup_Input>(Arena*);
template<> ::nvidia::inferenceserver::ModelWarmup_InputsEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelWarmup_InputsEntry_DoNotUse>(Arena*);
}  // namespace protobuf
}  // namespace google
namespace nvidia {
namespace inferenceserver {

enum ModelInstanceGroup_Kind {
  ModelInstanceGroup_Kind_KIND_AUTO = 0,
  ModelInstanceGroup_Kind_KIND_GPU = 1,
  ModelInstanceGroup_Kind_KIND_CPU = 2,
  ModelInstanceGroup_Kind_KIND_MODEL = 3,
  ModelInstanceGroup_Kind_ModelInstanceGroup_Kind_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ModelInstanceGroup_Kind_ModelInstanceGroup_Kind_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ModelInstanceGroup_Kind_IsValid(int value);
const ModelInstanceGroup_Kind ModelInstanceGroup_Kind_Kind_MIN = ModelInstanceGroup_Kind_KIND_AUTO;
const ModelInstanceGroup_Kind ModelInstanceGroup_Kind_Kind_MAX = ModelInstanceGroup_Kind_KIND_MODEL;
const int ModelInstanceGroup_Kind_Kind_ARRAYSIZE = ModelInstanceGroup_Kind_Kind_MAX + 1;

const ::google::protobuf::EnumDescriptor* ModelInstanceGroup_Kind_descriptor();
inline const ::std::string& ModelInstanceGroup_Kind_Name(ModelInstanceGroup_Kind value) {
  return ::google::protobuf::internal::NameOfEnum(
    ModelInstanceGroup_Kind_descriptor(), value);
}
inline bool ModelInstanceGroup_Kind_Parse(
    const ::std::string& name, ModelInstanceGroup_Kind* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ModelInstanceGroup_Kind>(
    ModelInstanceGroup_Kind_descriptor(), name, value);
}
enum ModelInput_Format {
  ModelInput_Format_FORMAT_NONE = 0,
  ModelInput_Format_FORMAT_NHWC = 1,
  ModelInput_Format_FORMAT_NCHW = 2,
  ModelInput_Format_ModelInput_Format_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ModelInput_Format_ModelInput_Format_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ModelInput_Format_IsValid(int value);
const ModelInput_Format ModelInput_Format_Format_MIN = ModelInput_Format_FORMAT_NONE;
const ModelInput_Format ModelInput_Format_Format_MAX = ModelInput_Format_FORMAT_NCHW;
const int ModelInput_Format_Format_ARRAYSIZE = ModelInput_Format_Format_MAX + 1;

const ::google::protobuf::EnumDescriptor* ModelInput_Format_descriptor();
inline const ::std::string& ModelInput_Format_Name(ModelInput_Format value) {
  return ::google::protobuf::internal::NameOfEnum(
    ModelInput_Format_descriptor(), value);
}
inline bool ModelInput_Format_Parse(
    const ::std::string& name, ModelInput_Format* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ModelInput_Format>(
    ModelInput_Format_descriptor(), name, value);
}
enum ModelOptimizationPolicy_ModelPriority {
  ModelOptimizationPolicy_ModelPriority_PRIORITY_DEFAULT = 0,
  ModelOptimizationPolicy_ModelPriority_PRIORITY_MAX = 1,
  ModelOptimizationPolicy_ModelPriority_PRIORITY_MIN = 2,
  ModelOptimizationPolicy_ModelPriority_ModelOptimizationPolicy_ModelPriority_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ModelOptimizationPolicy_ModelPriority_ModelOptimizationPolicy_ModelPriority_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ModelOptimizationPolicy_ModelPriority_IsValid(int value);
const ModelOptimizationPolicy_ModelPriority ModelOptimizationPolicy_ModelPriority_ModelPriority_MIN = ModelOptimizationPolicy_ModelPriority_PRIORITY_DEFAULT;
const ModelOptimizationPolicy_ModelPriority ModelOptimizationPolicy_ModelPriority_ModelPriority_MAX = ModelOptimizationPolicy_ModelPriority_PRIORITY_MIN;
const int ModelOptimizationPolicy_ModelPriority_ModelPriority_ARRAYSIZE = ModelOptimizationPolicy_ModelPriority_ModelPriority_MAX + 1;

const ::google::protobuf::EnumDescriptor* ModelOptimizationPolicy_ModelPriority_descriptor();
inline const ::std::string& ModelOptimizationPolicy_ModelPriority_Name(ModelOptimizationPolicy_ModelPriority value) {
  return ::google::protobuf::internal::NameOfEnum(
    ModelOptimizationPolicy_ModelPriority_descriptor(), value);
}
inline bool ModelOptimizationPolicy_ModelPriority_Parse(
    const ::std::string& name, ModelOptimizationPolicy_ModelPriority* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ModelOptimizationPolicy_ModelPriority>(
    ModelOptimizationPolicy_ModelPriority_descriptor(), name, value);
}
enum ModelQueuePolicy_TimeoutAction {
  ModelQueuePolicy_TimeoutAction_REJECT = 0,
  ModelQueuePolicy_TimeoutAction_DELAY = 1,
  ModelQueuePolicy_TimeoutAction_ModelQueuePolicy_TimeoutAction_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ModelQueuePolicy_TimeoutAction_ModelQueuePolicy_TimeoutAction_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ModelQueuePolicy_TimeoutAction_IsValid(int value);
const ModelQueuePolicy_TimeoutAction ModelQueuePolicy_TimeoutAction_TimeoutAction_MIN = ModelQueuePolicy_TimeoutAction_REJECT;
const ModelQueuePolicy_TimeoutAction ModelQueuePolicy_TimeoutAction_TimeoutAction_MAX = ModelQueuePolicy_TimeoutAction_DELAY;
const int ModelQueuePolicy_TimeoutAction_TimeoutAction_ARRAYSIZE = ModelQueuePolicy_TimeoutAction_TimeoutAction_MAX + 1;

const ::google::protobuf::EnumDescriptor* ModelQueuePolicy_TimeoutAction_descriptor();
inline const ::std::string& ModelQueuePolicy_TimeoutAction_Name(ModelQueuePolicy_TimeoutAction value) {
  return ::google::protobuf::internal::NameOfEnum(
    ModelQueuePolicy_TimeoutAction_descriptor(), value);
}
inline bool ModelQueuePolicy_TimeoutAction_Parse(
    const ::std::string& name, ModelQueuePolicy_TimeoutAction* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ModelQueuePolicy_TimeoutAction>(
    ModelQueuePolicy_TimeoutAction_descriptor(), name, value);
}
enum ModelSequenceBatching_Control_Kind {
  ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_START = 0,
  ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_READY = 1,
  ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_END = 2,
  ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_CORRID = 3,
  ModelSequenceBatching_Control_Kind_ModelSequenceBatching_Control_Kind_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ModelSequenceBatching_Control_Kind_ModelSequenceBatching_Control_Kind_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ModelSequenceBatching_Control_Kind_IsValid(int value);
const ModelSequenceBatching_Control_Kind ModelSequenceBatching_Control_Kind_Kind_MIN = ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_START;
const ModelSequenceBatching_Control_Kind ModelSequenceBatching_Control_Kind_Kind_MAX = ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_CORRID;
const int ModelSequenceBatching_Control_Kind_Kind_ARRAYSIZE = ModelSequenceBatching_Control_Kind_Kind_MAX + 1;

const ::google::protobuf::EnumDescriptor* ModelSequenceBatching_Control_Kind_descriptor();
inline const ::std::string& ModelSequenceBatching_Control_Kind_Name(ModelSequenceBatching_Control_Kind value) {
  return ::google::protobuf::internal::NameOfEnum(
    ModelSequenceBatching_Control_Kind_descriptor(), value);
}
inline bool ModelSequenceBatching_Control_Kind_Parse(
    const ::std::string& name, ModelSequenceBatching_Control_Kind* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ModelSequenceBatching_Control_Kind>(
    ModelSequenceBatching_Control_Kind_descriptor(), name, value);
}
enum DataType {
  TYPE_INVALID = 0,
  TYPE_BOOL = 1,
  TYPE_UINT8 = 2,
  TYPE_UINT16 = 3,
  TYPE_UINT32 = 4,
  TYPE_UINT64 = 5,
  TYPE_INT8 = 6,
  TYPE_INT16 = 7,
  TYPE_INT32 = 8,
  TYPE_INT64 = 9,
  TYPE_FP16 = 10,
  TYPE_FP32 = 11,
  TYPE_FP64 = 12,
  TYPE_STRING = 13,
  DataType_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  DataType_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool DataType_IsValid(int value);
const DataType DataType_MIN = TYPE_INVALID;
const DataType DataType_MAX = TYPE_STRING;
const int DataType_ARRAYSIZE = DataType_MAX + 1;

const ::google::protobuf::EnumDescriptor* DataType_descriptor();
inline const ::std::string& DataType_Name(DataType value) {
  return ::google::protobuf::internal::NameOfEnum(
    DataType_descriptor(), value);
}
inline bool DataType_Parse(
    const ::std::string& name, DataType* value) {
  return ::google::protobuf::internal::ParseNamedEnum<DataType>(
    DataType_descriptor(), name, value);
}
// ===================================================================

class ModelInstanceGroup : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelInstanceGroup) */ {
 public:
  ModelInstanceGroup();
  virtual ~ModelInstanceGroup();

  ModelInstanceGroup(const ModelInstanceGroup& from);

  inline ModelInstanceGroup& operator=(const ModelInstanceGroup& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelInstanceGroup(ModelInstanceGroup&& from) noexcept
    : ModelInstanceGroup() {
    *this = ::std::move(from);
  }

  inline ModelInstanceGroup& operator=(ModelInstanceGroup&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelInstanceGroup& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelInstanceGroup* internal_default_instance() {
    return reinterpret_cast<const ModelInstanceGroup*>(
               &_ModelInstanceGroup_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  void Swap(ModelInstanceGroup* other);
  friend void swap(ModelInstanceGroup& a, ModelInstanceGroup& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelInstanceGroup* New() const final {
    return CreateMaybeMessage<ModelInstanceGroup>(NULL);
  }

  ModelInstanceGroup* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelInstanceGroup>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelInstanceGroup& from);
  void MergeFrom(const ModelInstanceGroup& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelInstanceGroup* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelInstanceGroup_Kind Kind;
  static const Kind KIND_AUTO =
    ModelInstanceGroup_Kind_KIND_AUTO;
  static const Kind KIND_GPU =
    ModelInstanceGroup_Kind_KIND_GPU;
  static const Kind KIND_CPU =
    ModelInstanceGroup_Kind_KIND_CPU;
  static const Kind KIND_MODEL =
    ModelInstanceGroup_Kind_KIND_MODEL;
  static inline bool Kind_IsValid(int value) {
    return ModelInstanceGroup_Kind_IsValid(value);
  }
  static const Kind Kind_MIN =
    ModelInstanceGroup_Kind_Kind_MIN;
  static const Kind Kind_MAX =
    ModelInstanceGroup_Kind_Kind_MAX;
  static const int Kind_ARRAYSIZE =
    ModelInstanceGroup_Kind_Kind_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Kind_descriptor() {
    return ModelInstanceGroup_Kind_descriptor();
  }
  static inline const ::std::string& Kind_Name(Kind value) {
    return ModelInstanceGroup_Kind_Name(value);
  }
  static inline bool Kind_Parse(const ::std::string& name,
      Kind* value) {
    return ModelInstanceGroup_Kind_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // repeated int32 gpus = 3;
  int gpus_size() const;
  void clear_gpus();
  static const int kGpusFieldNumber = 3;
  ::google::protobuf::int32 gpus(int index) const;
  void set_gpus(int index, ::google::protobuf::int32 value);
  void add_gpus(::google::protobuf::int32 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
      gpus() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
      mutable_gpus();

  // repeated string profile = 5;
  int profile_size() const;
  void clear_profile();
  static const int kProfileFieldNumber = 5;
  const ::std::string& profile(int index) const;
  ::std::string* mutable_profile(int index);
  void set_profile(int index, const ::std::string& value);
  #if LANG_CXX11
  void set_profile(int index, ::std::string&& value);
  #endif
  void set_profile(int index, const char* value);
  void set_profile(int index, const char* value, size_t size);
  ::std::string* add_profile();
  void add_profile(const ::std::string& value);
  #if LANG_CXX11
  void add_profile(::std::string&& value);
  #endif
  void add_profile(const char* value);
  void add_profile(const char* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& profile() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_profile();

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // int32 count = 2;
  void clear_count();
  static const int kCountFieldNumber = 2;
  ::google::protobuf::int32 count() const;
  void set_count(::google::protobuf::int32 value);

  // .nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;
  void clear_kind();
  static const int kKindFieldNumber = 4;
  ::nvidia::inferenceserver::ModelInstanceGroup_Kind kind() const;
  void set_kind(::nvidia::inferenceserver::ModelInstanceGroup_Kind value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInstanceGroup)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 > gpus_;
  mutable int _gpus_cached_byte_size_;
  ::google::protobuf::RepeatedPtrField< ::std::string> profile_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  ::google::protobuf::int32 count_;
  int kind_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelTensorReshape : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelTensorReshape) */ {
 public:
  ModelTensorReshape();
  virtual ~ModelTensorReshape();

  ModelTensorReshape(const ModelTensorReshape& from);

  inline ModelTensorReshape& operator=(const ModelTensorReshape& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelTensorReshape(ModelTensorReshape&& from) noexcept
    : ModelTensorReshape() {
    *this = ::std::move(from);
  }

  inline ModelTensorReshape& operator=(ModelTensorReshape&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelTensorReshape& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelTensorReshape* internal_default_instance() {
    return reinterpret_cast<const ModelTensorReshape*>(
               &_ModelTensorReshape_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  void Swap(ModelTensorReshape* other);
  friend void swap(ModelTensorReshape& a, ModelTensorReshape& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelTensorReshape* New() const final {
    return CreateMaybeMessage<ModelTensorReshape>(NULL);
  }

  ModelTensorReshape* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelTensorReshape>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelTensorReshape& from);
  void MergeFrom(const ModelTensorReshape& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelTensorReshape* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 shape = 1;
  int shape_size() const;
  void clear_shape();
  static const int kShapeFieldNumber = 1;
  ::google::protobuf::int64 shape(int index) const;
  void set_shape(int index, ::google::protobuf::int64 value);
  void add_shape(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      shape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_shape();

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelTensorReshape)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > shape_;
  mutable int _shape_cached_byte_size_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelInput : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelInput) */ {
 public:
  ModelInput();
  virtual ~ModelInput();

  ModelInput(const ModelInput& from);

  inline ModelInput& operator=(const ModelInput& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelInput(ModelInput&& from) noexcept
    : ModelInput() {
    *this = ::std::move(from);
  }

  inline ModelInput& operator=(ModelInput&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelInput& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelInput* internal_default_instance() {
    return reinterpret_cast<const ModelInput*>(
               &_ModelInput_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  void Swap(ModelInput* other);
  friend void swap(ModelInput& a, ModelInput& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelInput* New() const final {
    return CreateMaybeMessage<ModelInput>(NULL);
  }

  ModelInput* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelInput>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelInput& from);
  void MergeFrom(const ModelInput& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelInput* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelInput_Format Format;
  static const Format FORMAT_NONE =
    ModelInput_Format_FORMAT_NONE;
  static const Format FORMAT_NHWC =
    ModelInput_Format_FORMAT_NHWC;
  static const Format FORMAT_NCHW =
    ModelInput_Format_FORMAT_NCHW;
  static inline bool Format_IsValid(int value) {
    return ModelInput_Format_IsValid(value);
  }
  static const Format Format_MIN =
    ModelInput_Format_Format_MIN;
  static const Format Format_MAX =
    ModelInput_Format_Format_MAX;
  static const int Format_ARRAYSIZE =
    ModelInput_Format_Format_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Format_descriptor() {
    return ModelInput_Format_descriptor();
  }
  static inline const ::std::string& Format_Name(Format value) {
    return ModelInput_Format_Name(value);
  }
  static inline bool Format_Parse(const ::std::string& name,
      Format* value) {
    return ModelInput_Format_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // repeated int64 dims = 4;
  int dims_size() const;
  void clear_dims();
  static const int kDimsFieldNumber = 4;
  ::google::protobuf::int64 dims(int index) const;
  void set_dims(int index, ::google::protobuf::int64 value);
  void add_dims(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      dims() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_dims();

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // .nvidia.inferenceserver.ModelTensorReshape reshape = 5;
  bool has_reshape() const;
  void clear_reshape();
  static const int kReshapeFieldNumber = 5;
  private:
  const ::nvidia::inferenceserver::ModelTensorReshape& _internal_reshape() const;
  public:
  const ::nvidia::inferenceserver::ModelTensorReshape& reshape() const;
  ::nvidia::inferenceserver::ModelTensorReshape* release_reshape();
  ::nvidia::inferenceserver::ModelTensorReshape* mutable_reshape();
  void set_allocated_reshape(::nvidia::inferenceserver::ModelTensorReshape* reshape);

  // .nvidia.inferenceserver.DataType data_type = 2;
  void clear_data_type();
  static const int kDataTypeFieldNumber = 2;
  ::nvidia::inferenceserver::DataType data_type() const;
  void set_data_type(::nvidia::inferenceserver::DataType value);

  // .nvidia.inferenceserver.ModelInput.Format format = 3;
  void clear_format();
  static const int kFormatFieldNumber = 3;
  ::nvidia::inferenceserver::ModelInput_Format format() const;
  void set_format(::nvidia::inferenceserver::ModelInput_Format value);

  // bool is_shape_tensor = 6;
  void clear_is_shape_tensor();
  static const int kIsShapeTensorFieldNumber = 6;
  bool is_shape_tensor() const;
  void set_is_shape_tensor(bool value);

  // bool allow_ragged_batch = 7;
  void clear_allow_ragged_batch();
  static const int kAllowRaggedBatchFieldNumber = 7;
  bool allow_ragged_batch() const;
  void set_allow_ragged_batch(bool value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInput)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > dims_;
  mutable int _dims_cached_byte_size_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  ::nvidia::inferenceserver::ModelTensorReshape* reshape_;
  int data_type_;
  int format_;
  bool is_shape_tensor_;
  bool allow_ragged_batch_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelOutput : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOutput) */ {
 public:
  ModelOutput();
  virtual ~ModelOutput();

  ModelOutput(const ModelOutput& from);

  inline ModelOutput& operator=(const ModelOutput& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelOutput(ModelOutput&& from) noexcept
    : ModelOutput() {
    *this = ::std::move(from);
  }

  inline ModelOutput& operator=(ModelOutput&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelOutput& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelOutput* internal_default_instance() {
    return reinterpret_cast<const ModelOutput*>(
               &_ModelOutput_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  void Swap(ModelOutput* other);
  friend void swap(ModelOutput& a, ModelOutput& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelOutput* New() const final {
    return CreateMaybeMessage<ModelOutput>(NULL);
  }

  ModelOutput* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelOutput>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelOutput& from);
  void MergeFrom(const ModelOutput& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOutput* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 dims = 3;
  int dims_size() const;
  void clear_dims();
  static const int kDimsFieldNumber = 3;
  ::google::protobuf::int64 dims(int index) const;
  void set_dims(int index, ::google::protobuf::int64 value);
  void add_dims(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      dims() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_dims();

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // string label_filename = 4;
  void clear_label_filename();
  static const int kLabelFilenameFieldNumber = 4;
  const ::std::string& label_filename() const;
  void set_label_filename(const ::std::string& value);
  #if LANG_CXX11
  void set_label_filename(::std::string&& value);
  #endif
  void set_label_filename(const char* value);
  void set_label_filename(const char* value, size_t size);
  ::std::string* mutable_label_filename();
  ::std::string* release_label_filename();
  void set_allocated_label_filename(::std::string* label_filename);

  // .nvidia.inferenceserver.ModelTensorReshape reshape = 5;
  bool has_reshape() const;
  void clear_reshape();
  static const int kReshapeFieldNumber = 5;
  private:
  const ::nvidia::inferenceserver::ModelTensorReshape& _internal_reshape() const;
  public:
  const ::nvidia::inferenceserver::ModelTensorReshape& reshape() const;
  ::nvidia::inferenceserver::ModelTensorReshape* release_reshape();
  ::nvidia::inferenceserver::ModelTensorReshape* mutable_reshape();
  void set_allocated_reshape(::nvidia::inferenceserver::ModelTensorReshape* reshape);

  // .nvidia.inferenceserver.DataType data_type = 2;
  void clear_data_type();
  static const int kDataTypeFieldNumber = 2;
  ::nvidia::inferenceserver::DataType data_type() const;
  void set_data_type(::nvidia::inferenceserver::DataType value);

  // bool is_shape_tensor = 6;
  void clear_is_shape_tensor();
  static const int kIsShapeTensorFieldNumber = 6;
  bool is_shape_tensor() const;
  void set_is_shape_tensor(bool value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOutput)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > dims_;
  mutable int _dims_cached_byte_size_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  ::google::protobuf::internal::ArenaStringPtr label_filename_;
  ::nvidia::inferenceserver::ModelTensorReshape* reshape_;
  int data_type_;
  bool is_shape_tensor_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelVersionPolicy_Latest : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy.Latest) */ {
 public:
  ModelVersionPolicy_Latest();
  virtual ~ModelVersionPolicy_Latest();

  ModelVersionPolicy_Latest(const ModelVersionPolicy_Latest& from);

  inline ModelVersionPolicy_Latest& operator=(const ModelVersionPolicy_Latest& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelVersionPolicy_Latest(ModelVersionPolicy_Latest&& from) noexcept
    : ModelVersionPolicy_Latest() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy_Latest& operator=(ModelVersionPolicy_Latest&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelVersionPolicy_Latest& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelVersionPolicy_Latest* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy_Latest*>(
               &_ModelVersionPolicy_Latest_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  void Swap(ModelVersionPolicy_Latest* other);
  friend void swap(ModelVersionPolicy_Latest& a, ModelVersionPolicy_Latest& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelVersionPolicy_Latest* New() const final {
    return CreateMaybeMessage<ModelVersionPolicy_Latest>(NULL);
  }

  ModelVersionPolicy_Latest* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelVersionPolicy_Latest>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelVersionPolicy_Latest& from);
  void MergeFrom(const ModelVersionPolicy_Latest& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionPolicy_Latest* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // uint32 num_versions = 1;
  void clear_num_versions();
  static const int kNumVersionsFieldNumber = 1;
  ::google::protobuf::uint32 num_versions() const;
  void set_num_versions(::google::protobuf::uint32 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::uint32 num_versions_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelVersionPolicy_All : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy.All) */ {
 public:
  ModelVersionPolicy_All();
  virtual ~ModelVersionPolicy_All();

  ModelVersionPolicy_All(const ModelVersionPolicy_All& from);

  inline ModelVersionPolicy_All& operator=(const ModelVersionPolicy_All& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelVersionPolicy_All(ModelVersionPolicy_All&& from) noexcept
    : ModelVersionPolicy_All() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy_All& operator=(ModelVersionPolicy_All&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelVersionPolicy_All& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelVersionPolicy_All* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy_All*>(
               &_ModelVersionPolicy_All_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  void Swap(ModelVersionPolicy_All* other);
  friend void swap(ModelVersionPolicy_All& a, ModelVersionPolicy_All& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelVersionPolicy_All* New() const final {
    return CreateMaybeMessage<ModelVersionPolicy_All>(NULL);
  }

  ModelVersionPolicy_All* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelVersionPolicy_All>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelVersionPolicy_All& from);
  void MergeFrom(const ModelVersionPolicy_All& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionPolicy_All* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelVersionPolicy_Specific : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy.Specific) */ {
 public:
  ModelVersionPolicy_Specific();
  virtual ~ModelVersionPolicy_Specific();

  ModelVersionPolicy_Specific(const ModelVersionPolicy_Specific& from);

  inline ModelVersionPolicy_Specific& operator=(const ModelVersionPolicy_Specific& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelVersionPolicy_Specific(ModelVersionPolicy_Specific&& from) noexcept
    : ModelVersionPolicy_Specific() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy_Specific& operator=(ModelVersionPolicy_Specific&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelVersionPolicy_Specific& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelVersionPolicy_Specific* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy_Specific*>(
               &_ModelVersionPolicy_Specific_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  void Swap(ModelVersionPolicy_Specific* other);
  friend void swap(ModelVersionPolicy_Specific& a, ModelVersionPolicy_Specific& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelVersionPolicy_Specific* New() const final {
    return CreateMaybeMessage<ModelVersionPolicy_Specific>(NULL);
  }

  ModelVersionPolicy_Specific* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelVersionPolicy_Specific>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelVersionPolicy_Specific& from);
  void MergeFrom(const ModelVersionPolicy_Specific& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionPolicy_Specific* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 versions = 1;
  int versions_size() const;
  void clear_versions();
  static const int kVersionsFieldNumber = 1;
  ::google::protobuf::int64 versions(int index) const;
  void set_versions(int index, ::google::protobuf::int64 value);
  void add_versions(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      versions() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_versions();

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > versions_;
  mutable int _versions_cached_byte_size_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelVersionPolicy : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy) */ {
 public:
  ModelVersionPolicy();
  virtual ~ModelVersionPolicy();

  ModelVersionPolicy(const ModelVersionPolicy& from);

  inline ModelVersionPolicy& operator=(const ModelVersionPolicy& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelVersionPolicy(ModelVersionPolicy&& from) noexcept
    : ModelVersionPolicy() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy& operator=(ModelVersionPolicy&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelVersionPolicy& default_instance();

  enum PolicyChoiceCase {
    kLatest = 1,
    kAll = 2,
    kSpecific = 3,
    POLICY_CHOICE_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelVersionPolicy* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy*>(
               &_ModelVersionPolicy_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  void Swap(ModelVersionPolicy* other);
  friend void swap(ModelVersionPolicy& a, ModelVersionPolicy& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelVersionPolicy* New() const final {
    return CreateMaybeMessage<ModelVersionPolicy>(NULL);
  }

  ModelVersionPolicy* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelVersionPolicy>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelVersionPolicy& from);
  void MergeFrom(const ModelVersionPolicy& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionPolicy* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelVersionPolicy_Latest Latest;
  typedef ModelVersionPolicy_All All;
  typedef ModelVersionPolicy_Specific Specific;

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;
  bool has_latest() const;
  void clear_latest();
  static const int kLatestFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& _internal_latest() const;
  public:
  const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& latest() const;
  ::nvidia::inferenceserver::ModelVersionPolicy_Latest* release_latest();
  ::nvidia::inferenceserver::ModelVersionPolicy_Latest* mutable_latest();
  void set_allocated_latest(::nvidia::inferenceserver::ModelVersionPolicy_Latest* latest);

  // .nvidia.inferenceserver.ModelVersionPolicy.All all = 2;
  bool has_all() const;
  void clear_all();
  static const int kAllFieldNumber = 2;
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy_All& _internal_all() const;
  public:
  const ::nvidia::inferenceserver::ModelVersionPolicy_All& all() const;
  ::nvidia::inferenceserver::ModelVersionPolicy_All* release_all();
  ::nvidia::inferenceserver::ModelVersionPolicy_All* mutable_all();
  void set_allocated_all(::nvidia::inferenceserver::ModelVersionPolicy_All* all);

  // .nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;
  bool has_specific() const;
  void clear_specific();
  static const int kSpecificFieldNumber = 3;
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& _internal_specific() const;
  public:
  const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& specific() const;
  ::nvidia::inferenceserver::ModelVersionPolicy_Specific* release_specific();
  ::nvidia::inferenceserver::ModelVersionPolicy_Specific* mutable_specific();
  void set_allocated_specific(::nvidia::inferenceserver::ModelVersionPolicy_Specific* specific);

  void clear_policy_choice();
  PolicyChoiceCase policy_choice_case() const;
  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy)
 private:
  void set_has_latest();
  void set_has_all();
  void set_has_specific();

  inline bool has_policy_choice() const;
  inline void clear_has_policy_choice();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  union PolicyChoiceUnion {
    PolicyChoiceUnion() {}
    ::nvidia::inferenceserver::ModelVersionPolicy_Latest* latest_;
    ::nvidia::inferenceserver::ModelVersionPolicy_All* all_;
    ::nvidia::inferenceserver::ModelVersionPolicy_Specific* specific_;
  } policy_choice_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy_Graph : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy.Graph) */ {
 public:
  ModelOptimizationPolicy_Graph();
  virtual ~ModelOptimizationPolicy_Graph();

  ModelOptimizationPolicy_Graph(const ModelOptimizationPolicy_Graph& from);

  inline ModelOptimizationPolicy_Graph& operator=(const ModelOptimizationPolicy_Graph& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelOptimizationPolicy_Graph(ModelOptimizationPolicy_Graph&& from) noexcept
    : ModelOptimizationPolicy_Graph() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy_Graph& operator=(ModelOptimizationPolicy_Graph&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelOptimizationPolicy_Graph& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelOptimizationPolicy_Graph* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy_Graph*>(
               &_ModelOptimizationPolicy_Graph_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    8;

  void Swap(ModelOptimizationPolicy_Graph* other);
  friend void swap(ModelOptimizationPolicy_Graph& a, ModelOptimizationPolicy_Graph& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelOptimizationPolicy_Graph* New() const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_Graph>(NULL);
  }

  ModelOptimizationPolicy_Graph* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_Graph>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelOptimizationPolicy_Graph& from);
  void MergeFrom(const ModelOptimizationPolicy_Graph& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy_Graph* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int32 level = 1;
  void clear_level();
  static const int kLevelFieldNumber = 1;
  ::google::protobuf::int32 level() const;
  void set_level(::google::protobuf::int32 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::int32 level_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy_Cuda : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda) */ {
 public:
  ModelOptimizationPolicy_Cuda();
  virtual ~ModelOptimizationPolicy_Cuda();

  ModelOptimizationPolicy_Cuda(const ModelOptimizationPolicy_Cuda& from);

  inline ModelOptimizationPolicy_Cuda& operator=(const ModelOptimizationPolicy_Cuda& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelOptimizationPolicy_Cuda(ModelOptimizationPolicy_Cuda&& from) noexcept
    : ModelOptimizationPolicy_Cuda() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy_Cuda& operator=(ModelOptimizationPolicy_Cuda&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelOptimizationPolicy_Cuda& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelOptimizationPolicy_Cuda* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy_Cuda*>(
               &_ModelOptimizationPolicy_Cuda_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    9;

  void Swap(ModelOptimizationPolicy_Cuda* other);
  friend void swap(ModelOptimizationPolicy_Cuda& a, ModelOptimizationPolicy_Cuda& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelOptimizationPolicy_Cuda* New() const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_Cuda>(NULL);
  }

  ModelOptimizationPolicy_Cuda* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_Cuda>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelOptimizationPolicy_Cuda& from);
  void MergeFrom(const ModelOptimizationPolicy_Cuda& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy_Cuda* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // bool graphs = 1;
  void clear_graphs();
  static const int kGraphsFieldNumber = 1;
  bool graphs() const;
  void set_graphs(bool value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  bool graphs_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > SuperType;
  ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse();
  ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse& other);
  static const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse*>(&_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelOptimizationPolicy_ExecutionAccelerators_Accelerator : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) */ {
 public:
  ModelOptimizationPolicy_ExecutionAccelerators_Accelerator();
  virtual ~ModelOptimizationPolicy_ExecutionAccelerators_Accelerator();

  ModelOptimizationPolicy_ExecutionAccelerators_Accelerator(const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& from);

  inline ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& operator=(const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelOptimizationPolicy_ExecutionAccelerators_Accelerator(ModelOptimizationPolicy_ExecutionAccelerators_Accelerator&& from) noexcept
    : ModelOptimizationPolicy_ExecutionAccelerators_Accelerator() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& operator=(ModelOptimizationPolicy_ExecutionAccelerators_Accelerator&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator*>(
               &_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    11;

  void Swap(ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* other);
  friend void swap(ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& a, ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* New() const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_ExecutionAccelerators_Accelerator>(NULL);
  }

  ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_ExecutionAccelerators_Accelerator>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& from);
  void MergeFrom(const ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  // map<string, string> parameters = 2;
  int parameters_size() const;
  void clear_parameters();
  static const int kParametersFieldNumber = 2;
  const ::google::protobuf::Map< ::std::string, ::std::string >&
      parameters() const;
  ::google::protobuf::Map< ::std::string, ::std::string >*
      mutable_parameters();

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::MapField<
      ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_DoNotUse,
      ::std::string, ::std::string,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      0 > parameters_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy_ExecutionAccelerators : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators) */ {
 public:
  ModelOptimizationPolicy_ExecutionAccelerators();
  virtual ~ModelOptimizationPolicy_ExecutionAccelerators();

  ModelOptimizationPolicy_ExecutionAccelerators(const ModelOptimizationPolicy_ExecutionAccelerators& from);

  inline ModelOptimizationPolicy_ExecutionAccelerators& operator=(const ModelOptimizationPolicy_ExecutionAccelerators& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelOptimizationPolicy_ExecutionAccelerators(ModelOptimizationPolicy_ExecutionAccelerators&& from) noexcept
    : ModelOptimizationPolicy_ExecutionAccelerators() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy_ExecutionAccelerators& operator=(ModelOptimizationPolicy_ExecutionAccelerators&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelOptimizationPolicy_ExecutionAccelerators& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelOptimizationPolicy_ExecutionAccelerators* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy_ExecutionAccelerators*>(
               &_ModelOptimizationPolicy_ExecutionAccelerators_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    12;

  void Swap(ModelOptimizationPolicy_ExecutionAccelerators* other);
  friend void swap(ModelOptimizationPolicy_ExecutionAccelerators& a, ModelOptimizationPolicy_ExecutionAccelerators& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelOptimizationPolicy_ExecutionAccelerators* New() const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_ExecutionAccelerators>(NULL);
  }

  ModelOptimizationPolicy_ExecutionAccelerators* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_ExecutionAccelerators>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelOptimizationPolicy_ExecutionAccelerators& from);
  void MergeFrom(const ModelOptimizationPolicy_ExecutionAccelerators& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy_ExecutionAccelerators* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelOptimizationPolicy_ExecutionAccelerators_Accelerator Accelerator;

  // accessors -------------------------------------------------------

  // repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;
  int gpu_execution_accelerator_size() const;
  void clear_gpu_execution_accelerator();
  static const int kGpuExecutionAcceleratorFieldNumber = 1;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* mutable_gpu_execution_accelerator(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >*
      mutable_gpu_execution_accelerator();
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& gpu_execution_accelerator(int index) const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* add_gpu_execution_accelerator();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >&
      gpu_execution_accelerator() const;

  // repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;
  int cpu_execution_accelerator_size() const;
  void clear_cpu_execution_accelerator();
  static const int kCpuExecutionAcceleratorFieldNumber = 2;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* mutable_cpu_execution_accelerator(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >*
      mutable_cpu_execution_accelerator();
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& cpu_execution_accelerator(int index) const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* add_cpu_execution_accelerator();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >&
      cpu_execution_accelerator() const;

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator > gpu_execution_accelerator_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator > cpu_execution_accelerator_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy_PinnedMemoryBuffer : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer) */ {
 public:
  ModelOptimizationPolicy_PinnedMemoryBuffer();
  virtual ~ModelOptimizationPolicy_PinnedMemoryBuffer();

  ModelOptimizationPolicy_PinnedMemoryBuffer(const ModelOptimizationPolicy_PinnedMemoryBuffer& from);

  inline ModelOptimizationPolicy_PinnedMemoryBuffer& operator=(const ModelOptimizationPolicy_PinnedMemoryBuffer& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelOptimizationPolicy_PinnedMemoryBuffer(ModelOptimizationPolicy_PinnedMemoryBuffer&& from) noexcept
    : ModelOptimizationPolicy_PinnedMemoryBuffer() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy_PinnedMemoryBuffer& operator=(ModelOptimizationPolicy_PinnedMemoryBuffer&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelOptimizationPolicy_PinnedMemoryBuffer& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelOptimizationPolicy_PinnedMemoryBuffer* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy_PinnedMemoryBuffer*>(
               &_ModelOptimizationPolicy_PinnedMemoryBuffer_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    13;

  void Swap(ModelOptimizationPolicy_PinnedMemoryBuffer* other);
  friend void swap(ModelOptimizationPolicy_PinnedMemoryBuffer& a, ModelOptimizationPolicy_PinnedMemoryBuffer& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelOptimizationPolicy_PinnedMemoryBuffer* New() const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_PinnedMemoryBuffer>(NULL);
  }

  ModelOptimizationPolicy_PinnedMemoryBuffer* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_PinnedMemoryBuffer>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelOptimizationPolicy_PinnedMemoryBuffer& from);
  void MergeFrom(const ModelOptimizationPolicy_PinnedMemoryBuffer& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy_PinnedMemoryBuffer* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // bool enable = 1;
  void clear_enable();
  static const int kEnableFieldNumber = 1;
  bool enable() const;
  void set_enable(bool value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  bool enable_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy) */ {
 public:
  ModelOptimizationPolicy();
  virtual ~ModelOptimizationPolicy();

  ModelOptimizationPolicy(const ModelOptimizationPolicy& from);

  inline ModelOptimizationPolicy& operator=(const ModelOptimizationPolicy& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelOptimizationPolicy(ModelOptimizationPolicy&& from) noexcept
    : ModelOptimizationPolicy() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy& operator=(ModelOptimizationPolicy&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelOptimizationPolicy& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelOptimizationPolicy* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy*>(
               &_ModelOptimizationPolicy_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    14;

  void Swap(ModelOptimizationPolicy* other);
  friend void swap(ModelOptimizationPolicy& a, ModelOptimizationPolicy& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelOptimizationPolicy* New() const final {
    return CreateMaybeMessage<ModelOptimizationPolicy>(NULL);
  }

  ModelOptimizationPolicy* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelOptimizationPolicy& from);
  void MergeFrom(const ModelOptimizationPolicy& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelOptimizationPolicy_Graph Graph;
  typedef ModelOptimizationPolicy_Cuda Cuda;
  typedef ModelOptimizationPolicy_ExecutionAccelerators ExecutionAccelerators;
  typedef ModelOptimizationPolicy_PinnedMemoryBuffer PinnedMemoryBuffer;

  typedef ModelOptimizationPolicy_ModelPriority ModelPriority;
  static const ModelPriority PRIORITY_DEFAULT =
    ModelOptimizationPolicy_ModelPriority_PRIORITY_DEFAULT;
  static const ModelPriority PRIORITY_MAX =
    ModelOptimizationPolicy_ModelPriority_PRIORITY_MAX;
  static const ModelPriority PRIORITY_MIN =
    ModelOptimizationPolicy_ModelPriority_PRIORITY_MIN;
  static inline bool ModelPriority_IsValid(int value) {
    return ModelOptimizationPolicy_ModelPriority_IsValid(value);
  }
  static const ModelPriority ModelPriority_MIN =
    ModelOptimizationPolicy_ModelPriority_ModelPriority_MIN;
  static const ModelPriority ModelPriority_MAX =
    ModelOptimizationPolicy_ModelPriority_ModelPriority_MAX;
  static const int ModelPriority_ARRAYSIZE =
    ModelOptimizationPolicy_ModelPriority_ModelPriority_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  ModelPriority_descriptor() {
    return ModelOptimizationPolicy_ModelPriority_descriptor();
  }
  static inline const ::std::string& ModelPriority_Name(ModelPriority value) {
    return ModelOptimizationPolicy_ModelPriority_Name(value);
  }
  static inline bool ModelPriority_Parse(const ::std::string& name,
      ModelPriority* value) {
    return ModelOptimizationPolicy_ModelPriority_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;
  bool has_graph() const;
  void clear_graph();
  static const int kGraphFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& _internal_graph() const;
  public:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& graph() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* release_graph();
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* mutable_graph();
  void set_allocated_graph(::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph);

  // .nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;
  bool has_cuda() const;
  void clear_cuda();
  static const int kCudaFieldNumber = 3;
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda& _internal_cuda() const;
  public:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda& cuda() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* release_cuda();
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* mutable_cuda();
  void set_allocated_cuda(::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* cuda);

  // .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;
  bool has_execution_accelerators() const;
  void clear_execution_accelerators();
  static const int kExecutionAcceleratorsFieldNumber = 4;
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators& _internal_execution_accelerators() const;
  public:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators& execution_accelerators() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* release_execution_accelerators();
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* mutable_execution_accelerators();
  void set_allocated_execution_accelerators(::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* execution_accelerators);

  // .nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;
  bool has_input_pinned_memory() const;
  void clear_input_pinned_memory();
  static const int kInputPinnedMemoryFieldNumber = 5;
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& _internal_input_pinned_memory() const;
  public:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& input_pinned_memory() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* release_input_pinned_memory();
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* mutable_input_pinned_memory();
  void set_allocated_input_pinned_memory(::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* input_pinned_memory);

  // .nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;
  bool has_output_pinned_memory() const;
  void clear_output_pinned_memory();
  static const int kOutputPinnedMemoryFieldNumber = 6;
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& _internal_output_pinned_memory() const;
  public:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& output_pinned_memory() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* release_output_pinned_memory();
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* mutable_output_pinned_memory();
  void set_allocated_output_pinned_memory(::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* output_pinned_memory);

  // .nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;
  void clear_priority();
  static const int kPriorityFieldNumber = 2;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority priority() const;
  void set_priority(::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* cuda_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* execution_accelerators_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* input_pinned_memory_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* output_pinned_memory_;
  int priority_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelQueuePolicy : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelQueuePolicy) */ {
 public:
  ModelQueuePolicy();
  virtual ~ModelQueuePolicy();

  ModelQueuePolicy(const ModelQueuePolicy& from);

  inline ModelQueuePolicy& operator=(const ModelQueuePolicy& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelQueuePolicy(ModelQueuePolicy&& from) noexcept
    : ModelQueuePolicy() {
    *this = ::std::move(from);
  }

  inline ModelQueuePolicy& operator=(ModelQueuePolicy&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelQueuePolicy& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelQueuePolicy* internal_default_instance() {
    return reinterpret_cast<const ModelQueuePolicy*>(
               &_ModelQueuePolicy_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    15;

  void Swap(ModelQueuePolicy* other);
  friend void swap(ModelQueuePolicy& a, ModelQueuePolicy& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelQueuePolicy* New() const final {
    return CreateMaybeMessage<ModelQueuePolicy>(NULL);
  }

  ModelQueuePolicy* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelQueuePolicy>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelQueuePolicy& from);
  void MergeFrom(const ModelQueuePolicy& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelQueuePolicy* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelQueuePolicy_TimeoutAction TimeoutAction;
  static const TimeoutAction REJECT =
    ModelQueuePolicy_TimeoutAction_REJECT;
  static const TimeoutAction DELAY =
    ModelQueuePolicy_TimeoutAction_DELAY;
  static inline bool TimeoutAction_IsValid(int value) {
    return ModelQueuePolicy_TimeoutAction_IsValid(value);
  }
  static const TimeoutAction TimeoutAction_MIN =
    ModelQueuePolicy_TimeoutAction_TimeoutAction_MIN;
  static const TimeoutAction TimeoutAction_MAX =
    ModelQueuePolicy_TimeoutAction_TimeoutAction_MAX;
  static const int TimeoutAction_ARRAYSIZE =
    ModelQueuePolicy_TimeoutAction_TimeoutAction_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  TimeoutAction_descriptor() {
    return ModelQueuePolicy_TimeoutAction_descriptor();
  }
  static inline const ::std::string& TimeoutAction_Name(TimeoutAction value) {
    return ModelQueuePolicy_TimeoutAction_Name(value);
  }
  static inline bool TimeoutAction_Parse(const ::std::string& name,
      TimeoutAction* value) {
    return ModelQueuePolicy_TimeoutAction_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // uint64 default_timeout_microseconds = 2;
  void clear_default_timeout_microseconds();
  static const int kDefaultTimeoutMicrosecondsFieldNumber = 2;
  ::google::protobuf::uint64 default_timeout_microseconds() const;
  void set_default_timeout_microseconds(::google::protobuf::uint64 value);

  // .nvidia.inferenceserver.ModelQueuePolicy.TimeoutAction timeout_action = 1;
  void clear_timeout_action();
  static const int kTimeoutActionFieldNumber = 1;
  ::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction timeout_action() const;
  void set_timeout_action(::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction value);

  // bool allow_timeout_override = 3;
  void clear_allow_timeout_override();
  static const int kAllowTimeoutOverrideFieldNumber = 3;
  bool allow_timeout_override() const;
  void set_allow_timeout_override(bool value);

  // uint32 max_queue_size = 4;
  void clear_max_queue_size();
  static const int kMaxQueueSizeFieldNumber = 4;
  ::google::protobuf::uint32 max_queue_size() const;
  void set_max_queue_size(::google::protobuf::uint32 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelQueuePolicy)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::uint64 default_timeout_microseconds_;
  int timeout_action_;
  bool allow_timeout_override_;
  ::google::protobuf::uint32 max_queue_size_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse, 
    ::google::protobuf::uint32, ::nvidia::inferenceserver::ModelQueuePolicy,
    ::google::protobuf::internal::WireFormatLite::TYPE_UINT32,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse, 
    ::google::protobuf::uint32, ::nvidia::inferenceserver::ModelQueuePolicy,
    ::google::protobuf::internal::WireFormatLite::TYPE_UINT32,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > SuperType;
  ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse();
  ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse& other);
  static const ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse*>(&_ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelDynamicBatching : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelDynamicBatching) */ {
 public:
  ModelDynamicBatching();
  virtual ~ModelDynamicBatching();

  ModelDynamicBatching(const ModelDynamicBatching& from);

  inline ModelDynamicBatching& operator=(const ModelDynamicBatching& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelDynamicBatching(ModelDynamicBatching&& from) noexcept
    : ModelDynamicBatching() {
    *this = ::std::move(from);
  }

  inline ModelDynamicBatching& operator=(ModelDynamicBatching&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelDynamicBatching& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelDynamicBatching* internal_default_instance() {
    return reinterpret_cast<const ModelDynamicBatching*>(
               &_ModelDynamicBatching_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    17;

  void Swap(ModelDynamicBatching* other);
  friend void swap(ModelDynamicBatching& a, ModelDynamicBatching& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelDynamicBatching* New() const final {
    return CreateMaybeMessage<ModelDynamicBatching>(NULL);
  }

  ModelDynamicBatching* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelDynamicBatching>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelDynamicBatching& from);
  void MergeFrom(const ModelDynamicBatching& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelDynamicBatching* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  // repeated int32 preferred_batch_size = 1;
  int preferred_batch_size_size() const;
  void clear_preferred_batch_size();
  static const int kPreferredBatchSizeFieldNumber = 1;
  ::google::protobuf::int32 preferred_batch_size(int index) const;
  void set_preferred_batch_size(int index, ::google::protobuf::int32 value);
  void add_preferred_batch_size(::google::protobuf::int32 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
      preferred_batch_size() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
      mutable_preferred_batch_size();

  // map<uint32, .nvidia.inferenceserver.ModelQueuePolicy> priority_queue_policy = 7;
  int priority_queue_policy_size() const;
  void clear_priority_queue_policy();
  static const int kPriorityQueuePolicyFieldNumber = 7;
  const ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::ModelQueuePolicy >&
      priority_queue_policy() const;
  ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::ModelQueuePolicy >*
      mutable_priority_queue_policy();

  // .nvidia.inferenceserver.ModelQueuePolicy default_queue_policy = 6;
  bool has_default_queue_policy() const;
  void clear_default_queue_policy();
  static const int kDefaultQueuePolicyFieldNumber = 6;
  private:
  const ::nvidia::inferenceserver::ModelQueuePolicy& _internal_default_queue_policy() const;
  public:
  const ::nvidia::inferenceserver::ModelQueuePolicy& default_queue_policy() const;
  ::nvidia::inferenceserver::ModelQueuePolicy* release_default_queue_policy();
  ::nvidia::inferenceserver::ModelQueuePolicy* mutable_default_queue_policy();
  void set_allocated_default_queue_policy(::nvidia::inferenceserver::ModelQueuePolicy* default_queue_policy);

  // uint64 max_queue_delay_microseconds = 2;
  void clear_max_queue_delay_microseconds();
  static const int kMaxQueueDelayMicrosecondsFieldNumber = 2;
  ::google::protobuf::uint64 max_queue_delay_microseconds() const;
  void set_max_queue_delay_microseconds(::google::protobuf::uint64 value);

  // bool preserve_ordering = 3;
  void clear_preserve_ordering();
  static const int kPreserveOrderingFieldNumber = 3;
  bool preserve_ordering() const;
  void set_preserve_ordering(bool value);

  // uint32 priority_levels = 4;
  void clear_priority_levels();
  static const int kPriorityLevelsFieldNumber = 4;
  ::google::protobuf::uint32 priority_levels() const;
  void set_priority_levels(::google::protobuf::uint32 value);

  // uint32 default_priority_level = 5;
  void clear_default_priority_level();
  static const int kDefaultPriorityLevelFieldNumber = 5;
  ::google::protobuf::uint32 default_priority_level() const;
  void set_default_priority_level(::google::protobuf::uint32 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelDynamicBatching)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 > preferred_batch_size_;
  mutable int _preferred_batch_size_cached_byte_size_;
  ::google::protobuf::internal::MapField<
      ModelDynamicBatching_PriorityQueuePolicyEntry_DoNotUse,
      ::google::protobuf::uint32, ::nvidia::inferenceserver::ModelQueuePolicy,
      ::google::protobuf::internal::WireFormatLite::TYPE_UINT32,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 > priority_queue_policy_;
  ::nvidia::inferenceserver::ModelQueuePolicy* default_queue_policy_;
  ::google::protobuf::uint64 max_queue_delay_microseconds_;
  bool preserve_ordering_;
  ::google::protobuf::uint32 priority_levels_;
  ::google::protobuf::uint32 default_priority_level_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelSequenceBatching_Control : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelSequenceBatching.Control) */ {
 public:
  ModelSequenceBatching_Control();
  virtual ~ModelSequenceBatching_Control();

  ModelSequenceBatching_Control(const ModelSequenceBatching_Control& from);

  inline ModelSequenceBatching_Control& operator=(const ModelSequenceBatching_Control& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelSequenceBatching_Control(ModelSequenceBatching_Control&& from) noexcept
    : ModelSequenceBatching_Control() {
    *this = ::std::move(from);
  }

  inline ModelSequenceBatching_Control& operator=(ModelSequenceBatching_Control&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelSequenceBatching_Control& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelSequenceBatching_Control* internal_default_instance() {
    return reinterpret_cast<const ModelSequenceBatching_Control*>(
               &_ModelSequenceBatching_Control_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    18;

  void Swap(ModelSequenceBatching_Control* other);
  friend void swap(ModelSequenceBatching_Control& a, ModelSequenceBatching_Control& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelSequenceBatching_Control* New() const final {
    return CreateMaybeMessage<ModelSequenceBatching_Control>(NULL);
  }

  ModelSequenceBatching_Control* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelSequenceBatching_Control>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelSequenceBatching_Control& from);
  void MergeFrom(const ModelSequenceBatching_Control& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelSequenceBatching_Control* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelSequenceBatching_Control_Kind Kind;
  static const Kind CONTROL_SEQUENCE_START =
    ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_START;
  static const Kind CONTROL_SEQUENCE_READY =
    ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_READY;
  static const Kind CONTROL_SEQUENCE_END =
    ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_END;
  static const Kind CONTROL_SEQUENCE_CORRID =
    ModelSequenceBatching_Control_Kind_CONTROL_SEQUENCE_CORRID;
  static inline bool Kind_IsValid(int value) {
    return ModelSequenceBatching_Control_Kind_IsValid(value);
  }
  static const Kind Kind_MIN =
    ModelSequenceBatching_Control_Kind_Kind_MIN;
  static const Kind Kind_MAX =
    ModelSequenceBatching_Control_Kind_Kind_MAX;
  static const int Kind_ARRAYSIZE =
    ModelSequenceBatching_Control_Kind_Kind_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Kind_descriptor() {
    return ModelSequenceBatching_Control_Kind_descriptor();
  }
  static inline const ::std::string& Kind_Name(Kind value) {
    return ModelSequenceBatching_Control_Kind_Name(value);
  }
  static inline bool Kind_Parse(const ::std::string& name,
      Kind* value) {
    return ModelSequenceBatching_Control_Kind_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // repeated int32 int32_false_true = 2;
  int int32_false_true_size() const;
  void clear_int32_false_true();
  static const int kInt32FalseTrueFieldNumber = 2;
  ::google::protobuf::int32 int32_false_true(int index) const;
  void set_int32_false_true(int index, ::google::protobuf::int32 value);
  void add_int32_false_true(::google::protobuf::int32 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
      int32_false_true() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
      mutable_int32_false_true();

  // repeated float fp32_false_true = 3;
  int fp32_false_true_size() const;
  void clear_fp32_false_true();
  static const int kFp32FalseTrueFieldNumber = 3;
  float fp32_false_true(int index) const;
  void set_fp32_false_true(int index, float value);
  void add_fp32_false_true(float value);
  const ::google::protobuf::RepeatedField< float >&
      fp32_false_true() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_fp32_false_true();

  // .nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;
  void clear_kind();
  static const int kKindFieldNumber = 1;
  ::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind kind() const;
  void set_kind(::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind value);

  // .nvidia.inferenceserver.DataType data_type = 4;
  void clear_data_type();
  static const int kDataTypeFieldNumber = 4;
  ::nvidia::inferenceserver::DataType data_type() const;
  void set_data_type(::nvidia::inferenceserver::DataType value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.Control)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 > int32_false_true_;
  mutable int _int32_false_true_cached_byte_size_;
  ::google::protobuf::RepeatedField< float > fp32_false_true_;
  mutable int _fp32_false_true_cached_byte_size_;
  int kind_;
  int data_type_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelSequenceBatching_ControlInput : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelSequenceBatching.ControlInput) */ {
 public:
  ModelSequenceBatching_ControlInput();
  virtual ~ModelSequenceBatching_ControlInput();

  ModelSequenceBatching_ControlInput(const ModelSequenceBatching_ControlInput& from);

  inline ModelSequenceBatching_ControlInput& operator=(const ModelSequenceBatching_ControlInput& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelSequenceBatching_ControlInput(ModelSequenceBatching_ControlInput&& from) noexcept
    : ModelSequenceBatching_ControlInput() {
    *this = ::std::move(from);
  }

  inline ModelSequenceBatching_ControlInput& operator=(ModelSequenceBatching_ControlInput&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelSequenceBatching_ControlInput& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelSequenceBatching_ControlInput* internal_default_instance() {
    return reinterpret_cast<const ModelSequenceBatching_ControlInput*>(
               &_ModelSequenceBatching_ControlInput_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    19;

  void Swap(ModelSequenceBatching_ControlInput* other);
  friend void swap(ModelSequenceBatching_ControlInput& a, ModelSequenceBatching_ControlInput& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelSequenceBatching_ControlInput* New() const final {
    return CreateMaybeMessage<ModelSequenceBatching_ControlInput>(NULL);
  }

  ModelSequenceBatching_ControlInput* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelSequenceBatching_ControlInput>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelSequenceBatching_ControlInput& from);
  void MergeFrom(const ModelSequenceBatching_ControlInput& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelSequenceBatching_ControlInput* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;
  int control_size() const;
  void clear_control();
  static const int kControlFieldNumber = 2;
  ::nvidia::inferenceserver::ModelSequenceBatching_Control* mutable_control(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_Control >*
      mutable_control();
  const ::nvidia::inferenceserver::ModelSequenceBatching_Control& control(int index) const;
  ::nvidia::inferenceserver::ModelSequenceBatching_Control* add_control();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_Control >&
      control() const;

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_Control > control_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelSequenceBatching_StrategyDirect : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect) */ {
 public:
  ModelSequenceBatching_StrategyDirect();
  virtual ~ModelSequenceBatching_StrategyDirect();

  ModelSequenceBatching_StrategyDirect(const ModelSequenceBatching_StrategyDirect& from);

  inline ModelSequenceBatching_StrategyDirect& operator=(const ModelSequenceBatching_StrategyDirect& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelSequenceBatching_StrategyDirect(ModelSequenceBatching_StrategyDirect&& from) noexcept
    : ModelSequenceBatching_StrategyDirect() {
    *this = ::std::move(from);
  }

  inline ModelSequenceBatching_StrategyDirect& operator=(ModelSequenceBatching_StrategyDirect&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelSequenceBatching_StrategyDirect& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelSequenceBatching_StrategyDirect* internal_default_instance() {
    return reinterpret_cast<const ModelSequenceBatching_StrategyDirect*>(
               &_ModelSequenceBatching_StrategyDirect_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    20;

  void Swap(ModelSequenceBatching_StrategyDirect* other);
  friend void swap(ModelSequenceBatching_StrategyDirect& a, ModelSequenceBatching_StrategyDirect& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelSequenceBatching_StrategyDirect* New() const final {
    return CreateMaybeMessage<ModelSequenceBatching_StrategyDirect>(NULL);
  }

  ModelSequenceBatching_StrategyDirect* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelSequenceBatching_StrategyDirect>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelSequenceBatching_StrategyDirect& from);
  void MergeFrom(const ModelSequenceBatching_StrategyDirect& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelSequenceBatching_StrategyDirect* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelSequenceBatching_StrategyOldest : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest) */ {
 public:
  ModelSequenceBatching_StrategyOldest();
  virtual ~ModelSequenceBatching_StrategyOldest();

  ModelSequenceBatching_StrategyOldest(const ModelSequenceBatching_StrategyOldest& from);

  inline ModelSequenceBatching_StrategyOldest& operator=(const ModelSequenceBatching_StrategyOldest& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelSequenceBatching_StrategyOldest(ModelSequenceBatching_StrategyOldest&& from) noexcept
    : ModelSequenceBatching_StrategyOldest() {
    *this = ::std::move(from);
  }

  inline ModelSequenceBatching_StrategyOldest& operator=(ModelSequenceBatching_StrategyOldest&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelSequenceBatching_StrategyOldest& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelSequenceBatching_StrategyOldest* internal_default_instance() {
    return reinterpret_cast<const ModelSequenceBatching_StrategyOldest*>(
               &_ModelSequenceBatching_StrategyOldest_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    21;

  void Swap(ModelSequenceBatching_StrategyOldest* other);
  friend void swap(ModelSequenceBatching_StrategyOldest& a, ModelSequenceBatching_StrategyOldest& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelSequenceBatching_StrategyOldest* New() const final {
    return CreateMaybeMessage<ModelSequenceBatching_StrategyOldest>(NULL);
  }

  ModelSequenceBatching_StrategyOldest* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelSequenceBatching_StrategyOldest>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelSequenceBatching_StrategyOldest& from);
  void MergeFrom(const ModelSequenceBatching_StrategyOldest& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelSequenceBatching_StrategyOldest* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int32 preferred_batch_size = 2;
  int preferred_batch_size_size() const;
  void clear_preferred_batch_size();
  static const int kPreferredBatchSizeFieldNumber = 2;
  ::google::protobuf::int32 preferred_batch_size(int index) const;
  void set_preferred_batch_size(int index, ::google::protobuf::int32 value);
  void add_preferred_batch_size(::google::protobuf::int32 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
      preferred_batch_size() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
      mutable_preferred_batch_size();

  // uint64 max_queue_delay_microseconds = 3;
  void clear_max_queue_delay_microseconds();
  static const int kMaxQueueDelayMicrosecondsFieldNumber = 3;
  ::google::protobuf::uint64 max_queue_delay_microseconds() const;
  void set_max_queue_delay_microseconds(::google::protobuf::uint64 value);

  // int32 max_candidate_sequences = 1;
  void clear_max_candidate_sequences();
  static const int kMaxCandidateSequencesFieldNumber = 1;
  ::google::protobuf::int32 max_candidate_sequences() const;
  void set_max_candidate_sequences(::google::protobuf::int32 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int32 > preferred_batch_size_;
  mutable int _preferred_batch_size_cached_byte_size_;
  ::google::protobuf::uint64 max_queue_delay_microseconds_;
  ::google::protobuf::int32 max_candidate_sequences_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelSequenceBatching : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelSequenceBatching) */ {
 public:
  ModelSequenceBatching();
  virtual ~ModelSequenceBatching();

  ModelSequenceBatching(const ModelSequenceBatching& from);

  inline ModelSequenceBatching& operator=(const ModelSequenceBatching& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelSequenceBatching(ModelSequenceBatching&& from) noexcept
    : ModelSequenceBatching() {
    *this = ::std::move(from);
  }

  inline ModelSequenceBatching& operator=(ModelSequenceBatching&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelSequenceBatching& default_instance();

  enum StrategyChoiceCase {
    kDirect = 3,
    kOldest = 4,
    STRATEGY_CHOICE_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelSequenceBatching* internal_default_instance() {
    return reinterpret_cast<const ModelSequenceBatching*>(
               &_ModelSequenceBatching_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    22;

  void Swap(ModelSequenceBatching* other);
  friend void swap(ModelSequenceBatching& a, ModelSequenceBatching& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelSequenceBatching* New() const final {
    return CreateMaybeMessage<ModelSequenceBatching>(NULL);
  }

  ModelSequenceBatching* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelSequenceBatching>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelSequenceBatching& from);
  void MergeFrom(const ModelSequenceBatching& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelSequenceBatching* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelSequenceBatching_Control Control;
  typedef ModelSequenceBatching_ControlInput ControlInput;
  typedef ModelSequenceBatching_StrategyDirect StrategyDirect;
  typedef ModelSequenceBatching_StrategyOldest StrategyOldest;

  // accessors -------------------------------------------------------

  // repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;
  int control_input_size() const;
  void clear_control_input();
  static const int kControlInputFieldNumber = 2;
  ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput* mutable_control_input(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput >*
      mutable_control_input();
  const ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput& control_input(int index) const;
  ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput* add_control_input();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput >&
      control_input() const;

  // uint64 max_sequence_idle_microseconds = 1;
  void clear_max_sequence_idle_microseconds();
  static const int kMaxSequenceIdleMicrosecondsFieldNumber = 1;
  ::google::protobuf::uint64 max_sequence_idle_microseconds() const;
  void set_max_sequence_idle_microseconds(::google::protobuf::uint64 value);

  // .nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;
  bool has_direct() const;
  void clear_direct();
  static const int kDirectFieldNumber = 3;
  private:
  const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect& _internal_direct() const;
  public:
  const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect& direct() const;
  ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* release_direct();
  ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* mutable_direct();
  void set_allocated_direct(::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* direct);

  // .nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;
  bool has_oldest() const;
  void clear_oldest();
  static const int kOldestFieldNumber = 4;
  private:
  const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest& _internal_oldest() const;
  public:
  const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest& oldest() const;
  ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* release_oldest();
  ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* mutable_oldest();
  void set_allocated_oldest(::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* oldest);

  void clear_strategy_choice();
  StrategyChoiceCase strategy_choice_case() const;
  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching)
 private:
  void set_has_direct();
  void set_has_oldest();

  inline bool has_strategy_choice() const;
  inline void clear_has_strategy_choice();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput > control_input_;
  ::google::protobuf::uint64 max_sequence_idle_microseconds_;
  union StrategyChoiceUnion {
    StrategyChoiceUnion() {}
    ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* direct_;
    ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* oldest_;
  } strategy_choice_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelEnsembling_Step_InputMapEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelEnsembling_Step_InputMapEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelEnsembling_Step_InputMapEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > SuperType;
  ModelEnsembling_Step_InputMapEntry_DoNotUse();
  ModelEnsembling_Step_InputMapEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelEnsembling_Step_InputMapEntry_DoNotUse& other);
  static const ModelEnsembling_Step_InputMapEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelEnsembling_Step_InputMapEntry_DoNotUse*>(&_ModelEnsembling_Step_InputMapEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelEnsembling_Step_OutputMapEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelEnsembling_Step_OutputMapEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelEnsembling_Step_OutputMapEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > SuperType;
  ModelEnsembling_Step_OutputMapEntry_DoNotUse();
  ModelEnsembling_Step_OutputMapEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelEnsembling_Step_OutputMapEntry_DoNotUse& other);
  static const ModelEnsembling_Step_OutputMapEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelEnsembling_Step_OutputMapEntry_DoNotUse*>(&_ModelEnsembling_Step_OutputMapEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelEnsembling_Step : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelEnsembling.Step) */ {
 public:
  ModelEnsembling_Step();
  virtual ~ModelEnsembling_Step();

  ModelEnsembling_Step(const ModelEnsembling_Step& from);

  inline ModelEnsembling_Step& operator=(const ModelEnsembling_Step& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelEnsembling_Step(ModelEnsembling_Step&& from) noexcept
    : ModelEnsembling_Step() {
    *this = ::std::move(from);
  }

  inline ModelEnsembling_Step& operator=(ModelEnsembling_Step&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelEnsembling_Step& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelEnsembling_Step* internal_default_instance() {
    return reinterpret_cast<const ModelEnsembling_Step*>(
               &_ModelEnsembling_Step_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    25;

  void Swap(ModelEnsembling_Step* other);
  friend void swap(ModelEnsembling_Step& a, ModelEnsembling_Step& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelEnsembling_Step* New() const final {
    return CreateMaybeMessage<ModelEnsembling_Step>(NULL);
  }

  ModelEnsembling_Step* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelEnsembling_Step>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelEnsembling_Step& from);
  void MergeFrom(const ModelEnsembling_Step& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelEnsembling_Step* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  // map<string, string> input_map = 3;
  int input_map_size() const;
  void clear_input_map();
  static const int kInputMapFieldNumber = 3;
  const ::google::protobuf::Map< ::std::string, ::std::string >&
      input_map() const;
  ::google::protobuf::Map< ::std::string, ::std::string >*
      mutable_input_map();

  // map<string, string> output_map = 4;
  int output_map_size() const;
  void clear_output_map();
  static const int kOutputMapFieldNumber = 4;
  const ::google::protobuf::Map< ::std::string, ::std::string >&
      output_map() const;
  ::google::protobuf::Map< ::std::string, ::std::string >*
      mutable_output_map();

  // string model_name = 1;
  void clear_model_name();
  static const int kModelNameFieldNumber = 1;
  const ::std::string& model_name() const;
  void set_model_name(const ::std::string& value);
  #if LANG_CXX11
  void set_model_name(::std::string&& value);
  #endif
  void set_model_name(const char* value);
  void set_model_name(const char* value, size_t size);
  ::std::string* mutable_model_name();
  ::std::string* release_model_name();
  void set_allocated_model_name(::std::string* model_name);

  // int64 model_version = 2;
  void clear_model_version();
  static const int kModelVersionFieldNumber = 2;
  ::google::protobuf::int64 model_version() const;
  void set_model_version(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::MapField<
      ModelEnsembling_Step_InputMapEntry_DoNotUse,
      ::std::string, ::std::string,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      0 > input_map_;
  ::google::protobuf::internal::MapField<
      ModelEnsembling_Step_OutputMapEntry_DoNotUse,
      ::std::string, ::std::string,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      0 > output_map_;
  ::google::protobuf::internal::ArenaStringPtr model_name_;
  ::google::protobuf::int64 model_version_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelEnsembling : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelEnsembling) */ {
 public:
  ModelEnsembling();
  virtual ~ModelEnsembling();

  ModelEnsembling(const ModelEnsembling& from);

  inline ModelEnsembling& operator=(const ModelEnsembling& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelEnsembling(ModelEnsembling&& from) noexcept
    : ModelEnsembling() {
    *this = ::std::move(from);
  }

  inline ModelEnsembling& operator=(ModelEnsembling&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelEnsembling& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelEnsembling* internal_default_instance() {
    return reinterpret_cast<const ModelEnsembling*>(
               &_ModelEnsembling_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    26;

  void Swap(ModelEnsembling* other);
  friend void swap(ModelEnsembling& a, ModelEnsembling& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelEnsembling* New() const final {
    return CreateMaybeMessage<ModelEnsembling>(NULL);
  }

  ModelEnsembling* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelEnsembling>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelEnsembling& from);
  void MergeFrom(const ModelEnsembling& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelEnsembling* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelEnsembling_Step Step;

  // accessors -------------------------------------------------------

  // repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;
  int step_size() const;
  void clear_step();
  static const int kStepFieldNumber = 1;
  ::nvidia::inferenceserver::ModelEnsembling_Step* mutable_step(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelEnsembling_Step >*
      mutable_step();
  const ::nvidia::inferenceserver::ModelEnsembling_Step& step(int index) const;
  ::nvidia::inferenceserver::ModelEnsembling_Step* add_step();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelEnsembling_Step >&
      step() const;

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelEnsembling_Step > step_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelParameter : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelParameter) */ {
 public:
  ModelParameter();
  virtual ~ModelParameter();

  ModelParameter(const ModelParameter& from);

  inline ModelParameter& operator=(const ModelParameter& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelParameter(ModelParameter&& from) noexcept
    : ModelParameter() {
    *this = ::std::move(from);
  }

  inline ModelParameter& operator=(ModelParameter&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelParameter& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelParameter* internal_default_instance() {
    return reinterpret_cast<const ModelParameter*>(
               &_ModelParameter_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    27;

  void Swap(ModelParameter* other);
  friend void swap(ModelParameter& a, ModelParameter& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelParameter* New() const final {
    return CreateMaybeMessage<ModelParameter>(NULL);
  }

  ModelParameter* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelParameter>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelParameter& from);
  void MergeFrom(const ModelParameter& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelParameter* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string string_value = 1;
  void clear_string_value();
  static const int kStringValueFieldNumber = 1;
  const ::std::string& string_value() const;
  void set_string_value(const ::std::string& value);
  #if LANG_CXX11
  void set_string_value(::std::string&& value);
  #endif
  void set_string_value(const char* value);
  void set_string_value(const char* value, size_t size);
  ::std::string* mutable_string_value();
  ::std::string* release_string_value();
  void set_allocated_string_value(::std::string* string_value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelParameter)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr string_value_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelWarmup_Input : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelWarmup.Input) */ {
 public:
  ModelWarmup_Input();
  virtual ~ModelWarmup_Input();

  ModelWarmup_Input(const ModelWarmup_Input& from);

  inline ModelWarmup_Input& operator=(const ModelWarmup_Input& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelWarmup_Input(ModelWarmup_Input&& from) noexcept
    : ModelWarmup_Input() {
    *this = ::std::move(from);
  }

  inline ModelWarmup_Input& operator=(ModelWarmup_Input&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelWarmup_Input& default_instance();

  enum InputDataTypeCase {
    kZeroData = 3,
    kRandomData = 4,
    kInputDataFile = 5,
    INPUT_DATA_TYPE_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelWarmup_Input* internal_default_instance() {
    return reinterpret_cast<const ModelWarmup_Input*>(
               &_ModelWarmup_Input_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    28;

  void Swap(ModelWarmup_Input* other);
  friend void swap(ModelWarmup_Input& a, ModelWarmup_Input& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelWarmup_Input* New() const final {
    return CreateMaybeMessage<ModelWarmup_Input>(NULL);
  }

  ModelWarmup_Input* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelWarmup_Input>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelWarmup_Input& from);
  void MergeFrom(const ModelWarmup_Input& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelWarmup_Input* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated int64 dims = 2;
  int dims_size() const;
  void clear_dims();
  static const int kDimsFieldNumber = 2;
  ::google::protobuf::int64 dims(int index) const;
  void set_dims(int index, ::google::protobuf::int64 value);
  void add_dims(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      dims() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_dims();

  // .nvidia.inferenceserver.DataType data_type = 1;
  void clear_data_type();
  static const int kDataTypeFieldNumber = 1;
  ::nvidia::inferenceserver::DataType data_type() const;
  void set_data_type(::nvidia::inferenceserver::DataType value);

  // bool zero_data = 3;
  private:
  bool has_zero_data() const;
  public:
  void clear_zero_data();
  static const int kZeroDataFieldNumber = 3;
  bool zero_data() const;
  void set_zero_data(bool value);

  // bool random_data = 4;
  private:
  bool has_random_data() const;
  public:
  void clear_random_data();
  static const int kRandomDataFieldNumber = 4;
  bool random_data() const;
  void set_random_data(bool value);

  // string input_data_file = 5;
  private:
  bool has_input_data_file() const;
  public:
  void clear_input_data_file();
  static const int kInputDataFileFieldNumber = 5;
  const ::std::string& input_data_file() const;
  void set_input_data_file(const ::std::string& value);
  #if LANG_CXX11
  void set_input_data_file(::std::string&& value);
  #endif
  void set_input_data_file(const char* value);
  void set_input_data_file(const char* value, size_t size);
  ::std::string* mutable_input_data_file();
  ::std::string* release_input_data_file();
  void set_allocated_input_data_file(::std::string* input_data_file);

  void clear_input_data_type();
  InputDataTypeCase input_data_type_case() const;
  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelWarmup.Input)
 private:
  void set_has_zero_data();
  void set_has_random_data();
  void set_has_input_data_file();

  inline bool has_input_data_type() const;
  inline void clear_has_input_data_type();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > dims_;
  mutable int _dims_cached_byte_size_;
  int data_type_;
  union InputDataTypeUnion {
    InputDataTypeUnion() {}
    bool zero_data_;
    bool random_data_;
    ::google::protobuf::internal::ArenaStringPtr input_data_file_;
  } input_data_type_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelWarmup_InputsEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelWarmup_InputsEntry_DoNotUse, 
    ::std::string, ::nvidia::inferenceserver::ModelWarmup_Input,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelWarmup_InputsEntry_DoNotUse, 
    ::std::string, ::nvidia::inferenceserver::ModelWarmup_Input,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > SuperType;
  ModelWarmup_InputsEntry_DoNotUse();
  ModelWarmup_InputsEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelWarmup_InputsEntry_DoNotUse& other);
  static const ModelWarmup_InputsEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelWarmup_InputsEntry_DoNotUse*>(&_ModelWarmup_InputsEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelWarmup : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelWarmup) */ {
 public:
  ModelWarmup();
  virtual ~ModelWarmup();

  ModelWarmup(const ModelWarmup& from);

  inline ModelWarmup& operator=(const ModelWarmup& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelWarmup(ModelWarmup&& from) noexcept
    : ModelWarmup() {
    *this = ::std::move(from);
  }

  inline ModelWarmup& operator=(ModelWarmup&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelWarmup& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelWarmup* internal_default_instance() {
    return reinterpret_cast<const ModelWarmup*>(
               &_ModelWarmup_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    30;

  void Swap(ModelWarmup* other);
  friend void swap(ModelWarmup& a, ModelWarmup& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelWarmup* New() const final {
    return CreateMaybeMessage<ModelWarmup>(NULL);
  }

  ModelWarmup* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelWarmup>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelWarmup& from);
  void MergeFrom(const ModelWarmup& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelWarmup* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelWarmup_Input Input;

  // accessors -------------------------------------------------------

  // map<string, .nvidia.inferenceserver.ModelWarmup.Input> inputs = 3;
  int inputs_size() const;
  void clear_inputs();
  static const int kInputsFieldNumber = 3;
  const ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelWarmup_Input >&
      inputs() const;
  ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelWarmup_Input >*
      mutable_inputs();

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // uint32 batch_size = 2;
  void clear_batch_size();
  static const int kBatchSizeFieldNumber = 2;
  ::google::protobuf::uint32 batch_size() const;
  void set_batch_size(::google::protobuf::uint32 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelWarmup)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::MapField<
      ModelWarmup_InputsEntry_DoNotUse,
      ::std::string, ::nvidia::inferenceserver::ModelWarmup_Input,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 > inputs_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  ::google::protobuf::uint32 batch_size_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelConfig_CcModelFilenamesEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelConfig_CcModelFilenamesEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelConfig_CcModelFilenamesEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > SuperType;
  ModelConfig_CcModelFilenamesEntry_DoNotUse();
  ModelConfig_CcModelFilenamesEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelConfig_CcModelFilenamesEntry_DoNotUse& other);
  static const ModelConfig_CcModelFilenamesEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelConfig_CcModelFilenamesEntry_DoNotUse*>(&_ModelConfig_CcModelFilenamesEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelConfig_MetricTagsEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelConfig_MetricTagsEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelConfig_MetricTagsEntry_DoNotUse, 
    ::std::string, ::std::string,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    0 > SuperType;
  ModelConfig_MetricTagsEntry_DoNotUse();
  ModelConfig_MetricTagsEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelConfig_MetricTagsEntry_DoNotUse& other);
  static const ModelConfig_MetricTagsEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelConfig_MetricTagsEntry_DoNotUse*>(&_ModelConfig_MetricTagsEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelConfig_ParametersEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelConfig_ParametersEntry_DoNotUse, 
    ::std::string, ::nvidia::inferenceserver::ModelParameter,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelConfig_ParametersEntry_DoNotUse, 
    ::std::string, ::nvidia::inferenceserver::ModelParameter,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > SuperType;
  ModelConfig_ParametersEntry_DoNotUse();
  ModelConfig_ParametersEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelConfig_ParametersEntry_DoNotUse& other);
  static const ModelConfig_ParametersEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelConfig_ParametersEntry_DoNotUse*>(&_ModelConfig_ParametersEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelConfig : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelConfig) */ {
 public:
  ModelConfig();
  virtual ~ModelConfig();

  ModelConfig(const ModelConfig& from);

  inline ModelConfig& operator=(const ModelConfig& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelConfig(ModelConfig&& from) noexcept
    : ModelConfig() {
    *this = ::std::move(from);
  }

  inline ModelConfig& operator=(ModelConfig&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelConfig& default_instance();

  enum SchedulingChoiceCase {
    kDynamicBatching = 11,
    kSequenceBatching = 13,
    kEnsembleScheduling = 15,
    SCHEDULING_CHOICE_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelConfig* internal_default_instance() {
    return reinterpret_cast<const ModelConfig*>(
               &_ModelConfig_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    34;

  void Swap(ModelConfig* other);
  friend void swap(ModelConfig& a, ModelConfig& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelConfig* New() const final {
    return CreateMaybeMessage<ModelConfig>(NULL);
  }

  ModelConfig* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelConfig>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelConfig& from);
  void MergeFrom(const ModelConfig& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelConfig* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  // repeated .nvidia.inferenceserver.ModelInput input = 5;
  int input_size() const;
  void clear_input();
  static const int kInputFieldNumber = 5;
  ::nvidia::inferenceserver::ModelInput* mutable_input(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >*
      mutable_input();
  const ::nvidia::inferenceserver::ModelInput& input(int index) const;
  ::nvidia::inferenceserver::ModelInput* add_input();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >&
      input() const;

  // repeated .nvidia.inferenceserver.ModelOutput output = 6;
  int output_size() const;
  void clear_output();
  static const int kOutputFieldNumber = 6;
  ::nvidia::inferenceserver::ModelOutput* mutable_output(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >*
      mutable_output();
  const ::nvidia::inferenceserver::ModelOutput& output(int index) const;
  ::nvidia::inferenceserver::ModelOutput* add_output();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >&
      output() const;

  // repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;
  int instance_group_size() const;
  void clear_instance_group();
  static const int kInstanceGroupFieldNumber = 7;
  ::nvidia::inferenceserver::ModelInstanceGroup* mutable_instance_group(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >*
      mutable_instance_group();
  const ::nvidia::inferenceserver::ModelInstanceGroup& instance_group(int index) const;
  ::nvidia::inferenceserver::ModelInstanceGroup* add_instance_group();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >&
      instance_group() const;

  // map<string, string> cc_model_filenames = 9;
  int cc_model_filenames_size() const;
  void clear_cc_model_filenames();
  static const int kCcModelFilenamesFieldNumber = 9;
  const ::google::protobuf::Map< ::std::string, ::std::string >&
      cc_model_filenames() const;
  ::google::protobuf::Map< ::std::string, ::std::string >*
      mutable_cc_model_filenames();

  // map<string, string> metric_tags = 10;
  int metric_tags_size() const;
  void clear_metric_tags();
  static const int kMetricTagsFieldNumber = 10;
  const ::google::protobuf::Map< ::std::string, ::std::string >&
      metric_tags() const;
  ::google::protobuf::Map< ::std::string, ::std::string >*
      mutable_metric_tags();

  // map<string, .nvidia.inferenceserver.ModelParameter> parameters = 14;
  int parameters_size() const;
  void clear_parameters();
  static const int kParametersFieldNumber = 14;
  const ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelParameter >&
      parameters() const;
  ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelParameter >*
      mutable_parameters();

  // repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;
  int model_warmup_size() const;
  void clear_model_warmup();
  static const int kModelWarmupFieldNumber = 16;
  ::nvidia::inferenceserver::ModelWarmup* mutable_model_warmup(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelWarmup >*
      mutable_model_warmup();
  const ::nvidia::inferenceserver::ModelWarmup& model_warmup(int index) const;
  ::nvidia::inferenceserver::ModelWarmup* add_model_warmup();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelWarmup >&
      model_warmup() const;

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // string platform = 2;
  void clear_platform();
  static const int kPlatformFieldNumber = 2;
  const ::std::string& platform() const;
  void set_platform(const ::std::string& value);
  #if LANG_CXX11
  void set_platform(::std::string&& value);
  #endif
  void set_platform(const char* value);
  void set_platform(const char* value, size_t size);
  ::std::string* mutable_platform();
  ::std::string* release_platform();
  void set_allocated_platform(::std::string* platform);

  // string default_model_filename = 8;
  void clear_default_model_filename();
  static const int kDefaultModelFilenameFieldNumber = 8;
  const ::std::string& default_model_filename() const;
  void set_default_model_filename(const ::std::string& value);
  #if LANG_CXX11
  void set_default_model_filename(::std::string&& value);
  #endif
  void set_default_model_filename(const char* value);
  void set_default_model_filename(const char* value, size_t size);
  ::std::string* mutable_default_model_filename();
  ::std::string* release_default_model_filename();
  void set_allocated_default_model_filename(::std::string* default_model_filename);

  // .nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;
  bool has_version_policy() const;
  void clear_version_policy();
  static const int kVersionPolicyFieldNumber = 3;
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy& _internal_version_policy() const;
  public:
  const ::nvidia::inferenceserver::ModelVersionPolicy& version_policy() const;
  ::nvidia::inferenceserver::ModelVersionPolicy* release_version_policy();
  ::nvidia::inferenceserver::ModelVersionPolicy* mutable_version_policy();
  void set_allocated_version_policy(::nvidia::inferenceserver::ModelVersionPolicy* version_policy);

  // .nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;
  bool has_optimization() const;
  void clear_optimization();
  static const int kOptimizationFieldNumber = 12;
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy& _internal_optimization() const;
  public:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy& optimization() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy* release_optimization();
  ::nvidia::inferenceserver::ModelOptimizationPolicy* mutable_optimization();
  void set_allocated_optimization(::nvidia::inferenceserver::ModelOptimizationPolicy* optimization);

  // int32 max_batch_size = 4;
  void clear_max_batch_size();
  static const int kMaxBatchSizeFieldNumber = 4;
  ::google::protobuf::int32 max_batch_size() const;
  void set_max_batch_size(::google::protobuf::int32 value);

  // .nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;
  bool has_dynamic_batching() const;
  void clear_dynamic_batching();
  static const int kDynamicBatchingFieldNumber = 11;
  private:
  const ::nvidia::inferenceserver::ModelDynamicBatching& _internal_dynamic_batching() const;
  public:
  const ::nvidia::inferenceserver::ModelDynamicBatching& dynamic_batching() const;
  ::nvidia::inferenceserver::ModelDynamicBatching* release_dynamic_batching();
  ::nvidia::inferenceserver::ModelDynamicBatching* mutable_dynamic_batching();
  void set_allocated_dynamic_batching(::nvidia::inferenceserver::ModelDynamicBatching* dynamic_batching);

  // .nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;
  bool has_sequence_batching() const;
  void clear_sequence_batching();
  static const int kSequenceBatchingFieldNumber = 13;
  private:
  const ::nvidia::inferenceserver::ModelSequenceBatching& _internal_sequence_batching() const;
  public:
  const ::nvidia::inferenceserver::ModelSequenceBatching& sequence_batching() const;
  ::nvidia::inferenceserver::ModelSequenceBatching* release_sequence_batching();
  ::nvidia::inferenceserver::ModelSequenceBatching* mutable_sequence_batching();
  void set_allocated_sequence_batching(::nvidia::inferenceserver::ModelSequenceBatching* sequence_batching);

  // .nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;
  bool has_ensemble_scheduling() const;
  void clear_ensemble_scheduling();
  static const int kEnsembleSchedulingFieldNumber = 15;
  private:
  const ::nvidia::inferenceserver::ModelEnsembling& _internal_ensemble_scheduling() const;
  public:
  const ::nvidia::inferenceserver::ModelEnsembling& ensemble_scheduling() const;
  ::nvidia::inferenceserver::ModelEnsembling* release_ensemble_scheduling();
  ::nvidia::inferenceserver::ModelEnsembling* mutable_ensemble_scheduling();
  void set_allocated_ensemble_scheduling(::nvidia::inferenceserver::ModelEnsembling* ensemble_scheduling);

  void clear_scheduling_choice();
  SchedulingChoiceCase scheduling_choice_case() const;
  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig)
 private:
  void set_has_dynamic_batching();
  void set_has_sequence_batching();
  void set_has_ensemble_scheduling();

  inline bool has_scheduling_choice() const;
  inline void clear_has_scheduling_choice();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput > input_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput > output_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup > instance_group_;
  ::google::protobuf::internal::MapField<
      ModelConfig_CcModelFilenamesEntry_DoNotUse,
      ::std::string, ::std::string,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      0 > cc_model_filenames_;
  ::google::protobuf::internal::MapField<
      ModelConfig_MetricTagsEntry_DoNotUse,
      ::std::string, ::std::string,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      0 > metric_tags_;
  ::google::protobuf::internal::MapField<
      ModelConfig_ParametersEntry_DoNotUse,
      ::std::string, ::nvidia::inferenceserver::ModelParameter,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 > parameters_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelWarmup > model_warmup_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  ::google::protobuf::internal::ArenaStringPtr platform_;
  ::google::protobuf::internal::ArenaStringPtr default_model_filename_;
  ::nvidia::inferenceserver::ModelVersionPolicy* version_policy_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy* optimization_;
  ::google::protobuf::int32 max_batch_size_;
  union SchedulingChoiceUnion {
    SchedulingChoiceUnion() {}
    ::nvidia::inferenceserver::ModelDynamicBatching* dynamic_batching_;
    ::nvidia::inferenceserver::ModelSequenceBatching* sequence_batching_;
    ::nvidia::inferenceserver::ModelEnsembling* ensemble_scheduling_;
  } scheduling_choice_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_model_5fconfig_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// ModelInstanceGroup

// string name = 1;
inline void ModelInstanceGroup::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelInstanceGroup::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.name)
  return name_.GetNoArena();
}
inline void ModelInstanceGroup::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.name)
}
#if LANG_CXX11
inline void ModelInstanceGroup::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelInstanceGroup.name)
}
#endif
inline void ModelInstanceGroup::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelInstanceGroup.name)
}
inline void ModelInstanceGroup::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelInstanceGroup.name)
}
inline ::std::string* ModelInstanceGroup::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelInstanceGroup.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelInstanceGroup::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelInstanceGroup.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelInstanceGroup::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelInstanceGroup.name)
}

// .nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;
inline void ModelInstanceGroup::clear_kind() {
  kind_ = 0;
}
inline ::nvidia::inferenceserver::ModelInstanceGroup_Kind ModelInstanceGroup::kind() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.kind)
  return static_cast< ::nvidia::inferenceserver::ModelInstanceGroup_Kind >(kind_);
}
inline void ModelInstanceGroup::set_kind(::nvidia::inferenceserver::ModelInstanceGroup_Kind value) {
  
  kind_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.kind)
}

// int32 count = 2;
inline void ModelInstanceGroup::clear_count() {
  count_ = 0;
}
inline ::google::protobuf::int32 ModelInstanceGroup::count() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.count)
  return count_;
}
inline void ModelInstanceGroup::set_count(::google::protobuf::int32 value) {
  
  count_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.count)
}

// repeated int32 gpus = 3;
inline int ModelInstanceGroup::gpus_size() const {
  return gpus_.size();
}
inline void ModelInstanceGroup::clear_gpus() {
  gpus_.Clear();
}
inline ::google::protobuf::int32 ModelInstanceGroup::gpus(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.gpus)
  return gpus_.Get(index);
}
inline void ModelInstanceGroup::set_gpus(int index, ::google::protobuf::int32 value) {
  gpus_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.gpus)
}
inline void ModelInstanceGroup::add_gpus(::google::protobuf::int32 value) {
  gpus_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelInstanceGroup.gpus)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
ModelInstanceGroup::gpus() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelInstanceGroup.gpus)
  return gpus_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
ModelInstanceGroup::mutable_gpus() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelInstanceGroup.gpus)
  return &gpus_;
}

// repeated string profile = 5;
inline int ModelInstanceGroup::profile_size() const {
  return profile_.size();
}
inline void ModelInstanceGroup::clear_profile() {
  profile_.Clear();
}
inline const ::std::string& ModelInstanceGroup::profile(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.profile)
  return profile_.Get(index);
}
inline ::std::string* ModelInstanceGroup::mutable_profile(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelInstanceGroup.profile)
  return profile_.Mutable(index);
}
inline void ModelInstanceGroup::set_profile(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.profile)
  profile_.Mutable(index)->assign(value);
}
#if LANG_CXX11
inline void ModelInstanceGroup::set_profile(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.profile)
  profile_.Mutable(index)->assign(std::move(value));
}
#endif
inline void ModelInstanceGroup::set_profile(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  profile_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelInstanceGroup.profile)
}
inline void ModelInstanceGroup::set_profile(int index, const char* value, size_t size) {
  profile_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelInstanceGroup.profile)
}
inline ::std::string* ModelInstanceGroup::add_profile() {
  // @@protoc_insertion_point(field_add_mutable:nvidia.inferenceserver.ModelInstanceGroup.profile)
  return profile_.Add();
}
inline void ModelInstanceGroup::add_profile(const ::std::string& value) {
  profile_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelInstanceGroup.profile)
}
#if LANG_CXX11
inline void ModelInstanceGroup::add_profile(::std::string&& value) {
  profile_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelInstanceGroup.profile)
}
#endif
inline void ModelInstanceGroup::add_profile(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  profile_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:nvidia.inferenceserver.ModelInstanceGroup.profile)
}
inline void ModelInstanceGroup::add_profile(const char* value, size_t size) {
  profile_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:nvidia.inferenceserver.ModelInstanceGroup.profile)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
ModelInstanceGroup::profile() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelInstanceGroup.profile)
  return profile_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
ModelInstanceGroup::mutable_profile() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelInstanceGroup.profile)
  return &profile_;
}

// -------------------------------------------------------------------

// ModelTensorReshape

// repeated int64 shape = 1;
inline int ModelTensorReshape::shape_size() const {
  return shape_.size();
}
inline void ModelTensorReshape::clear_shape() {
  shape_.Clear();
}
inline ::google::protobuf::int64 ModelTensorReshape::shape(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelTensorReshape.shape)
  return shape_.Get(index);
}
inline void ModelTensorReshape::set_shape(int index, ::google::protobuf::int64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelTensorReshape.shape)
}
inline void ModelTensorReshape::add_shape(::google::protobuf::int64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelTensorReshape.shape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ModelTensorReshape::shape() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelTensorReshape.shape)
  return shape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ModelTensorReshape::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelTensorReshape.shape)
  return &shape_;
}

// -------------------------------------------------------------------

// ModelInput

// string name = 1;
inline void ModelInput::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelInput::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.name)
  return name_.GetNoArena();
}
inline void ModelInput::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.name)
}
#if LANG_CXX11
inline void ModelInput::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelInput.name)
}
#endif
inline void ModelInput::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelInput.name)
}
inline void ModelInput::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelInput.name)
}
inline ::std::string* ModelInput::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelInput.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelInput::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelInput.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelInput::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelInput.name)
}

// .nvidia.inferenceserver.DataType data_type = 2;
inline void ModelInput::clear_data_type() {
  data_type_ = 0;
}
inline ::nvidia::inferenceserver::DataType ModelInput::data_type() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.data_type)
  return static_cast< ::nvidia::inferenceserver::DataType >(data_type_);
}
inline void ModelInput::set_data_type(::nvidia::inferenceserver::DataType value) {
  
  data_type_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.data_type)
}

// .nvidia.inferenceserver.ModelInput.Format format = 3;
inline void ModelInput::clear_format() {
  format_ = 0;
}
inline ::nvidia::inferenceserver::ModelInput_Format ModelInput::format() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.format)
  return static_cast< ::nvidia::inferenceserver::ModelInput_Format >(format_);
}
inline void ModelInput::set_format(::nvidia::inferenceserver::ModelInput_Format value) {
  
  format_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.format)
}

// repeated int64 dims = 4;
inline int ModelInput::dims_size() const {
  return dims_.size();
}
inline void ModelInput::clear_dims() {
  dims_.Clear();
}
inline ::google::protobuf::int64 ModelInput::dims(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.dims)
  return dims_.Get(index);
}
inline void ModelInput::set_dims(int index, ::google::protobuf::int64 value) {
  dims_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.dims)
}
inline void ModelInput::add_dims(::google::protobuf::int64 value) {
  dims_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelInput.dims)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ModelInput::dims() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelInput.dims)
  return dims_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ModelInput::mutable_dims() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelInput.dims)
  return &dims_;
}

// .nvidia.inferenceserver.ModelTensorReshape reshape = 5;
inline bool ModelInput::has_reshape() const {
  return this != internal_default_instance() && reshape_ != NULL;
}
inline void ModelInput::clear_reshape() {
  if (GetArenaNoVirtual() == NULL && reshape_ != NULL) {
    delete reshape_;
  }
  reshape_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelTensorReshape& ModelInput::_internal_reshape() const {
  return *reshape_;
}
inline const ::nvidia::inferenceserver::ModelTensorReshape& ModelInput::reshape() const {
  const ::nvidia::inferenceserver::ModelTensorReshape* p = reshape_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.reshape)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelTensorReshape*>(
      &::nvidia::inferenceserver::_ModelTensorReshape_default_instance_);
}
inline ::nvidia::inferenceserver::ModelTensorReshape* ModelInput::release_reshape() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelInput.reshape)
  
  ::nvidia::inferenceserver::ModelTensorReshape* temp = reshape_;
  reshape_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelTensorReshape* ModelInput::mutable_reshape() {
  
  if (reshape_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelTensorReshape>(GetArenaNoVirtual());
    reshape_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelInput.reshape)
  return reshape_;
}
inline void ModelInput::set_allocated_reshape(::nvidia::inferenceserver::ModelTensorReshape* reshape) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reshape_;
  }
  if (reshape) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      reshape = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, reshape, submessage_arena);
    }
    
  } else {
    
  }
  reshape_ = reshape;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelInput.reshape)
}

// bool is_shape_tensor = 6;
inline void ModelInput::clear_is_shape_tensor() {
  is_shape_tensor_ = false;
}
inline bool ModelInput::is_shape_tensor() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.is_shape_tensor)
  return is_shape_tensor_;
}
inline void ModelInput::set_is_shape_tensor(bool value) {
  
  is_shape_tensor_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.is_shape_tensor)
}

// bool allow_ragged_batch = 7;
inline void ModelInput::clear_allow_ragged_batch() {
  allow_ragged_batch_ = false;
}
inline bool ModelInput::allow_ragged_batch() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.allow_ragged_batch)
  return allow_ragged_batch_;
}
inline void ModelInput::set_allow_ragged_batch(bool value) {
  
  allow_ragged_batch_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.allow_ragged_batch)
}

// -------------------------------------------------------------------

// ModelOutput

// string name = 1;
inline void ModelOutput::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelOutput::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.name)
  return name_.GetNoArena();
}
inline void ModelOutput::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.name)
}
#if LANG_CXX11
inline void ModelOutput::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelOutput.name)
}
#endif
inline void ModelOutput::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelOutput.name)
}
inline void ModelOutput::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelOutput.name)
}
inline ::std::string* ModelOutput::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOutput.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelOutput::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOutput.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelOutput::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOutput.name)
}

// .nvidia.inferenceserver.DataType data_type = 2;
inline void ModelOutput::clear_data_type() {
  data_type_ = 0;
}
inline ::nvidia::inferenceserver::DataType ModelOutput::data_type() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.data_type)
  return static_cast< ::nvidia::inferenceserver::DataType >(data_type_);
}
inline void ModelOutput::set_data_type(::nvidia::inferenceserver::DataType value) {
  
  data_type_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.data_type)
}

// repeated int64 dims = 3;
inline int ModelOutput::dims_size() const {
  return dims_.size();
}
inline void ModelOutput::clear_dims() {
  dims_.Clear();
}
inline ::google::protobuf::int64 ModelOutput::dims(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.dims)
  return dims_.Get(index);
}
inline void ModelOutput::set_dims(int index, ::google::protobuf::int64 value) {
  dims_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.dims)
}
inline void ModelOutput::add_dims(::google::protobuf::int64 value) {
  dims_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelOutput.dims)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ModelOutput::dims() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelOutput.dims)
  return dims_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ModelOutput::mutable_dims() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelOutput.dims)
  return &dims_;
}

// .nvidia.inferenceserver.ModelTensorReshape reshape = 5;
inline bool ModelOutput::has_reshape() const {
  return this != internal_default_instance() && reshape_ != NULL;
}
inline void ModelOutput::clear_reshape() {
  if (GetArenaNoVirtual() == NULL && reshape_ != NULL) {
    delete reshape_;
  }
  reshape_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelTensorReshape& ModelOutput::_internal_reshape() const {
  return *reshape_;
}
inline const ::nvidia::inferenceserver::ModelTensorReshape& ModelOutput::reshape() const {
  const ::nvidia::inferenceserver::ModelTensorReshape* p = reshape_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.reshape)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelTensorReshape*>(
      &::nvidia::inferenceserver::_ModelTensorReshape_default_instance_);
}
inline ::nvidia::inferenceserver::ModelTensorReshape* ModelOutput::release_reshape() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOutput.reshape)
  
  ::nvidia::inferenceserver::ModelTensorReshape* temp = reshape_;
  reshape_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelTensorReshape* ModelOutput::mutable_reshape() {
  
  if (reshape_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelTensorReshape>(GetArenaNoVirtual());
    reshape_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOutput.reshape)
  return reshape_;
}
inline void ModelOutput::set_allocated_reshape(::nvidia::inferenceserver::ModelTensorReshape* reshape) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reshape_;
  }
  if (reshape) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      reshape = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, reshape, submessage_arena);
    }
    
  } else {
    
  }
  reshape_ = reshape;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOutput.reshape)
}

// string label_filename = 4;
inline void ModelOutput::clear_label_filename() {
  label_filename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelOutput::label_filename() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.label_filename)
  return label_filename_.GetNoArena();
}
inline void ModelOutput::set_label_filename(const ::std::string& value) {
  
  label_filename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.label_filename)
}
#if LANG_CXX11
inline void ModelOutput::set_label_filename(::std::string&& value) {
  
  label_filename_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelOutput.label_filename)
}
#endif
inline void ModelOutput::set_label_filename(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  label_filename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelOutput.label_filename)
}
inline void ModelOutput::set_label_filename(const char* value, size_t size) {
  
  label_filename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelOutput.label_filename)
}
inline ::std::string* ModelOutput::mutable_label_filename() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOutput.label_filename)
  return label_filename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelOutput::release_label_filename() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOutput.label_filename)
  
  return label_filename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelOutput::set_allocated_label_filename(::std::string* label_filename) {
  if (label_filename != NULL) {
    
  } else {
    
  }
  label_filename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), label_filename);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOutput.label_filename)
}

// bool is_shape_tensor = 6;
inline void ModelOutput::clear_is_shape_tensor() {
  is_shape_tensor_ = false;
}
inline bool ModelOutput::is_shape_tensor() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.is_shape_tensor)
  return is_shape_tensor_;
}
inline void ModelOutput::set_is_shape_tensor(bool value) {
  
  is_shape_tensor_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.is_shape_tensor)
}

// -------------------------------------------------------------------

// ModelVersionPolicy_Latest

// uint32 num_versions = 1;
inline void ModelVersionPolicy_Latest::clear_num_versions() {
  num_versions_ = 0u;
}
inline ::google::protobuf::uint32 ModelVersionPolicy_Latest::num_versions() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.Latest.num_versions)
  return num_versions_;
}
inline void ModelVersionPolicy_Latest::set_num_versions(::google::protobuf::uint32 value) {
  
  num_versions_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionPolicy.Latest.num_versions)
}

// -------------------------------------------------------------------

// ModelVersionPolicy_All

// -------------------------------------------------------------------

// ModelVersionPolicy_Specific

// repeated int64 versions = 1;
inline int ModelVersionPolicy_Specific::versions_size() const {
  return versions_.size();
}
inline void ModelVersionPolicy_Specific::clear_versions() {
  versions_.Clear();
}
inline ::google::protobuf::int64 ModelVersionPolicy_Specific::versions(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
  return versions_.Get(index);
}
inline void ModelVersionPolicy_Specific::set_versions(int index, ::google::protobuf::int64 value) {
  versions_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
}
inline void ModelVersionPolicy_Specific::add_versions(::google::protobuf::int64 value) {
  versions_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ModelVersionPolicy_Specific::versions() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
  return versions_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ModelVersionPolicy_Specific::mutable_versions() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
  return &versions_;
}

// -------------------------------------------------------------------

// ModelVersionPolicy

// .nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;
inline bool ModelVersionPolicy::has_latest() const {
  return policy_choice_case() == kLatest;
}
inline void ModelVersionPolicy::set_has_latest() {
  _oneof_case_[0] = kLatest;
}
inline void ModelVersionPolicy::clear_latest() {
  if (has_latest()) {
    delete policy_choice_.latest_;
    clear_has_policy_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& ModelVersionPolicy::_internal_latest() const {
  return *policy_choice_.latest_;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Latest* ModelVersionPolicy::release_latest() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelVersionPolicy.latest)
  if (has_latest()) {
    clear_has_policy_choice();
      ::nvidia::inferenceserver::ModelVersionPolicy_Latest* temp = policy_choice_.latest_;
    policy_choice_.latest_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& ModelVersionPolicy::latest() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.latest)
  return has_latest()
      ? *policy_choice_.latest_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelVersionPolicy_Latest*>(&::nvidia::inferenceserver::_ModelVersionPolicy_Latest_default_instance_);
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Latest* ModelVersionPolicy::mutable_latest() {
  if (!has_latest()) {
    clear_policy_choice();
    set_has_latest();
    policy_choice_.latest_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionPolicy_Latest >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelVersionPolicy.latest)
  return policy_choice_.latest_;
}

// .nvidia.inferenceserver.ModelVersionPolicy.All all = 2;
inline bool ModelVersionPolicy::has_all() const {
  return policy_choice_case() == kAll;
}
inline void ModelVersionPolicy::set_has_all() {
  _oneof_case_[0] = kAll;
}
inline void ModelVersionPolicy::clear_all() {
  if (has_all()) {
    delete policy_choice_.all_;
    clear_has_policy_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_All& ModelVersionPolicy::_internal_all() const {
  return *policy_choice_.all_;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_All* ModelVersionPolicy::release_all() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelVersionPolicy.all)
  if (has_all()) {
    clear_has_policy_choice();
      ::nvidia::inferenceserver::ModelVersionPolicy_All* temp = policy_choice_.all_;
    policy_choice_.all_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_All& ModelVersionPolicy::all() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.all)
  return has_all()
      ? *policy_choice_.all_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelVersionPolicy_All*>(&::nvidia::inferenceserver::_ModelVersionPolicy_All_default_instance_);
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_All* ModelVersionPolicy::mutable_all() {
  if (!has_all()) {
    clear_policy_choice();
    set_has_all();
    policy_choice_.all_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionPolicy_All >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelVersionPolicy.all)
  return policy_choice_.all_;
}

// .nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;
inline bool ModelVersionPolicy::has_specific() const {
  return policy_choice_case() == kSpecific;
}
inline void ModelVersionPolicy::set_has_specific() {
  _oneof_case_[0] = kSpecific;
}
inline void ModelVersionPolicy::clear_specific() {
  if (has_specific()) {
    delete policy_choice_.specific_;
    clear_has_policy_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& ModelVersionPolicy::_internal_specific() const {
  return *policy_choice_.specific_;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Specific* ModelVersionPolicy::release_specific() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelVersionPolicy.specific)
  if (has_specific()) {
    clear_has_policy_choice();
      ::nvidia::inferenceserver::ModelVersionPolicy_Specific* temp = policy_choice_.specific_;
    policy_choice_.specific_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& ModelVersionPolicy::specific() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.specific)
  return has_specific()
      ? *policy_choice_.specific_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelVersionPolicy_Specific*>(&::nvidia::inferenceserver::_ModelVersionPolicy_Specific_default_instance_);
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Specific* ModelVersionPolicy::mutable_specific() {
  if (!has_specific()) {
    clear_policy_choice();
    set_has_specific();
    policy_choice_.specific_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionPolicy_Specific >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelVersionPolicy.specific)
  return policy_choice_.specific_;
}

inline bool ModelVersionPolicy::has_policy_choice() const {
  return policy_choice_case() != POLICY_CHOICE_NOT_SET;
}
inline void ModelVersionPolicy::clear_has_policy_choice() {
  _oneof_case_[0] = POLICY_CHOICE_NOT_SET;
}
inline ModelVersionPolicy::PolicyChoiceCase ModelVersionPolicy::policy_choice_case() const {
  return ModelVersionPolicy::PolicyChoiceCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// ModelOptimizationPolicy_Graph

// int32 level = 1;
inline void ModelOptimizationPolicy_Graph::clear_level() {
  level_ = 0;
}
inline ::google::protobuf::int32 ModelOptimizationPolicy_Graph::level() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.Graph.level)
  return level_;
}
inline void ModelOptimizationPolicy_Graph::set_level(::google::protobuf::int32 value) {
  
  level_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOptimizationPolicy.Graph.level)
}

// -------------------------------------------------------------------

// ModelOptimizationPolicy_Cuda

// bool graphs = 1;
inline void ModelOptimizationPolicy_Cuda::clear_graphs() {
  graphs_ = false;
}
inline bool ModelOptimizationPolicy_Cuda::graphs() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda.graphs)
  return graphs_;
}
inline void ModelOptimizationPolicy_Cuda::set_graphs(bool value) {
  
  graphs_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda.graphs)
}

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelOptimizationPolicy_ExecutionAccelerators_Accelerator

// string name = 1;
inline void ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
  return name_.GetNoArena();
}
inline void ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
}
#if LANG_CXX11
inline void ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
}
#endif
inline void ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
}
inline void ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
}
inline ::std::string* ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name)
}

// map<string, string> parameters = 2;
inline int ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::parameters_size() const {
  return parameters_.size();
}
inline void ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::clear_parameters() {
  parameters_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::std::string >&
ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::parameters() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parameters)
  return parameters_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::std::string >*
ModelOptimizationPolicy_ExecutionAccelerators_Accelerator::mutable_parameters() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parameters)
  return parameters_.MutableMap();
}

// -------------------------------------------------------------------

// ModelOptimizationPolicy_ExecutionAccelerators

// repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;
inline int ModelOptimizationPolicy_ExecutionAccelerators::gpu_execution_accelerator_size() const {
  return gpu_execution_accelerator_.size();
}
inline void ModelOptimizationPolicy_ExecutionAccelerators::clear_gpu_execution_accelerator() {
  gpu_execution_accelerator_.Clear();
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* ModelOptimizationPolicy_ExecutionAccelerators::mutable_gpu_execution_accelerator(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator)
  return gpu_execution_accelerator_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >*
ModelOptimizationPolicy_ExecutionAccelerators::mutable_gpu_execution_accelerator() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator)
  return &gpu_execution_accelerator_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& ModelOptimizationPolicy_ExecutionAccelerators::gpu_execution_accelerator(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator)
  return gpu_execution_accelerator_.Get(index);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* ModelOptimizationPolicy_ExecutionAccelerators::add_gpu_execution_accelerator() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator)
  return gpu_execution_accelerator_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >&
ModelOptimizationPolicy_ExecutionAccelerators::gpu_execution_accelerator() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator)
  return gpu_execution_accelerator_;
}

// repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;
inline int ModelOptimizationPolicy_ExecutionAccelerators::cpu_execution_accelerator_size() const {
  return cpu_execution_accelerator_.size();
}
inline void ModelOptimizationPolicy_ExecutionAccelerators::clear_cpu_execution_accelerator() {
  cpu_execution_accelerator_.Clear();
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* ModelOptimizationPolicy_ExecutionAccelerators::mutable_cpu_execution_accelerator(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator)
  return cpu_execution_accelerator_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >*
ModelOptimizationPolicy_ExecutionAccelerators::mutable_cpu_execution_accelerator() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator)
  return &cpu_execution_accelerator_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator& ModelOptimizationPolicy_ExecutionAccelerators::cpu_execution_accelerator(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator)
  return cpu_execution_accelerator_.Get(index);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator* ModelOptimizationPolicy_ExecutionAccelerators::add_cpu_execution_accelerator() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator)
  return cpu_execution_accelerator_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators_Accelerator >&
ModelOptimizationPolicy_ExecutionAccelerators::cpu_execution_accelerator() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator)
  return cpu_execution_accelerator_;
}

// -------------------------------------------------------------------

// ModelOptimizationPolicy_PinnedMemoryBuffer

// bool enable = 1;
inline void ModelOptimizationPolicy_PinnedMemoryBuffer::clear_enable() {
  enable_ = false;
}
inline bool ModelOptimizationPolicy_PinnedMemoryBuffer::enable() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer.enable)
  return enable_;
}
inline void ModelOptimizationPolicy_PinnedMemoryBuffer::set_enable(bool value) {
  
  enable_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer.enable)
}

// -------------------------------------------------------------------

// ModelOptimizationPolicy

// .nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;
inline bool ModelOptimizationPolicy::has_graph() const {
  return this != internal_default_instance() && graph_ != NULL;
}
inline void ModelOptimizationPolicy::clear_graph() {
  if (GetArenaNoVirtual() == NULL && graph_ != NULL) {
    delete graph_;
  }
  graph_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& ModelOptimizationPolicy::_internal_graph() const {
  return *graph_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& ModelOptimizationPolicy::graph() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* p = graph_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph*>(
      &::nvidia::inferenceserver::_ModelOptimizationPolicy_Graph_default_instance_);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* ModelOptimizationPolicy::release_graph() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* temp = graph_;
  graph_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* ModelOptimizationPolicy::mutable_graph() {
  
  if (graph_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_Graph>(GetArenaNoVirtual());
    graph_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
  return graph_;
}
inline void ModelOptimizationPolicy::set_allocated_graph(::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete graph_;
  }
  if (graph) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      graph = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, graph, submessage_arena);
    }
    
  } else {
    
  }
  graph_ = graph;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
}

// .nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;
inline void ModelOptimizationPolicy::clear_priority() {
  priority_ = 0;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority ModelOptimizationPolicy::priority() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.priority)
  return static_cast< ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority >(priority_);
}
inline void ModelOptimizationPolicy::set_priority(::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority value) {
  
  priority_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOptimizationPolicy.priority)
}

// .nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;
inline bool ModelOptimizationPolicy::has_cuda() const {
  return this != internal_default_instance() && cuda_ != NULL;
}
inline void ModelOptimizationPolicy::clear_cuda() {
  if (GetArenaNoVirtual() == NULL && cuda_ != NULL) {
    delete cuda_;
  }
  cuda_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda& ModelOptimizationPolicy::_internal_cuda() const {
  return *cuda_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda& ModelOptimizationPolicy::cuda() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* p = cuda_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.cuda)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda*>(
      &::nvidia::inferenceserver::_ModelOptimizationPolicy_Cuda_default_instance_);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* ModelOptimizationPolicy::release_cuda() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOptimizationPolicy.cuda)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* temp = cuda_;
  cuda_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* ModelOptimizationPolicy::mutable_cuda() {
  
  if (cuda_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda>(GetArenaNoVirtual());
    cuda_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.cuda)
  return cuda_;
}
inline void ModelOptimizationPolicy::set_allocated_cuda(::nvidia::inferenceserver::ModelOptimizationPolicy_Cuda* cuda) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete cuda_;
  }
  if (cuda) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      cuda = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, cuda, submessage_arena);
    }
    
  } else {
    
  }
  cuda_ = cuda;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.cuda)
}

// .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;
inline bool ModelOptimizationPolicy::has_execution_accelerators() const {
  return this != internal_default_instance() && execution_accelerators_ != NULL;
}
inline void ModelOptimizationPolicy::clear_execution_accelerators() {
  if (GetArenaNoVirtual() == NULL && execution_accelerators_ != NULL) {
    delete execution_accelerators_;
  }
  execution_accelerators_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators& ModelOptimizationPolicy::_internal_execution_accelerators() const {
  return *execution_accelerators_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators& ModelOptimizationPolicy::execution_accelerators() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* p = execution_accelerators_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.execution_accelerators)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators*>(
      &::nvidia::inferenceserver::_ModelOptimizationPolicy_ExecutionAccelerators_default_instance_);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* ModelOptimizationPolicy::release_execution_accelerators() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOptimizationPolicy.execution_accelerators)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* temp = execution_accelerators_;
  execution_accelerators_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* ModelOptimizationPolicy::mutable_execution_accelerators() {
  
  if (execution_accelerators_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators>(GetArenaNoVirtual());
    execution_accelerators_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.execution_accelerators)
  return execution_accelerators_;
}
inline void ModelOptimizationPolicy::set_allocated_execution_accelerators(::nvidia::inferenceserver::ModelOptimizationPolicy_ExecutionAccelerators* execution_accelerators) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete execution_accelerators_;
  }
  if (execution_accelerators) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      execution_accelerators = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, execution_accelerators, submessage_arena);
    }
    
  } else {
    
  }
  execution_accelerators_ = execution_accelerators;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.execution_accelerators)
}

// .nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;
inline bool ModelOptimizationPolicy::has_input_pinned_memory() const {
  return this != internal_default_instance() && input_pinned_memory_ != NULL;
}
inline void ModelOptimizationPolicy::clear_input_pinned_memory() {
  if (GetArenaNoVirtual() == NULL && input_pinned_memory_ != NULL) {
    delete input_pinned_memory_;
  }
  input_pinned_memory_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& ModelOptimizationPolicy::_internal_input_pinned_memory() const {
  return *input_pinned_memory_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& ModelOptimizationPolicy::input_pinned_memory() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* p = input_pinned_memory_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.input_pinned_memory)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer*>(
      &::nvidia::inferenceserver::_ModelOptimizationPolicy_PinnedMemoryBuffer_default_instance_);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* ModelOptimizationPolicy::release_input_pinned_memory() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOptimizationPolicy.input_pinned_memory)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* temp = input_pinned_memory_;
  input_pinned_memory_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* ModelOptimizationPolicy::mutable_input_pinned_memory() {
  
  if (input_pinned_memory_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer>(GetArenaNoVirtual());
    input_pinned_memory_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.input_pinned_memory)
  return input_pinned_memory_;
}
inline void ModelOptimizationPolicy::set_allocated_input_pinned_memory(::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* input_pinned_memory) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete input_pinned_memory_;
  }
  if (input_pinned_memory) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      input_pinned_memory = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, input_pinned_memory, submessage_arena);
    }
    
  } else {
    
  }
  input_pinned_memory_ = input_pinned_memory;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.input_pinned_memory)
}

// .nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;
inline bool ModelOptimizationPolicy::has_output_pinned_memory() const {
  return this != internal_default_instance() && output_pinned_memory_ != NULL;
}
inline void ModelOptimizationPolicy::clear_output_pinned_memory() {
  if (GetArenaNoVirtual() == NULL && output_pinned_memory_ != NULL) {
    delete output_pinned_memory_;
  }
  output_pinned_memory_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& ModelOptimizationPolicy::_internal_output_pinned_memory() const {
  return *output_pinned_memory_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer& ModelOptimizationPolicy::output_pinned_memory() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* p = output_pinned_memory_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.output_pinned_memory)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer*>(
      &::nvidia::inferenceserver::_ModelOptimizationPolicy_PinnedMemoryBuffer_default_instance_);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* ModelOptimizationPolicy::release_output_pinned_memory() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOptimizationPolicy.output_pinned_memory)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* temp = output_pinned_memory_;
  output_pinned_memory_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* ModelOptimizationPolicy::mutable_output_pinned_memory() {
  
  if (output_pinned_memory_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer>(GetArenaNoVirtual());
    output_pinned_memory_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.output_pinned_memory)
  return output_pinned_memory_;
}
inline void ModelOptimizationPolicy::set_allocated_output_pinned_memory(::nvidia::inferenceserver::ModelOptimizationPolicy_PinnedMemoryBuffer* output_pinned_memory) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete output_pinned_memory_;
  }
  if (output_pinned_memory) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      output_pinned_memory = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, output_pinned_memory, submessage_arena);
    }
    
  } else {
    
  }
  output_pinned_memory_ = output_pinned_memory;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.output_pinned_memory)
}

// -------------------------------------------------------------------

// ModelQueuePolicy

// .nvidia.inferenceserver.ModelQueuePolicy.TimeoutAction timeout_action = 1;
inline void ModelQueuePolicy::clear_timeout_action() {
  timeout_action_ = 0;
}
inline ::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction ModelQueuePolicy::timeout_action() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelQueuePolicy.timeout_action)
  return static_cast< ::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction >(timeout_action_);
}
inline void ModelQueuePolicy::set_timeout_action(::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction value) {
  
  timeout_action_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelQueuePolicy.timeout_action)
}

// uint64 default_timeout_microseconds = 2;
inline void ModelQueuePolicy::clear_default_timeout_microseconds() {
  default_timeout_microseconds_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ModelQueuePolicy::default_timeout_microseconds() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelQueuePolicy.default_timeout_microseconds)
  return default_timeout_microseconds_;
}
inline void ModelQueuePolicy::set_default_timeout_microseconds(::google::protobuf::uint64 value) {
  
  default_timeout_microseconds_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelQueuePolicy.default_timeout_microseconds)
}

// bool allow_timeout_override = 3;
inline void ModelQueuePolicy::clear_allow_timeout_override() {
  allow_timeout_override_ = false;
}
inline bool ModelQueuePolicy::allow_timeout_override() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelQueuePolicy.allow_timeout_override)
  return allow_timeout_override_;
}
inline void ModelQueuePolicy::set_allow_timeout_override(bool value) {
  
  allow_timeout_override_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelQueuePolicy.allow_timeout_override)
}

// uint32 max_queue_size = 4;
inline void ModelQueuePolicy::clear_max_queue_size() {
  max_queue_size_ = 0u;
}
inline ::google::protobuf::uint32 ModelQueuePolicy::max_queue_size() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelQueuePolicy.max_queue_size)
  return max_queue_size_;
}
inline void ModelQueuePolicy::set_max_queue_size(::google::protobuf::uint32 value) {
  
  max_queue_size_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelQueuePolicy.max_queue_size)
}

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelDynamicBatching

// repeated int32 preferred_batch_size = 1;
inline int ModelDynamicBatching::preferred_batch_size_size() const {
  return preferred_batch_size_.size();
}
inline void ModelDynamicBatching::clear_preferred_batch_size() {
  preferred_batch_size_.Clear();
}
inline ::google::protobuf::int32 ModelDynamicBatching::preferred_batch_size(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
  return preferred_batch_size_.Get(index);
}
inline void ModelDynamicBatching::set_preferred_batch_size(int index, ::google::protobuf::int32 value) {
  preferred_batch_size_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
}
inline void ModelDynamicBatching::add_preferred_batch_size(::google::protobuf::int32 value) {
  preferred_batch_size_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
ModelDynamicBatching::preferred_batch_size() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
  return preferred_batch_size_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
ModelDynamicBatching::mutable_preferred_batch_size() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
  return &preferred_batch_size_;
}

// uint64 max_queue_delay_microseconds = 2;
inline void ModelDynamicBatching::clear_max_queue_delay_microseconds() {
  max_queue_delay_microseconds_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ModelDynamicBatching::max_queue_delay_microseconds() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.max_queue_delay_microseconds)
  return max_queue_delay_microseconds_;
}
inline void ModelDynamicBatching::set_max_queue_delay_microseconds(::google::protobuf::uint64 value) {
  
  max_queue_delay_microseconds_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelDynamicBatching.max_queue_delay_microseconds)
}

// bool preserve_ordering = 3;
inline void ModelDynamicBatching::clear_preserve_ordering() {
  preserve_ordering_ = false;
}
inline bool ModelDynamicBatching::preserve_ordering() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.preserve_ordering)
  return preserve_ordering_;
}
inline void ModelDynamicBatching::set_preserve_ordering(bool value) {
  
  preserve_ordering_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelDynamicBatching.preserve_ordering)
}

// uint32 priority_levels = 4;
inline void ModelDynamicBatching::clear_priority_levels() {
  priority_levels_ = 0u;
}
inline ::google::protobuf::uint32 ModelDynamicBatching::priority_levels() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.priority_levels)
  return priority_levels_;
}
inline void ModelDynamicBatching::set_priority_levels(::google::protobuf::uint32 value) {
  
  priority_levels_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelDynamicBatching.priority_levels)
}

// uint32 default_priority_level = 5;
inline void ModelDynamicBatching::clear_default_priority_level() {
  default_priority_level_ = 0u;
}
inline ::google::protobuf::uint32 ModelDynamicBatching::default_priority_level() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.default_priority_level)
  return default_priority_level_;
}
inline void ModelDynamicBatching::set_default_priority_level(::google::protobuf::uint32 value) {
  
  default_priority_level_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelDynamicBatching.default_priority_level)
}

// .nvidia.inferenceserver.ModelQueuePolicy default_queue_policy = 6;
inline bool ModelDynamicBatching::has_default_queue_policy() const {
  return this != internal_default_instance() && default_queue_policy_ != NULL;
}
inline void ModelDynamicBatching::clear_default_queue_policy() {
  if (GetArenaNoVirtual() == NULL && default_queue_policy_ != NULL) {
    delete default_queue_policy_;
  }
  default_queue_policy_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelQueuePolicy& ModelDynamicBatching::_internal_default_queue_policy() const {
  return *default_queue_policy_;
}
inline const ::nvidia::inferenceserver::ModelQueuePolicy& ModelDynamicBatching::default_queue_policy() const {
  const ::nvidia::inferenceserver::ModelQueuePolicy* p = default_queue_policy_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.default_queue_policy)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelQueuePolicy*>(
      &::nvidia::inferenceserver::_ModelQueuePolicy_default_instance_);
}
inline ::nvidia::inferenceserver::ModelQueuePolicy* ModelDynamicBatching::release_default_queue_policy() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelDynamicBatching.default_queue_policy)
  
  ::nvidia::inferenceserver::ModelQueuePolicy* temp = default_queue_policy_;
  default_queue_policy_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelQueuePolicy* ModelDynamicBatching::mutable_default_queue_policy() {
  
  if (default_queue_policy_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelQueuePolicy>(GetArenaNoVirtual());
    default_queue_policy_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelDynamicBatching.default_queue_policy)
  return default_queue_policy_;
}
inline void ModelDynamicBatching::set_allocated_default_queue_policy(::nvidia::inferenceserver::ModelQueuePolicy* default_queue_policy) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete default_queue_policy_;
  }
  if (default_queue_policy) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      default_queue_policy = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, default_queue_policy, submessage_arena);
    }
    
  } else {
    
  }
  default_queue_policy_ = default_queue_policy;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelDynamicBatching.default_queue_policy)
}

// map<uint32, .nvidia.inferenceserver.ModelQueuePolicy> priority_queue_policy = 7;
inline int ModelDynamicBatching::priority_queue_policy_size() const {
  return priority_queue_policy_.size();
}
inline void ModelDynamicBatching::clear_priority_queue_policy() {
  priority_queue_policy_.Clear();
}
inline const ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::ModelQueuePolicy >&
ModelDynamicBatching::priority_queue_policy() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelDynamicBatching.priority_queue_policy)
  return priority_queue_policy_.GetMap();
}
inline ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::ModelQueuePolicy >*
ModelDynamicBatching::mutable_priority_queue_policy() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelDynamicBatching.priority_queue_policy)
  return priority_queue_policy_.MutableMap();
}

// -------------------------------------------------------------------

// ModelSequenceBatching_Control

// .nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;
inline void ModelSequenceBatching_Control::clear_kind() {
  kind_ = 0;
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind ModelSequenceBatching_Control::kind() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.Control.kind)
  return static_cast< ::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind >(kind_);
}
inline void ModelSequenceBatching_Control::set_kind(::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind value) {
  
  kind_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.Control.kind)
}

// repeated int32 int32_false_true = 2;
inline int ModelSequenceBatching_Control::int32_false_true_size() const {
  return int32_false_true_.size();
}
inline void ModelSequenceBatching_Control::clear_int32_false_true() {
  int32_false_true_.Clear();
}
inline ::google::protobuf::int32 ModelSequenceBatching_Control::int32_false_true(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.Control.int32_false_true)
  return int32_false_true_.Get(index);
}
inline void ModelSequenceBatching_Control::set_int32_false_true(int index, ::google::protobuf::int32 value) {
  int32_false_true_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.Control.int32_false_true)
}
inline void ModelSequenceBatching_Control::add_int32_false_true(::google::protobuf::int32 value) {
  int32_false_true_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelSequenceBatching.Control.int32_false_true)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
ModelSequenceBatching_Control::int32_false_true() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelSequenceBatching.Control.int32_false_true)
  return int32_false_true_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
ModelSequenceBatching_Control::mutable_int32_false_true() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelSequenceBatching.Control.int32_false_true)
  return &int32_false_true_;
}

// repeated float fp32_false_true = 3;
inline int ModelSequenceBatching_Control::fp32_false_true_size() const {
  return fp32_false_true_.size();
}
inline void ModelSequenceBatching_Control::clear_fp32_false_true() {
  fp32_false_true_.Clear();
}
inline float ModelSequenceBatching_Control::fp32_false_true(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.Control.fp32_false_true)
  return fp32_false_true_.Get(index);
}
inline void ModelSequenceBatching_Control::set_fp32_false_true(int index, float value) {
  fp32_false_true_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.Control.fp32_false_true)
}
inline void ModelSequenceBatching_Control::add_fp32_false_true(float value) {
  fp32_false_true_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelSequenceBatching.Control.fp32_false_true)
}
inline const ::google::protobuf::RepeatedField< float >&
ModelSequenceBatching_Control::fp32_false_true() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelSequenceBatching.Control.fp32_false_true)
  return fp32_false_true_;
}
inline ::google::protobuf::RepeatedField< float >*
ModelSequenceBatching_Control::mutable_fp32_false_true() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelSequenceBatching.Control.fp32_false_true)
  return &fp32_false_true_;
}

// .nvidia.inferenceserver.DataType data_type = 4;
inline void ModelSequenceBatching_Control::clear_data_type() {
  data_type_ = 0;
}
inline ::nvidia::inferenceserver::DataType ModelSequenceBatching_Control::data_type() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.Control.data_type)
  return static_cast< ::nvidia::inferenceserver::DataType >(data_type_);
}
inline void ModelSequenceBatching_Control::set_data_type(::nvidia::inferenceserver::DataType value) {
  
  data_type_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.Control.data_type)
}

// -------------------------------------------------------------------

// ModelSequenceBatching_ControlInput

// string name = 1;
inline void ModelSequenceBatching_ControlInput::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelSequenceBatching_ControlInput::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
  return name_.GetNoArena();
}
inline void ModelSequenceBatching_ControlInput::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
}
#if LANG_CXX11
inline void ModelSequenceBatching_ControlInput::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
}
#endif
inline void ModelSequenceBatching_ControlInput::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
}
inline void ModelSequenceBatching_ControlInput::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
}
inline ::std::string* ModelSequenceBatching_ControlInput::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelSequenceBatching_ControlInput::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelSequenceBatching_ControlInput::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name)
}

// repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;
inline int ModelSequenceBatching_ControlInput::control_size() const {
  return control_.size();
}
inline void ModelSequenceBatching_ControlInput::clear_control() {
  control_.Clear();
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_Control* ModelSequenceBatching_ControlInput::mutable_control(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.control)
  return control_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_Control >*
ModelSequenceBatching_ControlInput::mutable_control() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.control)
  return &control_;
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching_Control& ModelSequenceBatching_ControlInput::control(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.control)
  return control_.Get(index);
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_Control* ModelSequenceBatching_ControlInput::add_control() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.control)
  return control_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_Control >&
ModelSequenceBatching_ControlInput::control() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelSequenceBatching.ControlInput.control)
  return control_;
}

// -------------------------------------------------------------------

// ModelSequenceBatching_StrategyDirect

// -------------------------------------------------------------------

// ModelSequenceBatching_StrategyOldest

// int32 max_candidate_sequences = 1;
inline void ModelSequenceBatching_StrategyOldest::clear_max_candidate_sequences() {
  max_candidate_sequences_ = 0;
}
inline ::google::protobuf::int32 ModelSequenceBatching_StrategyOldest::max_candidate_sequences() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.max_candidate_sequences)
  return max_candidate_sequences_;
}
inline void ModelSequenceBatching_StrategyOldest::set_max_candidate_sequences(::google::protobuf::int32 value) {
  
  max_candidate_sequences_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.max_candidate_sequences)
}

// repeated int32 preferred_batch_size = 2;
inline int ModelSequenceBatching_StrategyOldest::preferred_batch_size_size() const {
  return preferred_batch_size_.size();
}
inline void ModelSequenceBatching_StrategyOldest::clear_preferred_batch_size() {
  preferred_batch_size_.Clear();
}
inline ::google::protobuf::int32 ModelSequenceBatching_StrategyOldest::preferred_batch_size(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.preferred_batch_size)
  return preferred_batch_size_.Get(index);
}
inline void ModelSequenceBatching_StrategyOldest::set_preferred_batch_size(int index, ::google::protobuf::int32 value) {
  preferred_batch_size_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.preferred_batch_size)
}
inline void ModelSequenceBatching_StrategyOldest::add_preferred_batch_size(::google::protobuf::int32 value) {
  preferred_batch_size_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.preferred_batch_size)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int32 >&
ModelSequenceBatching_StrategyOldest::preferred_batch_size() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.preferred_batch_size)
  return preferred_batch_size_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int32 >*
ModelSequenceBatching_StrategyOldest::mutable_preferred_batch_size() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.preferred_batch_size)
  return &preferred_batch_size_;
}

// uint64 max_queue_delay_microseconds = 3;
inline void ModelSequenceBatching_StrategyOldest::clear_max_queue_delay_microseconds() {
  max_queue_delay_microseconds_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ModelSequenceBatching_StrategyOldest::max_queue_delay_microseconds() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.max_queue_delay_microseconds)
  return max_queue_delay_microseconds_;
}
inline void ModelSequenceBatching_StrategyOldest::set_max_queue_delay_microseconds(::google::protobuf::uint64 value) {
  
  max_queue_delay_microseconds_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.max_queue_delay_microseconds)
}

// -------------------------------------------------------------------

// ModelSequenceBatching

// .nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;
inline bool ModelSequenceBatching::has_direct() const {
  return strategy_choice_case() == kDirect;
}
inline void ModelSequenceBatching::set_has_direct() {
  _oneof_case_[0] = kDirect;
}
inline void ModelSequenceBatching::clear_direct() {
  if (has_direct()) {
    delete strategy_choice_.direct_;
    clear_has_strategy_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect& ModelSequenceBatching::_internal_direct() const {
  return *strategy_choice_.direct_;
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* ModelSequenceBatching::release_direct() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelSequenceBatching.direct)
  if (has_direct()) {
    clear_has_strategy_choice();
      ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* temp = strategy_choice_.direct_;
    strategy_choice_.direct_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect& ModelSequenceBatching::direct() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.direct)
  return has_direct()
      ? *strategy_choice_.direct_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect*>(&::nvidia::inferenceserver::_ModelSequenceBatching_StrategyDirect_default_instance_);
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect* ModelSequenceBatching::mutable_direct() {
  if (!has_direct()) {
    clear_strategy_choice();
    set_has_direct();
    strategy_choice_.direct_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelSequenceBatching_StrategyDirect >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelSequenceBatching.direct)
  return strategy_choice_.direct_;
}

// .nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;
inline bool ModelSequenceBatching::has_oldest() const {
  return strategy_choice_case() == kOldest;
}
inline void ModelSequenceBatching::set_has_oldest() {
  _oneof_case_[0] = kOldest;
}
inline void ModelSequenceBatching::clear_oldest() {
  if (has_oldest()) {
    delete strategy_choice_.oldest_;
    clear_has_strategy_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest& ModelSequenceBatching::_internal_oldest() const {
  return *strategy_choice_.oldest_;
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* ModelSequenceBatching::release_oldest() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelSequenceBatching.oldest)
  if (has_oldest()) {
    clear_has_strategy_choice();
      ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* temp = strategy_choice_.oldest_;
    strategy_choice_.oldest_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest& ModelSequenceBatching::oldest() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.oldest)
  return has_oldest()
      ? *strategy_choice_.oldest_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest*>(&::nvidia::inferenceserver::_ModelSequenceBatching_StrategyOldest_default_instance_);
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest* ModelSequenceBatching::mutable_oldest() {
  if (!has_oldest()) {
    clear_strategy_choice();
    set_has_oldest();
    strategy_choice_.oldest_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelSequenceBatching_StrategyOldest >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelSequenceBatching.oldest)
  return strategy_choice_.oldest_;
}

// uint64 max_sequence_idle_microseconds = 1;
inline void ModelSequenceBatching::clear_max_sequence_idle_microseconds() {
  max_sequence_idle_microseconds_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ModelSequenceBatching::max_sequence_idle_microseconds() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.max_sequence_idle_microseconds)
  return max_sequence_idle_microseconds_;
}
inline void ModelSequenceBatching::set_max_sequence_idle_microseconds(::google::protobuf::uint64 value) {
  
  max_sequence_idle_microseconds_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.max_sequence_idle_microseconds)
}

// repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;
inline int ModelSequenceBatching::control_input_size() const {
  return control_input_.size();
}
inline void ModelSequenceBatching::clear_control_input() {
  control_input_.Clear();
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput* ModelSequenceBatching::mutable_control_input(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelSequenceBatching.control_input)
  return control_input_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput >*
ModelSequenceBatching::mutable_control_input() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelSequenceBatching.control_input)
  return &control_input_;
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput& ModelSequenceBatching::control_input(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.control_input)
  return control_input_.Get(index);
}
inline ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput* ModelSequenceBatching::add_control_input() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelSequenceBatching.control_input)
  return control_input_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelSequenceBatching_ControlInput >&
ModelSequenceBatching::control_input() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelSequenceBatching.control_input)
  return control_input_;
}

inline bool ModelSequenceBatching::has_strategy_choice() const {
  return strategy_choice_case() != STRATEGY_CHOICE_NOT_SET;
}
inline void ModelSequenceBatching::clear_has_strategy_choice() {
  _oneof_case_[0] = STRATEGY_CHOICE_NOT_SET;
}
inline ModelSequenceBatching::StrategyChoiceCase ModelSequenceBatching::strategy_choice_case() const {
  return ModelSequenceBatching::StrategyChoiceCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelEnsembling_Step

// string model_name = 1;
inline void ModelEnsembling_Step::clear_model_name() {
  model_name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelEnsembling_Step::model_name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
  return model_name_.GetNoArena();
}
inline void ModelEnsembling_Step::set_model_name(const ::std::string& value) {
  
  model_name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
}
#if LANG_CXX11
inline void ModelEnsembling_Step::set_model_name(::std::string&& value) {
  
  model_name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
}
#endif
inline void ModelEnsembling_Step::set_model_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  model_name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
}
inline void ModelEnsembling_Step::set_model_name(const char* value, size_t size) {
  
  model_name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
}
inline ::std::string* ModelEnsembling_Step::mutable_model_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
  return model_name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelEnsembling_Step::release_model_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
  
  return model_name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelEnsembling_Step::set_allocated_model_name(::std::string* model_name) {
  if (model_name != NULL) {
    
  } else {
    
  }
  model_name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), model_name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelEnsembling.Step.model_name)
}

// int64 model_version = 2;
inline void ModelEnsembling_Step::clear_model_version() {
  model_version_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 ModelEnsembling_Step::model_version() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelEnsembling.Step.model_version)
  return model_version_;
}
inline void ModelEnsembling_Step::set_model_version(::google::protobuf::int64 value) {
  
  model_version_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelEnsembling.Step.model_version)
}

// map<string, string> input_map = 3;
inline int ModelEnsembling_Step::input_map_size() const {
  return input_map_.size();
}
inline void ModelEnsembling_Step::clear_input_map() {
  input_map_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::std::string >&
ModelEnsembling_Step::input_map() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelEnsembling.Step.input_map)
  return input_map_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::std::string >*
ModelEnsembling_Step::mutable_input_map() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelEnsembling.Step.input_map)
  return input_map_.MutableMap();
}

// map<string, string> output_map = 4;
inline int ModelEnsembling_Step::output_map_size() const {
  return output_map_.size();
}
inline void ModelEnsembling_Step::clear_output_map() {
  output_map_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::std::string >&
ModelEnsembling_Step::output_map() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelEnsembling.Step.output_map)
  return output_map_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::std::string >*
ModelEnsembling_Step::mutable_output_map() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelEnsembling.Step.output_map)
  return output_map_.MutableMap();
}

// -------------------------------------------------------------------

// ModelEnsembling

// repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;
inline int ModelEnsembling::step_size() const {
  return step_.size();
}
inline void ModelEnsembling::clear_step() {
  step_.Clear();
}
inline ::nvidia::inferenceserver::ModelEnsembling_Step* ModelEnsembling::mutable_step(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelEnsembling.step)
  return step_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelEnsembling_Step >*
ModelEnsembling::mutable_step() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelEnsembling.step)
  return &step_;
}
inline const ::nvidia::inferenceserver::ModelEnsembling_Step& ModelEnsembling::step(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelEnsembling.step)
  return step_.Get(index);
}
inline ::nvidia::inferenceserver::ModelEnsembling_Step* ModelEnsembling::add_step() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelEnsembling.step)
  return step_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelEnsembling_Step >&
ModelEnsembling::step() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelEnsembling.step)
  return step_;
}

// -------------------------------------------------------------------

// ModelParameter

// string string_value = 1;
inline void ModelParameter::clear_string_value() {
  string_value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelParameter::string_value() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelParameter.string_value)
  return string_value_.GetNoArena();
}
inline void ModelParameter::set_string_value(const ::std::string& value) {
  
  string_value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelParameter.string_value)
}
#if LANG_CXX11
inline void ModelParameter::set_string_value(::std::string&& value) {
  
  string_value_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelParameter.string_value)
}
#endif
inline void ModelParameter::set_string_value(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  string_value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelParameter.string_value)
}
inline void ModelParameter::set_string_value(const char* value, size_t size) {
  
  string_value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelParameter.string_value)
}
inline ::std::string* ModelParameter::mutable_string_value() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelParameter.string_value)
  return string_value_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelParameter::release_string_value() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelParameter.string_value)
  
  return string_value_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelParameter::set_allocated_string_value(::std::string* string_value) {
  if (string_value != NULL) {
    
  } else {
    
  }
  string_value_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), string_value);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelParameter.string_value)
}

// -------------------------------------------------------------------

// ModelWarmup_Input

// .nvidia.inferenceserver.DataType data_type = 1;
inline void ModelWarmup_Input::clear_data_type() {
  data_type_ = 0;
}
inline ::nvidia::inferenceserver::DataType ModelWarmup_Input::data_type() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelWarmup.Input.data_type)
  return static_cast< ::nvidia::inferenceserver::DataType >(data_type_);
}
inline void ModelWarmup_Input::set_data_type(::nvidia::inferenceserver::DataType value) {
  
  data_type_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.Input.data_type)
}

// repeated int64 dims = 2;
inline int ModelWarmup_Input::dims_size() const {
  return dims_.size();
}
inline void ModelWarmup_Input::clear_dims() {
  dims_.Clear();
}
inline ::google::protobuf::int64 ModelWarmup_Input::dims(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelWarmup.Input.dims)
  return dims_.Get(index);
}
inline void ModelWarmup_Input::set_dims(int index, ::google::protobuf::int64 value) {
  dims_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.Input.dims)
}
inline void ModelWarmup_Input::add_dims(::google::protobuf::int64 value) {
  dims_.Add(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelWarmup.Input.dims)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ModelWarmup_Input::dims() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelWarmup.Input.dims)
  return dims_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ModelWarmup_Input::mutable_dims() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelWarmup.Input.dims)
  return &dims_;
}

// bool zero_data = 3;
inline bool ModelWarmup_Input::has_zero_data() const {
  return input_data_type_case() == kZeroData;
}
inline void ModelWarmup_Input::set_has_zero_data() {
  _oneof_case_[0] = kZeroData;
}
inline void ModelWarmup_Input::clear_zero_data() {
  if (has_zero_data()) {
    input_data_type_.zero_data_ = false;
    clear_has_input_data_type();
  }
}
inline bool ModelWarmup_Input::zero_data() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelWarmup.Input.zero_data)
  if (has_zero_data()) {
    return input_data_type_.zero_data_;
  }
  return false;
}
inline void ModelWarmup_Input::set_zero_data(bool value) {
  if (!has_zero_data()) {
    clear_input_data_type();
    set_has_zero_data();
  }
  input_data_type_.zero_data_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.Input.zero_data)
}

// bool random_data = 4;
inline bool ModelWarmup_Input::has_random_data() const {
  return input_data_type_case() == kRandomData;
}
inline void ModelWarmup_Input::set_has_random_data() {
  _oneof_case_[0] = kRandomData;
}
inline void ModelWarmup_Input::clear_random_data() {
  if (has_random_data()) {
    input_data_type_.random_data_ = false;
    clear_has_input_data_type();
  }
}
inline bool ModelWarmup_Input::random_data() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelWarmup.Input.random_data)
  if (has_random_data()) {
    return input_data_type_.random_data_;
  }
  return false;
}
inline void ModelWarmup_Input::set_random_data(bool value) {
  if (!has_random_data()) {
    clear_input_data_type();
    set_has_random_data();
  }
  input_data_type_.random_data_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.Input.random_data)
}

// string input_data_file = 5;
inline bool ModelWarmup_Input::has_input_data_file() const {
  return input_data_type_case() == kInputDataFile;
}
inline void ModelWarmup_Input::set_has_input_data_file() {
  _oneof_case_[0] = kInputDataFile;
}
inline void ModelWarmup_Input::clear_input_data_file() {
  if (has_input_data_file()) {
    input_data_type_.input_data_file_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
    clear_has_input_data_type();
  }
}
inline const ::std::string& ModelWarmup_Input::input_data_file() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
  if (has_input_data_file()) {
    return input_data_type_.input_data_file_.GetNoArena();
  }
  return *&::google::protobuf::internal::GetEmptyStringAlreadyInited();
}
inline void ModelWarmup_Input::set_input_data_file(const ::std::string& value) {
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
  if (!has_input_data_file()) {
    clear_input_data_type();
    set_has_input_data_file();
    input_data_type_.input_data_file_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  input_data_type_.input_data_file_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
}
#if LANG_CXX11
inline void ModelWarmup_Input::set_input_data_file(::std::string&& value) {
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
  if (!has_input_data_file()) {
    clear_input_data_type();
    set_has_input_data_file();
    input_data_type_.input_data_file_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  input_data_type_.input_data_file_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
}
#endif
inline void ModelWarmup_Input::set_input_data_file(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  if (!has_input_data_file()) {
    clear_input_data_type();
    set_has_input_data_file();
    input_data_type_.input_data_file_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  input_data_type_.input_data_file_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
}
inline void ModelWarmup_Input::set_input_data_file(const char* value, size_t size) {
  if (!has_input_data_file()) {
    clear_input_data_type();
    set_has_input_data_file();
    input_data_type_.input_data_file_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  input_data_type_.input_data_file_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(
      reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
}
inline ::std::string* ModelWarmup_Input::mutable_input_data_file() {
  if (!has_input_data_file()) {
    clear_input_data_type();
    set_has_input_data_file();
    input_data_type_.input_data_file_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
  return input_data_type_.input_data_file_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelWarmup_Input::release_input_data_file() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
  if (has_input_data_file()) {
    clear_has_input_data_type();
    return input_data_type_.input_data_file_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  } else {
    return NULL;
  }
}
inline void ModelWarmup_Input::set_allocated_input_data_file(::std::string* input_data_file) {
  if (!has_input_data_file()) {
    input_data_type_.input_data_file_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  clear_input_data_type();
  if (input_data_file != NULL) {
    set_has_input_data_file();
    input_data_type_.input_data_file_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), input_data_file);
  }
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelWarmup.Input.input_data_file)
}

inline bool ModelWarmup_Input::has_input_data_type() const {
  return input_data_type_case() != INPUT_DATA_TYPE_NOT_SET;
}
inline void ModelWarmup_Input::clear_has_input_data_type() {
  _oneof_case_[0] = INPUT_DATA_TYPE_NOT_SET;
}
inline ModelWarmup_Input::InputDataTypeCase ModelWarmup_Input::input_data_type_case() const {
  return ModelWarmup_Input::InputDataTypeCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelWarmup

// string name = 1;
inline void ModelWarmup::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelWarmup::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelWarmup.name)
  return name_.GetNoArena();
}
inline void ModelWarmup::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.name)
}
#if LANG_CXX11
inline void ModelWarmup::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelWarmup.name)
}
#endif
inline void ModelWarmup::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelWarmup.name)
}
inline void ModelWarmup::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelWarmup.name)
}
inline ::std::string* ModelWarmup::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelWarmup.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelWarmup::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelWarmup.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelWarmup::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelWarmup.name)
}

// uint32 batch_size = 2;
inline void ModelWarmup::clear_batch_size() {
  batch_size_ = 0u;
}
inline ::google::protobuf::uint32 ModelWarmup::batch_size() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelWarmup.batch_size)
  return batch_size_;
}
inline void ModelWarmup::set_batch_size(::google::protobuf::uint32 value) {
  
  batch_size_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelWarmup.batch_size)
}

// map<string, .nvidia.inferenceserver.ModelWarmup.Input> inputs = 3;
inline int ModelWarmup::inputs_size() const {
  return inputs_.size();
}
inline void ModelWarmup::clear_inputs() {
  inputs_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelWarmup_Input >&
ModelWarmup::inputs() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelWarmup.inputs)
  return inputs_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelWarmup_Input >*
ModelWarmup::mutable_inputs() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelWarmup.inputs)
  return inputs_.MutableMap();
}

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelConfig

// string name = 1;
inline void ModelConfig::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelConfig::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.name)
  return name_.GetNoArena();
}
inline void ModelConfig::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.name)
}
#if LANG_CXX11
inline void ModelConfig::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelConfig.name)
}
#endif
inline void ModelConfig::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelConfig.name)
}
inline void ModelConfig::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelConfig.name)
}
inline ::std::string* ModelConfig::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelConfig::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelConfig::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.name)
}

// string platform = 2;
inline void ModelConfig::clear_platform() {
  platform_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelConfig::platform() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.platform)
  return platform_.GetNoArena();
}
inline void ModelConfig::set_platform(const ::std::string& value) {
  
  platform_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.platform)
}
#if LANG_CXX11
inline void ModelConfig::set_platform(::std::string&& value) {
  
  platform_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelConfig.platform)
}
#endif
inline void ModelConfig::set_platform(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  platform_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelConfig.platform)
}
inline void ModelConfig::set_platform(const char* value, size_t size) {
  
  platform_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelConfig.platform)
}
inline ::std::string* ModelConfig::mutable_platform() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.platform)
  return platform_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelConfig::release_platform() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.platform)
  
  return platform_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelConfig::set_allocated_platform(::std::string* platform) {
  if (platform != NULL) {
    
  } else {
    
  }
  platform_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), platform);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.platform)
}

// .nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;
inline bool ModelConfig::has_version_policy() const {
  return this != internal_default_instance() && version_policy_ != NULL;
}
inline void ModelConfig::clear_version_policy() {
  if (GetArenaNoVirtual() == NULL && version_policy_ != NULL) {
    delete version_policy_;
  }
  version_policy_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy& ModelConfig::_internal_version_policy() const {
  return *version_policy_;
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy& ModelConfig::version_policy() const {
  const ::nvidia::inferenceserver::ModelVersionPolicy* p = version_policy_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.version_policy)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelVersionPolicy*>(
      &::nvidia::inferenceserver::_ModelVersionPolicy_default_instance_);
}
inline ::nvidia::inferenceserver::ModelVersionPolicy* ModelConfig::release_version_policy() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.version_policy)
  
  ::nvidia::inferenceserver::ModelVersionPolicy* temp = version_policy_;
  version_policy_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy* ModelConfig::mutable_version_policy() {
  
  if (version_policy_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy>(GetArenaNoVirtual());
    version_policy_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.version_policy)
  return version_policy_;
}
inline void ModelConfig::set_allocated_version_policy(::nvidia::inferenceserver::ModelVersionPolicy* version_policy) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete version_policy_;
  }
  if (version_policy) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      version_policy = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, version_policy, submessage_arena);
    }
    
  } else {
    
  }
  version_policy_ = version_policy;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.version_policy)
}

// int32 max_batch_size = 4;
inline void ModelConfig::clear_max_batch_size() {
  max_batch_size_ = 0;
}
inline ::google::protobuf::int32 ModelConfig::max_batch_size() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.max_batch_size)
  return max_batch_size_;
}
inline void ModelConfig::set_max_batch_size(::google::protobuf::int32 value) {
  
  max_batch_size_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.max_batch_size)
}

// repeated .nvidia.inferenceserver.ModelInput input = 5;
inline int ModelConfig::input_size() const {
  return input_.size();
}
inline void ModelConfig::clear_input() {
  input_.Clear();
}
inline ::nvidia::inferenceserver::ModelInput* ModelConfig::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.input)
  return input_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >*
ModelConfig::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelConfig.input)
  return &input_;
}
inline const ::nvidia::inferenceserver::ModelInput& ModelConfig::input(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.input)
  return input_.Get(index);
}
inline ::nvidia::inferenceserver::ModelInput* ModelConfig::add_input() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelConfig.input)
  return input_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >&
ModelConfig::input() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelConfig.input)
  return input_;
}

// repeated .nvidia.inferenceserver.ModelOutput output = 6;
inline int ModelConfig::output_size() const {
  return output_.size();
}
inline void ModelConfig::clear_output() {
  output_.Clear();
}
inline ::nvidia::inferenceserver::ModelOutput* ModelConfig::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.output)
  return output_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >*
ModelConfig::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelConfig.output)
  return &output_;
}
inline const ::nvidia::inferenceserver::ModelOutput& ModelConfig::output(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.output)
  return output_.Get(index);
}
inline ::nvidia::inferenceserver::ModelOutput* ModelConfig::add_output() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelConfig.output)
  return output_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >&
ModelConfig::output() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelConfig.output)
  return output_;
}

// .nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;
inline bool ModelConfig::has_optimization() const {
  return this != internal_default_instance() && optimization_ != NULL;
}
inline void ModelConfig::clear_optimization() {
  if (GetArenaNoVirtual() == NULL && optimization_ != NULL) {
    delete optimization_;
  }
  optimization_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy& ModelConfig::_internal_optimization() const {
  return *optimization_;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy& ModelConfig::optimization() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy* p = optimization_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.optimization)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy*>(
      &::nvidia::inferenceserver::_ModelOptimizationPolicy_default_instance_);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy* ModelConfig::release_optimization() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.optimization)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy* temp = optimization_;
  optimization_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy* ModelConfig::mutable_optimization() {
  
  if (optimization_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy>(GetArenaNoVirtual());
    optimization_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.optimization)
  return optimization_;
}
inline void ModelConfig::set_allocated_optimization(::nvidia::inferenceserver::ModelOptimizationPolicy* optimization) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete optimization_;
  }
  if (optimization) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      optimization = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, optimization, submessage_arena);
    }
    
  } else {
    
  }
  optimization_ = optimization;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.optimization)
}

// .nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;
inline bool ModelConfig::has_dynamic_batching() const {
  return scheduling_choice_case() == kDynamicBatching;
}
inline void ModelConfig::set_has_dynamic_batching() {
  _oneof_case_[0] = kDynamicBatching;
}
inline void ModelConfig::clear_dynamic_batching() {
  if (has_dynamic_batching()) {
    delete scheduling_choice_.dynamic_batching_;
    clear_has_scheduling_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelDynamicBatching& ModelConfig::_internal_dynamic_batching() const {
  return *scheduling_choice_.dynamic_batching_;
}
inline ::nvidia::inferenceserver::ModelDynamicBatching* ModelConfig::release_dynamic_batching() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.dynamic_batching)
  if (has_dynamic_batching()) {
    clear_has_scheduling_choice();
      ::nvidia::inferenceserver::ModelDynamicBatching* temp = scheduling_choice_.dynamic_batching_;
    scheduling_choice_.dynamic_batching_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelDynamicBatching& ModelConfig::dynamic_batching() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.dynamic_batching)
  return has_dynamic_batching()
      ? *scheduling_choice_.dynamic_batching_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelDynamicBatching*>(&::nvidia::inferenceserver::_ModelDynamicBatching_default_instance_);
}
inline ::nvidia::inferenceserver::ModelDynamicBatching* ModelConfig::mutable_dynamic_batching() {
  if (!has_dynamic_batching()) {
    clear_scheduling_choice();
    set_has_dynamic_batching();
    scheduling_choice_.dynamic_batching_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelDynamicBatching >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.dynamic_batching)
  return scheduling_choice_.dynamic_batching_;
}

// .nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;
inline bool ModelConfig::has_sequence_batching() const {
  return scheduling_choice_case() == kSequenceBatching;
}
inline void ModelConfig::set_has_sequence_batching() {
  _oneof_case_[0] = kSequenceBatching;
}
inline void ModelConfig::clear_sequence_batching() {
  if (has_sequence_batching()) {
    delete scheduling_choice_.sequence_batching_;
    clear_has_scheduling_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching& ModelConfig::_internal_sequence_batching() const {
  return *scheduling_choice_.sequence_batching_;
}
inline ::nvidia::inferenceserver::ModelSequenceBatching* ModelConfig::release_sequence_batching() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.sequence_batching)
  if (has_sequence_batching()) {
    clear_has_scheduling_choice();
      ::nvidia::inferenceserver::ModelSequenceBatching* temp = scheduling_choice_.sequence_batching_;
    scheduling_choice_.sequence_batching_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching& ModelConfig::sequence_batching() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.sequence_batching)
  return has_sequence_batching()
      ? *scheduling_choice_.sequence_batching_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelSequenceBatching*>(&::nvidia::inferenceserver::_ModelSequenceBatching_default_instance_);
}
inline ::nvidia::inferenceserver::ModelSequenceBatching* ModelConfig::mutable_sequence_batching() {
  if (!has_sequence_batching()) {
    clear_scheduling_choice();
    set_has_sequence_batching();
    scheduling_choice_.sequence_batching_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelSequenceBatching >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.sequence_batching)
  return scheduling_choice_.sequence_batching_;
}

// .nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;
inline bool ModelConfig::has_ensemble_scheduling() const {
  return scheduling_choice_case() == kEnsembleScheduling;
}
inline void ModelConfig::set_has_ensemble_scheduling() {
  _oneof_case_[0] = kEnsembleScheduling;
}
inline void ModelConfig::clear_ensemble_scheduling() {
  if (has_ensemble_scheduling()) {
    delete scheduling_choice_.ensemble_scheduling_;
    clear_has_scheduling_choice();
  }
}
inline const ::nvidia::inferenceserver::ModelEnsembling& ModelConfig::_internal_ensemble_scheduling() const {
  return *scheduling_choice_.ensemble_scheduling_;
}
inline ::nvidia::inferenceserver::ModelEnsembling* ModelConfig::release_ensemble_scheduling() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.ensemble_scheduling)
  if (has_ensemble_scheduling()) {
    clear_has_scheduling_choice();
      ::nvidia::inferenceserver::ModelEnsembling* temp = scheduling_choice_.ensemble_scheduling_;
    scheduling_choice_.ensemble_scheduling_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::ModelEnsembling& ModelConfig::ensemble_scheduling() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.ensemble_scheduling)
  return has_ensemble_scheduling()
      ? *scheduling_choice_.ensemble_scheduling_
      : *reinterpret_cast< ::nvidia::inferenceserver::ModelEnsembling*>(&::nvidia::inferenceserver::_ModelEnsembling_default_instance_);
}
inline ::nvidia::inferenceserver::ModelEnsembling* ModelConfig::mutable_ensemble_scheduling() {
  if (!has_ensemble_scheduling()) {
    clear_scheduling_choice();
    set_has_ensemble_scheduling();
    scheduling_choice_.ensemble_scheduling_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelEnsembling >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.ensemble_scheduling)
  return scheduling_choice_.ensemble_scheduling_;
}

// repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;
inline int ModelConfig::instance_group_size() const {
  return instance_group_.size();
}
inline void ModelConfig::clear_instance_group() {
  instance_group_.Clear();
}
inline ::nvidia::inferenceserver::ModelInstanceGroup* ModelConfig::mutable_instance_group(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.instance_group)
  return instance_group_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >*
ModelConfig::mutable_instance_group() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelConfig.instance_group)
  return &instance_group_;
}
inline const ::nvidia::inferenceserver::ModelInstanceGroup& ModelConfig::instance_group(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.instance_group)
  return instance_group_.Get(index);
}
inline ::nvidia::inferenceserver::ModelInstanceGroup* ModelConfig::add_instance_group() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelConfig.instance_group)
  return instance_group_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >&
ModelConfig::instance_group() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelConfig.instance_group)
  return instance_group_;
}

// string default_model_filename = 8;
inline void ModelConfig::clear_default_model_filename() {
  default_model_filename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelConfig::default_model_filename() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.default_model_filename)
  return default_model_filename_.GetNoArena();
}
inline void ModelConfig::set_default_model_filename(const ::std::string& value) {
  
  default_model_filename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.default_model_filename)
}
#if LANG_CXX11
inline void ModelConfig::set_default_model_filename(::std::string&& value) {
  
  default_model_filename_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelConfig.default_model_filename)
}
#endif
inline void ModelConfig::set_default_model_filename(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  default_model_filename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelConfig.default_model_filename)
}
inline void ModelConfig::set_default_model_filename(const char* value, size_t size) {
  
  default_model_filename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelConfig.default_model_filename)
}
inline ::std::string* ModelConfig::mutable_default_model_filename() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.default_model_filename)
  return default_model_filename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelConfig::release_default_model_filename() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.default_model_filename)
  
  return default_model_filename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelConfig::set_allocated_default_model_filename(::std::string* default_model_filename) {
  if (default_model_filename != NULL) {
    
  } else {
    
  }
  default_model_filename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), default_model_filename);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.default_model_filename)
}

// map<string, string> cc_model_filenames = 9;
inline int ModelConfig::cc_model_filenames_size() const {
  return cc_model_filenames_.size();
}
inline void ModelConfig::clear_cc_model_filenames() {
  cc_model_filenames_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::std::string >&
ModelConfig::cc_model_filenames() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelConfig.cc_model_filenames)
  return cc_model_filenames_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::std::string >*
ModelConfig::mutable_cc_model_filenames() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelConfig.cc_model_filenames)
  return cc_model_filenames_.MutableMap();
}

// map<string, string> metric_tags = 10;
inline int ModelConfig::metric_tags_size() const {
  return metric_tags_.size();
}
inline void ModelConfig::clear_metric_tags() {
  metric_tags_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::std::string >&
ModelConfig::metric_tags() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelConfig.metric_tags)
  return metric_tags_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::std::string >*
ModelConfig::mutable_metric_tags() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelConfig.metric_tags)
  return metric_tags_.MutableMap();
}

// map<string, .nvidia.inferenceserver.ModelParameter> parameters = 14;
inline int ModelConfig::parameters_size() const {
  return parameters_.size();
}
inline void ModelConfig::clear_parameters() {
  parameters_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelParameter >&
ModelConfig::parameters() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelConfig.parameters)
  return parameters_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelParameter >*
ModelConfig::mutable_parameters() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelConfig.parameters)
  return parameters_.MutableMap();
}

// repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;
inline int ModelConfig::model_warmup_size() const {
  return model_warmup_.size();
}
inline void ModelConfig::clear_model_warmup() {
  model_warmup_.Clear();
}
inline ::nvidia::inferenceserver::ModelWarmup* ModelConfig::mutable_model_warmup(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.model_warmup)
  return model_warmup_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelWarmup >*
ModelConfig::mutable_model_warmup() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelConfig.model_warmup)
  return &model_warmup_;
}
inline const ::nvidia::inferenceserver::ModelWarmup& ModelConfig::model_warmup(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.model_warmup)
  return model_warmup_.Get(index);
}
inline ::nvidia::inferenceserver::ModelWarmup* ModelConfig::add_model_warmup() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelConfig.model_warmup)
  return model_warmup_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelWarmup >&
ModelConfig::model_warmup() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelConfig.model_warmup)
  return model_warmup_;
}

inline bool ModelConfig::has_scheduling_choice() const {
  return scheduling_choice_case() != SCHEDULING_CHOICE_NOT_SET;
}
inline void ModelConfig::clear_has_scheduling_choice() {
  _oneof_case_[0] = SCHEDULING_CHOICE_NOT_SET;
}
inline ModelConfig::SchedulingChoiceCase ModelConfig::scheduling_choice_case() const {
  return ModelConfig::SchedulingChoiceCase(_oneof_case_[0]);
}
#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace inferenceserver
}  // namespace nvidia

namespace google {
namespace protobuf {

template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelInstanceGroup_Kind> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelInstanceGroup_Kind>() {
  return ::nvidia::inferenceserver::ModelInstanceGroup_Kind_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelInput_Format> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelInput_Format>() {
  return ::nvidia::inferenceserver::ModelInput_Format_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority>() {
  return ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction>() {
  return ::nvidia::inferenceserver::ModelQueuePolicy_TimeoutAction_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind>() {
  return ::nvidia::inferenceserver::ModelSequenceBatching_Control_Kind_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::DataType> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::DataType>() {
  return ::nvidia::inferenceserver::DataType_descriptor();
}

}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_INCLUDED_model_5fconfig_2eproto
