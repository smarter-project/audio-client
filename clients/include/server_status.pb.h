// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: server_status.proto

#ifndef PROTOBUF_INCLUDED_server_5fstatus_2eproto
#define PROTOBUF_INCLUDED_server_5fstatus_2eproto

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3006001
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3006001 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/inlined_string_field.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/map.h>  // IWYU pragma: export
#include <google/protobuf/map_entry.h>
#include <google/protobuf/map_field_inl.h>
#include <google/protobuf/generated_enum_reflection.h>
#include <google/protobuf/unknown_field_set.h>
#include "model_config.pb.h"
// @@protoc_insertion_point(includes)
#define PROTOBUF_INTERNAL_EXPORT_protobuf_server_5fstatus_2eproto 

namespace protobuf_server_5fstatus_2eproto {
// Internal implementation detail -- do not use these members.
struct TableStruct {
  static const ::google::protobuf::internal::ParseTableField entries[];
  static const ::google::protobuf::internal::AuxillaryParseTableField aux[];
  static const ::google::protobuf::internal::ParseTable schema[20];
  static const ::google::protobuf::internal::FieldMetadata field_metadata[];
  static const ::google::protobuf::internal::SerializationTable serialization_table[];
  static const ::google::protobuf::uint32 offsets[];
};
void AddDescriptors();
}  // namespace protobuf_server_5fstatus_2eproto
namespace nvidia {
namespace inferenceserver {
class HealthRequestStats;
class HealthRequestStatsDefaultTypeInternal;
extern HealthRequestStatsDefaultTypeInternal _HealthRequestStats_default_instance_;
class InferRequestStats;
class InferRequestStatsDefaultTypeInternal;
extern InferRequestStatsDefaultTypeInternal _InferRequestStats_default_instance_;
class ModelControlRequestStats;
class ModelControlRequestStatsDefaultTypeInternal;
extern ModelControlRequestStatsDefaultTypeInternal _ModelControlRequestStats_default_instance_;
class ModelReadyStateReason;
class ModelReadyStateReasonDefaultTypeInternal;
extern ModelReadyStateReasonDefaultTypeInternal _ModelReadyStateReason_default_instance_;
class ModelRepositoryIndex;
class ModelRepositoryIndexDefaultTypeInternal;
extern ModelRepositoryIndexDefaultTypeInternal _ModelRepositoryIndex_default_instance_;
class ModelRepositoryIndex_ModelEntry;
class ModelRepositoryIndex_ModelEntryDefaultTypeInternal;
extern ModelRepositoryIndex_ModelEntryDefaultTypeInternal _ModelRepositoryIndex_ModelEntry_default_instance_;
class ModelStatus;
class ModelStatusDefaultTypeInternal;
extern ModelStatusDefaultTypeInternal _ModelStatus_default_instance_;
class ModelStatus_VersionStatusEntry_DoNotUse;
class ModelStatus_VersionStatusEntry_DoNotUseDefaultTypeInternal;
extern ModelStatus_VersionStatusEntry_DoNotUseDefaultTypeInternal _ModelStatus_VersionStatusEntry_DoNotUse_default_instance_;
class ModelVersionStatus;
class ModelVersionStatusDefaultTypeInternal;
extern ModelVersionStatusDefaultTypeInternal _ModelVersionStatus_default_instance_;
class ModelVersionStatus_InferStatsEntry_DoNotUse;
class ModelVersionStatus_InferStatsEntry_DoNotUseDefaultTypeInternal;
extern ModelVersionStatus_InferStatsEntry_DoNotUseDefaultTypeInternal _ModelVersionStatus_InferStatsEntry_DoNotUse_default_instance_;
class RepositoryRequestStats;
class RepositoryRequestStatsDefaultTypeInternal;
extern RepositoryRequestStatsDefaultTypeInternal _RepositoryRequestStats_default_instance_;
class ServerStatus;
class ServerStatusDefaultTypeInternal;
extern ServerStatusDefaultTypeInternal _ServerStatus_default_instance_;
class ServerStatus_ModelStatusEntry_DoNotUse;
class ServerStatus_ModelStatusEntry_DoNotUseDefaultTypeInternal;
extern ServerStatus_ModelStatusEntry_DoNotUseDefaultTypeInternal _ServerStatus_ModelStatusEntry_DoNotUse_default_instance_;
class SharedMemoryControlRequestStats;
class SharedMemoryControlRequestStatsDefaultTypeInternal;
extern SharedMemoryControlRequestStatsDefaultTypeInternal _SharedMemoryControlRequestStats_default_instance_;
class SharedMemoryRegion;
class SharedMemoryRegionDefaultTypeInternal;
extern SharedMemoryRegionDefaultTypeInternal _SharedMemoryRegion_default_instance_;
class SharedMemoryRegion_CudaSharedMemory;
class SharedMemoryRegion_CudaSharedMemoryDefaultTypeInternal;
extern SharedMemoryRegion_CudaSharedMemoryDefaultTypeInternal _SharedMemoryRegion_CudaSharedMemory_default_instance_;
class SharedMemoryRegion_SystemSharedMemory;
class SharedMemoryRegion_SystemSharedMemoryDefaultTypeInternal;
extern SharedMemoryRegion_SystemSharedMemoryDefaultTypeInternal _SharedMemoryRegion_SystemSharedMemory_default_instance_;
class SharedMemoryStatus;
class SharedMemoryStatusDefaultTypeInternal;
extern SharedMemoryStatusDefaultTypeInternal _SharedMemoryStatus_default_instance_;
class StatDuration;
class StatDurationDefaultTypeInternal;
extern StatDurationDefaultTypeInternal _StatDuration_default_instance_;
class StatusRequestStats;
class StatusRequestStatsDefaultTypeInternal;
extern StatusRequestStatsDefaultTypeInternal _StatusRequestStats_default_instance_;
}  // namespace inferenceserver
}  // namespace nvidia
namespace google {
namespace protobuf {
template<> ::nvidia::inferenceserver::HealthRequestStats* Arena::CreateMaybeMessage<::nvidia::inferenceserver::HealthRequestStats>(Arena*);
template<> ::nvidia::inferenceserver::InferRequestStats* Arena::CreateMaybeMessage<::nvidia::inferenceserver::InferRequestStats>(Arena*);
template<> ::nvidia::inferenceserver::ModelControlRequestStats* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelControlRequestStats>(Arena*);
template<> ::nvidia::inferenceserver::ModelReadyStateReason* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelReadyStateReason>(Arena*);
template<> ::nvidia::inferenceserver::ModelRepositoryIndex* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelRepositoryIndex>(Arena*);
template<> ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry>(Arena*);
template<> ::nvidia::inferenceserver::ModelStatus* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelStatus>(Arena*);
template<> ::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionStatus* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionStatus>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::RepositoryRequestStats* Arena::CreateMaybeMessage<::nvidia::inferenceserver::RepositoryRequestStats>(Arena*);
template<> ::nvidia::inferenceserver::ServerStatus* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ServerStatus>(Arena*);
template<> ::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::SharedMemoryControlRequestStats* Arena::CreateMaybeMessage<::nvidia::inferenceserver::SharedMemoryControlRequestStats>(Arena*);
template<> ::nvidia::inferenceserver::SharedMemoryRegion* Arena::CreateMaybeMessage<::nvidia::inferenceserver::SharedMemoryRegion>(Arena*);
template<> ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* Arena::CreateMaybeMessage<::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory>(Arena*);
template<> ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* Arena::CreateMaybeMessage<::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory>(Arena*);
template<> ::nvidia::inferenceserver::SharedMemoryStatus* Arena::CreateMaybeMessage<::nvidia::inferenceserver::SharedMemoryStatus>(Arena*);
template<> ::nvidia::inferenceserver::StatDuration* Arena::CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(Arena*);
template<> ::nvidia::inferenceserver::StatusRequestStats* Arena::CreateMaybeMessage<::nvidia::inferenceserver::StatusRequestStats>(Arena*);
}  // namespace protobuf
}  // namespace google
namespace nvidia {
namespace inferenceserver {

enum ModelReadyState {
  MODEL_UNKNOWN = 0,
  MODEL_READY = 1,
  MODEL_UNAVAILABLE = 2,
  MODEL_LOADING = 3,
  MODEL_UNLOADING = 4,
  ModelReadyState_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ModelReadyState_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ModelReadyState_IsValid(int value);
const ModelReadyState ModelReadyState_MIN = MODEL_UNKNOWN;
const ModelReadyState ModelReadyState_MAX = MODEL_UNLOADING;
const int ModelReadyState_ARRAYSIZE = ModelReadyState_MAX + 1;

const ::google::protobuf::EnumDescriptor* ModelReadyState_descriptor();
inline const ::std::string& ModelReadyState_Name(ModelReadyState value) {
  return ::google::protobuf::internal::NameOfEnum(
    ModelReadyState_descriptor(), value);
}
inline bool ModelReadyState_Parse(
    const ::std::string& name, ModelReadyState* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ModelReadyState>(
    ModelReadyState_descriptor(), name, value);
}
enum ServerReadyState {
  SERVER_INVALID = 0,
  SERVER_INITIALIZING = 1,
  SERVER_READY = 2,
  SERVER_EXITING = 3,
  SERVER_FAILED_TO_INITIALIZE = 10,
  ServerReadyState_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ServerReadyState_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ServerReadyState_IsValid(int value);
const ServerReadyState ServerReadyState_MIN = SERVER_INVALID;
const ServerReadyState ServerReadyState_MAX = SERVER_FAILED_TO_INITIALIZE;
const int ServerReadyState_ARRAYSIZE = ServerReadyState_MAX + 1;

const ::google::protobuf::EnumDescriptor* ServerReadyState_descriptor();
inline const ::std::string& ServerReadyState_Name(ServerReadyState value) {
  return ::google::protobuf::internal::NameOfEnum(
    ServerReadyState_descriptor(), value);
}
inline bool ServerReadyState_Parse(
    const ::std::string& name, ServerReadyState* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ServerReadyState>(
    ServerReadyState_descriptor(), name, value);
}
// ===================================================================

class StatDuration : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.StatDuration) */ {
 public:
  StatDuration();
  virtual ~StatDuration();

  StatDuration(const StatDuration& from);

  inline StatDuration& operator=(const StatDuration& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StatDuration(StatDuration&& from) noexcept
    : StatDuration() {
    *this = ::std::move(from);
  }

  inline StatDuration& operator=(StatDuration&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const StatDuration& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StatDuration* internal_default_instance() {
    return reinterpret_cast<const StatDuration*>(
               &_StatDuration_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  void Swap(StatDuration* other);
  friend void swap(StatDuration& a, StatDuration& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StatDuration* New() const final {
    return CreateMaybeMessage<StatDuration>(NULL);
  }

  StatDuration* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StatDuration>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StatDuration& from);
  void MergeFrom(const StatDuration& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StatDuration* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // uint64 count = 1;
  void clear_count();
  static const int kCountFieldNumber = 1;
  ::google::protobuf::uint64 count() const;
  void set_count(::google::protobuf::uint64 value);

  // uint64 total_time_ns = 2;
  void clear_total_time_ns();
  static const int kTotalTimeNsFieldNumber = 2;
  ::google::protobuf::uint64 total_time_ns() const;
  void set_total_time_ns(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.StatDuration)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::uint64 count_;
  ::google::protobuf::uint64 total_time_ns_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StatusRequestStats : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.StatusRequestStats) */ {
 public:
  StatusRequestStats();
  virtual ~StatusRequestStats();

  StatusRequestStats(const StatusRequestStats& from);

  inline StatusRequestStats& operator=(const StatusRequestStats& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StatusRequestStats(StatusRequestStats&& from) noexcept
    : StatusRequestStats() {
    *this = ::std::move(from);
  }

  inline StatusRequestStats& operator=(StatusRequestStats&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const StatusRequestStats& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StatusRequestStats* internal_default_instance() {
    return reinterpret_cast<const StatusRequestStats*>(
               &_StatusRequestStats_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  void Swap(StatusRequestStats* other);
  friend void swap(StatusRequestStats& a, StatusRequestStats& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StatusRequestStats* New() const final {
    return CreateMaybeMessage<StatusRequestStats>(NULL);
  }

  StatusRequestStats* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StatusRequestStats>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StatusRequestStats& from);
  void MergeFrom(const StatusRequestStats& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StatusRequestStats* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.StatDuration success = 1;
  bool has_success() const;
  void clear_success();
  static const int kSuccessFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_success() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& success() const;
  ::nvidia::inferenceserver::StatDuration* release_success();
  ::nvidia::inferenceserver::StatDuration* mutable_success();
  void set_allocated_success(::nvidia::inferenceserver::StatDuration* success);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.StatusRequestStats)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::nvidia::inferenceserver::StatDuration* success_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class HealthRequestStats : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.HealthRequestStats) */ {
 public:
  HealthRequestStats();
  virtual ~HealthRequestStats();

  HealthRequestStats(const HealthRequestStats& from);

  inline HealthRequestStats& operator=(const HealthRequestStats& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  HealthRequestStats(HealthRequestStats&& from) noexcept
    : HealthRequestStats() {
    *this = ::std::move(from);
  }

  inline HealthRequestStats& operator=(HealthRequestStats&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const HealthRequestStats& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const HealthRequestStats* internal_default_instance() {
    return reinterpret_cast<const HealthRequestStats*>(
               &_HealthRequestStats_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  void Swap(HealthRequestStats* other);
  friend void swap(HealthRequestStats& a, HealthRequestStats& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline HealthRequestStats* New() const final {
    return CreateMaybeMessage<HealthRequestStats>(NULL);
  }

  HealthRequestStats* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<HealthRequestStats>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const HealthRequestStats& from);
  void MergeFrom(const HealthRequestStats& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(HealthRequestStats* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.StatDuration success = 1;
  bool has_success() const;
  void clear_success();
  static const int kSuccessFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_success() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& success() const;
  ::nvidia::inferenceserver::StatDuration* release_success();
  ::nvidia::inferenceserver::StatDuration* mutable_success();
  void set_allocated_success(::nvidia::inferenceserver::StatDuration* success);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.HealthRequestStats)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::nvidia::inferenceserver::StatDuration* success_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelControlRequestStats : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelControlRequestStats) */ {
 public:
  ModelControlRequestStats();
  virtual ~ModelControlRequestStats();

  ModelControlRequestStats(const ModelControlRequestStats& from);

  inline ModelControlRequestStats& operator=(const ModelControlRequestStats& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelControlRequestStats(ModelControlRequestStats&& from) noexcept
    : ModelControlRequestStats() {
    *this = ::std::move(from);
  }

  inline ModelControlRequestStats& operator=(ModelControlRequestStats&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelControlRequestStats& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelControlRequestStats* internal_default_instance() {
    return reinterpret_cast<const ModelControlRequestStats*>(
               &_ModelControlRequestStats_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  void Swap(ModelControlRequestStats* other);
  friend void swap(ModelControlRequestStats& a, ModelControlRequestStats& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelControlRequestStats* New() const final {
    return CreateMaybeMessage<ModelControlRequestStats>(NULL);
  }

  ModelControlRequestStats* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelControlRequestStats>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelControlRequestStats& from);
  void MergeFrom(const ModelControlRequestStats& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelControlRequestStats* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.StatDuration success = 1;
  bool has_success() const;
  void clear_success();
  static const int kSuccessFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_success() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& success() const;
  ::nvidia::inferenceserver::StatDuration* release_success();
  ::nvidia::inferenceserver::StatDuration* mutable_success();
  void set_allocated_success(::nvidia::inferenceserver::StatDuration* success);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelControlRequestStats)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::nvidia::inferenceserver::StatDuration* success_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SharedMemoryControlRequestStats : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.SharedMemoryControlRequestStats) */ {
 public:
  SharedMemoryControlRequestStats();
  virtual ~SharedMemoryControlRequestStats();

  SharedMemoryControlRequestStats(const SharedMemoryControlRequestStats& from);

  inline SharedMemoryControlRequestStats& operator=(const SharedMemoryControlRequestStats& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SharedMemoryControlRequestStats(SharedMemoryControlRequestStats&& from) noexcept
    : SharedMemoryControlRequestStats() {
    *this = ::std::move(from);
  }

  inline SharedMemoryControlRequestStats& operator=(SharedMemoryControlRequestStats&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const SharedMemoryControlRequestStats& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SharedMemoryControlRequestStats* internal_default_instance() {
    return reinterpret_cast<const SharedMemoryControlRequestStats*>(
               &_SharedMemoryControlRequestStats_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  void Swap(SharedMemoryControlRequestStats* other);
  friend void swap(SharedMemoryControlRequestStats& a, SharedMemoryControlRequestStats& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SharedMemoryControlRequestStats* New() const final {
    return CreateMaybeMessage<SharedMemoryControlRequestStats>(NULL);
  }

  SharedMemoryControlRequestStats* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SharedMemoryControlRequestStats>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SharedMemoryControlRequestStats& from);
  void MergeFrom(const SharedMemoryControlRequestStats& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SharedMemoryControlRequestStats* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.StatDuration success = 1;
  bool has_success() const;
  void clear_success();
  static const int kSuccessFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_success() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& success() const;
  ::nvidia::inferenceserver::StatDuration* release_success();
  ::nvidia::inferenceserver::StatDuration* mutable_success();
  void set_allocated_success(::nvidia::inferenceserver::StatDuration* success);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SharedMemoryControlRequestStats)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::nvidia::inferenceserver::StatDuration* success_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RepositoryRequestStats : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.RepositoryRequestStats) */ {
 public:
  RepositoryRequestStats();
  virtual ~RepositoryRequestStats();

  RepositoryRequestStats(const RepositoryRequestStats& from);

  inline RepositoryRequestStats& operator=(const RepositoryRequestStats& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  RepositoryRequestStats(RepositoryRequestStats&& from) noexcept
    : RepositoryRequestStats() {
    *this = ::std::move(from);
  }

  inline RepositoryRequestStats& operator=(RepositoryRequestStats&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const RepositoryRequestStats& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const RepositoryRequestStats* internal_default_instance() {
    return reinterpret_cast<const RepositoryRequestStats*>(
               &_RepositoryRequestStats_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  void Swap(RepositoryRequestStats* other);
  friend void swap(RepositoryRequestStats& a, RepositoryRequestStats& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline RepositoryRequestStats* New() const final {
    return CreateMaybeMessage<RepositoryRequestStats>(NULL);
  }

  RepositoryRequestStats* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<RepositoryRequestStats>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const RepositoryRequestStats& from);
  void MergeFrom(const RepositoryRequestStats& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(RepositoryRequestStats* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.StatDuration success = 1;
  bool has_success() const;
  void clear_success();
  static const int kSuccessFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_success() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& success() const;
  ::nvidia::inferenceserver::StatDuration* release_success();
  ::nvidia::inferenceserver::StatDuration* mutable_success();
  void set_allocated_success(::nvidia::inferenceserver::StatDuration* success);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryRequestStats)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::nvidia::inferenceserver::StatDuration* success_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class InferRequestStats : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.InferRequestStats) */ {
 public:
  InferRequestStats();
  virtual ~InferRequestStats();

  InferRequestStats(const InferRequestStats& from);

  inline InferRequestStats& operator=(const InferRequestStats& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  InferRequestStats(InferRequestStats&& from) noexcept
    : InferRequestStats() {
    *this = ::std::move(from);
  }

  inline InferRequestStats& operator=(InferRequestStats&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const InferRequestStats& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const InferRequestStats* internal_default_instance() {
    return reinterpret_cast<const InferRequestStats*>(
               &_InferRequestStats_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  void Swap(InferRequestStats* other);
  friend void swap(InferRequestStats& a, InferRequestStats& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline InferRequestStats* New() const final {
    return CreateMaybeMessage<InferRequestStats>(NULL);
  }

  InferRequestStats* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<InferRequestStats>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const InferRequestStats& from);
  void MergeFrom(const InferRequestStats& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(InferRequestStats* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .nvidia.inferenceserver.StatDuration success = 1;
  bool has_success() const;
  void clear_success();
  static const int kSuccessFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_success() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& success() const;
  ::nvidia::inferenceserver::StatDuration* release_success();
  ::nvidia::inferenceserver::StatDuration* mutable_success();
  void set_allocated_success(::nvidia::inferenceserver::StatDuration* success);

  // .nvidia.inferenceserver.StatDuration failed = 2;
  bool has_failed() const;
  void clear_failed();
  static const int kFailedFieldNumber = 2;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_failed() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& failed() const;
  ::nvidia::inferenceserver::StatDuration* release_failed();
  ::nvidia::inferenceserver::StatDuration* mutable_failed();
  void set_allocated_failed(::nvidia::inferenceserver::StatDuration* failed);

  // .nvidia.inferenceserver.StatDuration compute = 3;
  bool has_compute() const;
  void clear_compute();
  static const int kComputeFieldNumber = 3;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_compute() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& compute() const;
  ::nvidia::inferenceserver::StatDuration* release_compute();
  ::nvidia::inferenceserver::StatDuration* mutable_compute();
  void set_allocated_compute(::nvidia::inferenceserver::StatDuration* compute);

  // .nvidia.inferenceserver.StatDuration queue = 4;
  bool has_queue() const;
  void clear_queue();
  static const int kQueueFieldNumber = 4;
  private:
  const ::nvidia::inferenceserver::StatDuration& _internal_queue() const;
  public:
  const ::nvidia::inferenceserver::StatDuration& queue() const;
  ::nvidia::inferenceserver::StatDuration* release_queue();
  ::nvidia::inferenceserver::StatDuration* mutable_queue();
  void set_allocated_queue(::nvidia::inferenceserver::StatDuration* queue);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferRequestStats)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::nvidia::inferenceserver::StatDuration* success_;
  ::nvidia::inferenceserver::StatDuration* failed_;
  ::nvidia::inferenceserver::StatDuration* compute_;
  ::nvidia::inferenceserver::StatDuration* queue_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelReadyStateReason : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelReadyStateReason) */ {
 public:
  ModelReadyStateReason();
  virtual ~ModelReadyStateReason();

  ModelReadyStateReason(const ModelReadyStateReason& from);

  inline ModelReadyStateReason& operator=(const ModelReadyStateReason& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelReadyStateReason(ModelReadyStateReason&& from) noexcept
    : ModelReadyStateReason() {
    *this = ::std::move(from);
  }

  inline ModelReadyStateReason& operator=(ModelReadyStateReason&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelReadyStateReason& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelReadyStateReason* internal_default_instance() {
    return reinterpret_cast<const ModelReadyStateReason*>(
               &_ModelReadyStateReason_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  void Swap(ModelReadyStateReason* other);
  friend void swap(ModelReadyStateReason& a, ModelReadyStateReason& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelReadyStateReason* New() const final {
    return CreateMaybeMessage<ModelReadyStateReason>(NULL);
  }

  ModelReadyStateReason* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelReadyStateReason>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelReadyStateReason& from);
  void MergeFrom(const ModelReadyStateReason& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelReadyStateReason* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string message = 1;
  void clear_message();
  static const int kMessageFieldNumber = 1;
  const ::std::string& message() const;
  void set_message(const ::std::string& value);
  #if LANG_CXX11
  void set_message(::std::string&& value);
  #endif
  void set_message(const char* value);
  void set_message(const char* value, size_t size);
  ::std::string* mutable_message();
  ::std::string* release_message();
  void set_allocated_message(::std::string* message);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelReadyStateReason)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr message_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelVersionStatus_InferStatsEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelVersionStatus_InferStatsEntry_DoNotUse, 
    ::google::protobuf::uint32, ::nvidia::inferenceserver::InferRequestStats,
    ::google::protobuf::internal::WireFormatLite::TYPE_UINT32,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelVersionStatus_InferStatsEntry_DoNotUse, 
    ::google::protobuf::uint32, ::nvidia::inferenceserver::InferRequestStats,
    ::google::protobuf::internal::WireFormatLite::TYPE_UINT32,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > SuperType;
  ModelVersionStatus_InferStatsEntry_DoNotUse();
  ModelVersionStatus_InferStatsEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelVersionStatus_InferStatsEntry_DoNotUse& other);
  static const ModelVersionStatus_InferStatsEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelVersionStatus_InferStatsEntry_DoNotUse*>(&_ModelVersionStatus_InferStatsEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelVersionStatus : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionStatus) */ {
 public:
  ModelVersionStatus();
  virtual ~ModelVersionStatus();

  ModelVersionStatus(const ModelVersionStatus& from);

  inline ModelVersionStatus& operator=(const ModelVersionStatus& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelVersionStatus(ModelVersionStatus&& from) noexcept
    : ModelVersionStatus() {
    *this = ::std::move(from);
  }

  inline ModelVersionStatus& operator=(ModelVersionStatus&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelVersionStatus& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelVersionStatus* internal_default_instance() {
    return reinterpret_cast<const ModelVersionStatus*>(
               &_ModelVersionStatus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    9;

  void Swap(ModelVersionStatus* other);
  friend void swap(ModelVersionStatus& a, ModelVersionStatus& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelVersionStatus* New() const final {
    return CreateMaybeMessage<ModelVersionStatus>(NULL);
  }

  ModelVersionStatus* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelVersionStatus>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelVersionStatus& from);
  void MergeFrom(const ModelVersionStatus& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionStatus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  // map<uint32, .nvidia.inferenceserver.InferRequestStats> infer_stats = 2;
  int infer_stats_size() const;
  void clear_infer_stats();
  static const int kInferStatsFieldNumber = 2;
  const ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::InferRequestStats >&
      infer_stats() const;
  ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::InferRequestStats >*
      mutable_infer_stats();

  // .nvidia.inferenceserver.ModelReadyStateReason ready_state_reason = 5;
  bool has_ready_state_reason() const;
  void clear_ready_state_reason();
  static const int kReadyStateReasonFieldNumber = 5;
  private:
  const ::nvidia::inferenceserver::ModelReadyStateReason& _internal_ready_state_reason() const;
  public:
  const ::nvidia::inferenceserver::ModelReadyStateReason& ready_state_reason() const;
  ::nvidia::inferenceserver::ModelReadyStateReason* release_ready_state_reason();
  ::nvidia::inferenceserver::ModelReadyStateReason* mutable_ready_state_reason();
  void set_allocated_ready_state_reason(::nvidia::inferenceserver::ModelReadyStateReason* ready_state_reason);

  // uint64 model_execution_count = 3;
  void clear_model_execution_count();
  static const int kModelExecutionCountFieldNumber = 3;
  ::google::protobuf::uint64 model_execution_count() const;
  void set_model_execution_count(::google::protobuf::uint64 value);

  // uint64 model_inference_count = 4;
  void clear_model_inference_count();
  static const int kModelInferenceCountFieldNumber = 4;
  ::google::protobuf::uint64 model_inference_count() const;
  void set_model_inference_count(::google::protobuf::uint64 value);

  // uint64 last_inference_timestamp_milliseconds = 6;
  void clear_last_inference_timestamp_milliseconds();
  static const int kLastInferenceTimestampMillisecondsFieldNumber = 6;
  ::google::protobuf::uint64 last_inference_timestamp_milliseconds() const;
  void set_last_inference_timestamp_milliseconds(::google::protobuf::uint64 value);

  // .nvidia.inferenceserver.ModelReadyState ready_state = 1;
  void clear_ready_state();
  static const int kReadyStateFieldNumber = 1;
  ::nvidia::inferenceserver::ModelReadyState ready_state() const;
  void set_ready_state(::nvidia::inferenceserver::ModelReadyState value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionStatus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::MapField<
      ModelVersionStatus_InferStatsEntry_DoNotUse,
      ::google::protobuf::uint32, ::nvidia::inferenceserver::InferRequestStats,
      ::google::protobuf::internal::WireFormatLite::TYPE_UINT32,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 > infer_stats_;
  ::nvidia::inferenceserver::ModelReadyStateReason* ready_state_reason_;
  ::google::protobuf::uint64 model_execution_count_;
  ::google::protobuf::uint64 model_inference_count_;
  ::google::protobuf::uint64 last_inference_timestamp_milliseconds_;
  int ready_state_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelStatus_VersionStatusEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ModelStatus_VersionStatusEntry_DoNotUse, 
    ::google::protobuf::int64, ::nvidia::inferenceserver::ModelVersionStatus,
    ::google::protobuf::internal::WireFormatLite::TYPE_INT64,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ModelStatus_VersionStatusEntry_DoNotUse, 
    ::google::protobuf::int64, ::nvidia::inferenceserver::ModelVersionStatus,
    ::google::protobuf::internal::WireFormatLite::TYPE_INT64,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > SuperType;
  ModelStatus_VersionStatusEntry_DoNotUse();
  ModelStatus_VersionStatusEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ModelStatus_VersionStatusEntry_DoNotUse& other);
  static const ModelStatus_VersionStatusEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelStatus_VersionStatusEntry_DoNotUse*>(&_ModelStatus_VersionStatusEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ModelStatus : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelStatus) */ {
 public:
  ModelStatus();
  virtual ~ModelStatus();

  ModelStatus(const ModelStatus& from);

  inline ModelStatus& operator=(const ModelStatus& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelStatus(ModelStatus&& from) noexcept
    : ModelStatus() {
    *this = ::std::move(from);
  }

  inline ModelStatus& operator=(ModelStatus&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelStatus& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelStatus* internal_default_instance() {
    return reinterpret_cast<const ModelStatus*>(
               &_ModelStatus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    11;

  void Swap(ModelStatus* other);
  friend void swap(ModelStatus& a, ModelStatus& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelStatus* New() const final {
    return CreateMaybeMessage<ModelStatus>(NULL);
  }

  ModelStatus* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelStatus>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelStatus& from);
  void MergeFrom(const ModelStatus& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelStatus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  // map<int64, .nvidia.inferenceserver.ModelVersionStatus> version_status = 2;
  int version_status_size() const;
  void clear_version_status();
  static const int kVersionStatusFieldNumber = 2;
  const ::google::protobuf::Map< ::google::protobuf::int64, ::nvidia::inferenceserver::ModelVersionStatus >&
      version_status() const;
  ::google::protobuf::Map< ::google::protobuf::int64, ::nvidia::inferenceserver::ModelVersionStatus >*
      mutable_version_status();

  // .nvidia.inferenceserver.ModelConfig config = 1;
  bool has_config() const;
  void clear_config();
  static const int kConfigFieldNumber = 1;
  private:
  const ::nvidia::inferenceserver::ModelConfig& _internal_config() const;
  public:
  const ::nvidia::inferenceserver::ModelConfig& config() const;
  ::nvidia::inferenceserver::ModelConfig* release_config();
  ::nvidia::inferenceserver::ModelConfig* mutable_config();
  void set_allocated_config(::nvidia::inferenceserver::ModelConfig* config);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelStatus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::MapField<
      ModelStatus_VersionStatusEntry_DoNotUse,
      ::google::protobuf::int64, ::nvidia::inferenceserver::ModelVersionStatus,
      ::google::protobuf::internal::WireFormatLite::TYPE_INT64,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 > version_status_;
  ::nvidia::inferenceserver::ModelConfig* config_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SharedMemoryRegion_SystemSharedMemory : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory) */ {
 public:
  SharedMemoryRegion_SystemSharedMemory();
  virtual ~SharedMemoryRegion_SystemSharedMemory();

  SharedMemoryRegion_SystemSharedMemory(const SharedMemoryRegion_SystemSharedMemory& from);

  inline SharedMemoryRegion_SystemSharedMemory& operator=(const SharedMemoryRegion_SystemSharedMemory& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SharedMemoryRegion_SystemSharedMemory(SharedMemoryRegion_SystemSharedMemory&& from) noexcept
    : SharedMemoryRegion_SystemSharedMemory() {
    *this = ::std::move(from);
  }

  inline SharedMemoryRegion_SystemSharedMemory& operator=(SharedMemoryRegion_SystemSharedMemory&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const SharedMemoryRegion_SystemSharedMemory& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SharedMemoryRegion_SystemSharedMemory* internal_default_instance() {
    return reinterpret_cast<const SharedMemoryRegion_SystemSharedMemory*>(
               &_SharedMemoryRegion_SystemSharedMemory_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    12;

  void Swap(SharedMemoryRegion_SystemSharedMemory* other);
  friend void swap(SharedMemoryRegion_SystemSharedMemory& a, SharedMemoryRegion_SystemSharedMemory& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SharedMemoryRegion_SystemSharedMemory* New() const final {
    return CreateMaybeMessage<SharedMemoryRegion_SystemSharedMemory>(NULL);
  }

  SharedMemoryRegion_SystemSharedMemory* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SharedMemoryRegion_SystemSharedMemory>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SharedMemoryRegion_SystemSharedMemory& from);
  void MergeFrom(const SharedMemoryRegion_SystemSharedMemory& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SharedMemoryRegion_SystemSharedMemory* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string shared_memory_key = 1;
  void clear_shared_memory_key();
  static const int kSharedMemoryKeyFieldNumber = 1;
  const ::std::string& shared_memory_key() const;
  void set_shared_memory_key(const ::std::string& value);
  #if LANG_CXX11
  void set_shared_memory_key(::std::string&& value);
  #endif
  void set_shared_memory_key(const char* value);
  void set_shared_memory_key(const char* value, size_t size);
  ::std::string* mutable_shared_memory_key();
  ::std::string* release_shared_memory_key();
  void set_allocated_shared_memory_key(::std::string* shared_memory_key);

  // uint64 offset = 2;
  void clear_offset();
  static const int kOffsetFieldNumber = 2;
  ::google::protobuf::uint64 offset() const;
  void set_offset(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr shared_memory_key_;
  ::google::protobuf::uint64 offset_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SharedMemoryRegion_CudaSharedMemory : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.SharedMemoryRegion.CudaSharedMemory) */ {
 public:
  SharedMemoryRegion_CudaSharedMemory();
  virtual ~SharedMemoryRegion_CudaSharedMemory();

  SharedMemoryRegion_CudaSharedMemory(const SharedMemoryRegion_CudaSharedMemory& from);

  inline SharedMemoryRegion_CudaSharedMemory& operator=(const SharedMemoryRegion_CudaSharedMemory& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SharedMemoryRegion_CudaSharedMemory(SharedMemoryRegion_CudaSharedMemory&& from) noexcept
    : SharedMemoryRegion_CudaSharedMemory() {
    *this = ::std::move(from);
  }

  inline SharedMemoryRegion_CudaSharedMemory& operator=(SharedMemoryRegion_CudaSharedMemory&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const SharedMemoryRegion_CudaSharedMemory& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SharedMemoryRegion_CudaSharedMemory* internal_default_instance() {
    return reinterpret_cast<const SharedMemoryRegion_CudaSharedMemory*>(
               &_SharedMemoryRegion_CudaSharedMemory_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    13;

  void Swap(SharedMemoryRegion_CudaSharedMemory* other);
  friend void swap(SharedMemoryRegion_CudaSharedMemory& a, SharedMemoryRegion_CudaSharedMemory& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SharedMemoryRegion_CudaSharedMemory* New() const final {
    return CreateMaybeMessage<SharedMemoryRegion_CudaSharedMemory>(NULL);
  }

  SharedMemoryRegion_CudaSharedMemory* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SharedMemoryRegion_CudaSharedMemory>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SharedMemoryRegion_CudaSharedMemory& from);
  void MergeFrom(const SharedMemoryRegion_CudaSharedMemory& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SharedMemoryRegion_CudaSharedMemory* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int64 device_id = 1;
  void clear_device_id();
  static const int kDeviceIdFieldNumber = 1;
  ::google::protobuf::int64 device_id() const;
  void set_device_id(::google::protobuf::int64 value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SharedMemoryRegion.CudaSharedMemory)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::int64 device_id_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SharedMemoryRegion : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.SharedMemoryRegion) */ {
 public:
  SharedMemoryRegion();
  virtual ~SharedMemoryRegion();

  SharedMemoryRegion(const SharedMemoryRegion& from);

  inline SharedMemoryRegion& operator=(const SharedMemoryRegion& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SharedMemoryRegion(SharedMemoryRegion&& from) noexcept
    : SharedMemoryRegion() {
    *this = ::std::move(from);
  }

  inline SharedMemoryRegion& operator=(SharedMemoryRegion&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const SharedMemoryRegion& default_instance();

  enum SharedMemoryTypesCase {
    kSystemSharedMemory = 2,
    kCudaSharedMemory = 3,
    SHARED_MEMORY_TYPES_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SharedMemoryRegion* internal_default_instance() {
    return reinterpret_cast<const SharedMemoryRegion*>(
               &_SharedMemoryRegion_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    14;

  void Swap(SharedMemoryRegion* other);
  friend void swap(SharedMemoryRegion& a, SharedMemoryRegion& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SharedMemoryRegion* New() const final {
    return CreateMaybeMessage<SharedMemoryRegion>(NULL);
  }

  SharedMemoryRegion* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SharedMemoryRegion>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SharedMemoryRegion& from);
  void MergeFrom(const SharedMemoryRegion& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SharedMemoryRegion* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef SharedMemoryRegion_SystemSharedMemory SystemSharedMemory;
  typedef SharedMemoryRegion_CudaSharedMemory CudaSharedMemory;

  // accessors -------------------------------------------------------

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // uint64 byte_size = 5;
  void clear_byte_size();
  static const int kByteSizeFieldNumber = 5;
  ::google::protobuf::uint64 byte_size() const;
  void set_byte_size(::google::protobuf::uint64 value);

  // .nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory system_shared_memory = 2;
  bool has_system_shared_memory() const;
  void clear_system_shared_memory();
  static const int kSystemSharedMemoryFieldNumber = 2;
  private:
  const ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory& _internal_system_shared_memory() const;
  public:
  const ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory& system_shared_memory() const;
  ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* release_system_shared_memory();
  ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* mutable_system_shared_memory();
  void set_allocated_system_shared_memory(::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* system_shared_memory);

  // .nvidia.inferenceserver.SharedMemoryRegion.CudaSharedMemory cuda_shared_memory = 3;
  bool has_cuda_shared_memory() const;
  void clear_cuda_shared_memory();
  static const int kCudaSharedMemoryFieldNumber = 3;
  private:
  const ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory& _internal_cuda_shared_memory() const;
  public:
  const ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory& cuda_shared_memory() const;
  ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* release_cuda_shared_memory();
  ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* mutable_cuda_shared_memory();
  void set_allocated_cuda_shared_memory(::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* cuda_shared_memory);

  void clear_shared_memory_types();
  SharedMemoryTypesCase shared_memory_types_case() const;
  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SharedMemoryRegion)
 private:
  void set_has_system_shared_memory();
  void set_has_cuda_shared_memory();

  inline bool has_shared_memory_types() const;
  inline void clear_has_shared_memory_types();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  ::google::protobuf::uint64 byte_size_;
  union SharedMemoryTypesUnion {
    SharedMemoryTypesUnion() {}
    ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* system_shared_memory_;
    ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* cuda_shared_memory_;
  } shared_memory_types_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ServerStatus_ModelStatusEntry_DoNotUse : public ::google::protobuf::internal::MapEntry<ServerStatus_ModelStatusEntry_DoNotUse, 
    ::std::string, ::nvidia::inferenceserver::ModelStatus,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > {
public:
  typedef ::google::protobuf::internal::MapEntry<ServerStatus_ModelStatusEntry_DoNotUse, 
    ::std::string, ::nvidia::inferenceserver::ModelStatus,
    ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
    ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
    0 > SuperType;
  ServerStatus_ModelStatusEntry_DoNotUse();
  ServerStatus_ModelStatusEntry_DoNotUse(::google::protobuf::Arena* arena);
  void MergeFrom(const ServerStatus_ModelStatusEntry_DoNotUse& other);
  static const ServerStatus_ModelStatusEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ServerStatus_ModelStatusEntry_DoNotUse*>(&_ServerStatus_ModelStatusEntry_DoNotUse_default_instance_); }
  void MergeFrom(const ::google::protobuf::Message& other) final;
  ::google::protobuf::Metadata GetMetadata() const;
};

// -------------------------------------------------------------------

class ServerStatus : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ServerStatus) */ {
 public:
  ServerStatus();
  virtual ~ServerStatus();

  ServerStatus(const ServerStatus& from);

  inline ServerStatus& operator=(const ServerStatus& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ServerStatus(ServerStatus&& from) noexcept
    : ServerStatus() {
    *this = ::std::move(from);
  }

  inline ServerStatus& operator=(ServerStatus&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ServerStatus& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ServerStatus* internal_default_instance() {
    return reinterpret_cast<const ServerStatus*>(
               &_ServerStatus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    16;

  void Swap(ServerStatus* other);
  friend void swap(ServerStatus& a, ServerStatus& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ServerStatus* New() const final {
    return CreateMaybeMessage<ServerStatus>(NULL);
  }

  ServerStatus* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ServerStatus>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ServerStatus& from);
  void MergeFrom(const ServerStatus& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ServerStatus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  // map<string, .nvidia.inferenceserver.ModelStatus> model_status = 4;
  int model_status_size() const;
  void clear_model_status();
  static const int kModelStatusFieldNumber = 4;
  const ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelStatus >&
      model_status() const;
  ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelStatus >*
      mutable_model_status();

  // string id = 1;
  void clear_id();
  static const int kIdFieldNumber = 1;
  const ::std::string& id() const;
  void set_id(const ::std::string& value);
  #if LANG_CXX11
  void set_id(::std::string&& value);
  #endif
  void set_id(const char* value);
  void set_id(const char* value, size_t size);
  ::std::string* mutable_id();
  ::std::string* release_id();
  void set_allocated_id(::std::string* id);

  // string version = 2;
  void clear_version();
  static const int kVersionFieldNumber = 2;
  const ::std::string& version() const;
  void set_version(const ::std::string& value);
  #if LANG_CXX11
  void set_version(::std::string&& value);
  #endif
  void set_version(const char* value);
  void set_version(const char* value, size_t size);
  ::std::string* mutable_version();
  ::std::string* release_version();
  void set_allocated_version(::std::string* version);

  // .nvidia.inferenceserver.StatusRequestStats status_stats = 5;
  bool has_status_stats() const;
  void clear_status_stats();
  static const int kStatusStatsFieldNumber = 5;
  private:
  const ::nvidia::inferenceserver::StatusRequestStats& _internal_status_stats() const;
  public:
  const ::nvidia::inferenceserver::StatusRequestStats& status_stats() const;
  ::nvidia::inferenceserver::StatusRequestStats* release_status_stats();
  ::nvidia::inferenceserver::StatusRequestStats* mutable_status_stats();
  void set_allocated_status_stats(::nvidia::inferenceserver::StatusRequestStats* status_stats);

  // .nvidia.inferenceserver.HealthRequestStats health_stats = 8;
  bool has_health_stats() const;
  void clear_health_stats();
  static const int kHealthStatsFieldNumber = 8;
  private:
  const ::nvidia::inferenceserver::HealthRequestStats& _internal_health_stats() const;
  public:
  const ::nvidia::inferenceserver::HealthRequestStats& health_stats() const;
  ::nvidia::inferenceserver::HealthRequestStats* release_health_stats();
  ::nvidia::inferenceserver::HealthRequestStats* mutable_health_stats();
  void set_allocated_health_stats(::nvidia::inferenceserver::HealthRequestStats* health_stats);

  // .nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;
  bool has_model_control_stats() const;
  void clear_model_control_stats();
  static const int kModelControlStatsFieldNumber = 9;
  private:
  const ::nvidia::inferenceserver::ModelControlRequestStats& _internal_model_control_stats() const;
  public:
  const ::nvidia::inferenceserver::ModelControlRequestStats& model_control_stats() const;
  ::nvidia::inferenceserver::ModelControlRequestStats* release_model_control_stats();
  ::nvidia::inferenceserver::ModelControlRequestStats* mutable_model_control_stats();
  void set_allocated_model_control_stats(::nvidia::inferenceserver::ModelControlRequestStats* model_control_stats);

  // .nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;
  bool has_shm_control_stats() const;
  void clear_shm_control_stats();
  static const int kShmControlStatsFieldNumber = 10;
  private:
  const ::nvidia::inferenceserver::SharedMemoryControlRequestStats& _internal_shm_control_stats() const;
  public:
  const ::nvidia::inferenceserver::SharedMemoryControlRequestStats& shm_control_stats() const;
  ::nvidia::inferenceserver::SharedMemoryControlRequestStats* release_shm_control_stats();
  ::nvidia::inferenceserver::SharedMemoryControlRequestStats* mutable_shm_control_stats();
  void set_allocated_shm_control_stats(::nvidia::inferenceserver::SharedMemoryControlRequestStats* shm_control_stats);

  // .nvidia.inferenceserver.RepositoryRequestStats repository_stats = 11;
  bool has_repository_stats() const;
  void clear_repository_stats();
  static const int kRepositoryStatsFieldNumber = 11;
  private:
  const ::nvidia::inferenceserver::RepositoryRequestStats& _internal_repository_stats() const;
  public:
  const ::nvidia::inferenceserver::RepositoryRequestStats& repository_stats() const;
  ::nvidia::inferenceserver::RepositoryRequestStats* release_repository_stats();
  ::nvidia::inferenceserver::RepositoryRequestStats* mutable_repository_stats();
  void set_allocated_repository_stats(::nvidia::inferenceserver::RepositoryRequestStats* repository_stats);

  // uint64 uptime_ns = 3;
  void clear_uptime_ns();
  static const int kUptimeNsFieldNumber = 3;
  ::google::protobuf::uint64 uptime_ns() const;
  void set_uptime_ns(::google::protobuf::uint64 value);

  // .nvidia.inferenceserver.ServerReadyState ready_state = 7;
  void clear_ready_state();
  static const int kReadyStateFieldNumber = 7;
  ::nvidia::inferenceserver::ServerReadyState ready_state() const;
  void set_ready_state(::nvidia::inferenceserver::ServerReadyState value);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerStatus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::MapField<
      ServerStatus_ModelStatusEntry_DoNotUse,
      ::std::string, ::nvidia::inferenceserver::ModelStatus,
      ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
      ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
      0 > model_status_;
  ::google::protobuf::internal::ArenaStringPtr id_;
  ::google::protobuf::internal::ArenaStringPtr version_;
  ::nvidia::inferenceserver::StatusRequestStats* status_stats_;
  ::nvidia::inferenceserver::HealthRequestStats* health_stats_;
  ::nvidia::inferenceserver::ModelControlRequestStats* model_control_stats_;
  ::nvidia::inferenceserver::SharedMemoryControlRequestStats* shm_control_stats_;
  ::nvidia::inferenceserver::RepositoryRequestStats* repository_stats_;
  ::google::protobuf::uint64 uptime_ns_;
  int ready_state_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SharedMemoryStatus : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.SharedMemoryStatus) */ {
 public:
  SharedMemoryStatus();
  virtual ~SharedMemoryStatus();

  SharedMemoryStatus(const SharedMemoryStatus& from);

  inline SharedMemoryStatus& operator=(const SharedMemoryStatus& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SharedMemoryStatus(SharedMemoryStatus&& from) noexcept
    : SharedMemoryStatus() {
    *this = ::std::move(from);
  }

  inline SharedMemoryStatus& operator=(SharedMemoryStatus&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const SharedMemoryStatus& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SharedMemoryStatus* internal_default_instance() {
    return reinterpret_cast<const SharedMemoryStatus*>(
               &_SharedMemoryStatus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    17;

  void Swap(SharedMemoryStatus* other);
  friend void swap(SharedMemoryStatus& a, SharedMemoryStatus& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SharedMemoryStatus* New() const final {
    return CreateMaybeMessage<SharedMemoryStatus>(NULL);
  }

  SharedMemoryStatus* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SharedMemoryStatus>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SharedMemoryStatus& from);
  void MergeFrom(const SharedMemoryStatus& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SharedMemoryStatus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .nvidia.inferenceserver.SharedMemoryRegion shared_memory_region = 2;
  int shared_memory_region_size() const;
  void clear_shared_memory_region();
  static const int kSharedMemoryRegionFieldNumber = 2;
  ::nvidia::inferenceserver::SharedMemoryRegion* mutable_shared_memory_region(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::SharedMemoryRegion >*
      mutable_shared_memory_region();
  const ::nvidia::inferenceserver::SharedMemoryRegion& shared_memory_region(int index) const;
  ::nvidia::inferenceserver::SharedMemoryRegion* add_shared_memory_region();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::SharedMemoryRegion >&
      shared_memory_region() const;

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SharedMemoryStatus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::SharedMemoryRegion > shared_memory_region_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelRepositoryIndex_ModelEntry : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry) */ {
 public:
  ModelRepositoryIndex_ModelEntry();
  virtual ~ModelRepositoryIndex_ModelEntry();

  ModelRepositoryIndex_ModelEntry(const ModelRepositoryIndex_ModelEntry& from);

  inline ModelRepositoryIndex_ModelEntry& operator=(const ModelRepositoryIndex_ModelEntry& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelRepositoryIndex_ModelEntry(ModelRepositoryIndex_ModelEntry&& from) noexcept
    : ModelRepositoryIndex_ModelEntry() {
    *this = ::std::move(from);
  }

  inline ModelRepositoryIndex_ModelEntry& operator=(ModelRepositoryIndex_ModelEntry&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelRepositoryIndex_ModelEntry& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelRepositoryIndex_ModelEntry* internal_default_instance() {
    return reinterpret_cast<const ModelRepositoryIndex_ModelEntry*>(
               &_ModelRepositoryIndex_ModelEntry_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    18;

  void Swap(ModelRepositoryIndex_ModelEntry* other);
  friend void swap(ModelRepositoryIndex_ModelEntry& a, ModelRepositoryIndex_ModelEntry& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelRepositoryIndex_ModelEntry* New() const final {
    return CreateMaybeMessage<ModelRepositoryIndex_ModelEntry>(NULL);
  }

  ModelRepositoryIndex_ModelEntry* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelRepositoryIndex_ModelEntry>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelRepositoryIndex_ModelEntry& from);
  void MergeFrom(const ModelRepositoryIndex_ModelEntry& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelRepositoryIndex_ModelEntry* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ModelRepositoryIndex : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelRepositoryIndex) */ {
 public:
  ModelRepositoryIndex();
  virtual ~ModelRepositoryIndex();

  ModelRepositoryIndex(const ModelRepositoryIndex& from);

  inline ModelRepositoryIndex& operator=(const ModelRepositoryIndex& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ModelRepositoryIndex(ModelRepositoryIndex&& from) noexcept
    : ModelRepositoryIndex() {
    *this = ::std::move(from);
  }

  inline ModelRepositoryIndex& operator=(ModelRepositoryIndex&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ModelRepositoryIndex& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ModelRepositoryIndex* internal_default_instance() {
    return reinterpret_cast<const ModelRepositoryIndex*>(
               &_ModelRepositoryIndex_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    19;

  void Swap(ModelRepositoryIndex* other);
  friend void swap(ModelRepositoryIndex& a, ModelRepositoryIndex& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ModelRepositoryIndex* New() const final {
    return CreateMaybeMessage<ModelRepositoryIndex>(NULL);
  }

  ModelRepositoryIndex* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ModelRepositoryIndex>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ModelRepositoryIndex& from);
  void MergeFrom(const ModelRepositoryIndex& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelRepositoryIndex* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelRepositoryIndex_ModelEntry ModelEntry;

  // accessors -------------------------------------------------------

  // repeated .nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry models = 1;
  int models_size() const;
  void clear_models();
  static const int kModelsFieldNumber = 1;
  ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry* mutable_models(int index);
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry >*
      mutable_models();
  const ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry& models(int index) const;
  ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry* add_models();
  const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry >&
      models() const;

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelRepositoryIndex)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry > models_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_server_5fstatus_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// StatDuration

// uint64 count = 1;
inline void StatDuration::clear_count() {
  count_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 StatDuration::count() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.StatDuration.count)
  return count_;
}
inline void StatDuration::set_count(::google::protobuf::uint64 value) {
  
  count_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.StatDuration.count)
}

// uint64 total_time_ns = 2;
inline void StatDuration::clear_total_time_ns() {
  total_time_ns_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 StatDuration::total_time_ns() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.StatDuration.total_time_ns)
  return total_time_ns_;
}
inline void StatDuration::set_total_time_ns(::google::protobuf::uint64 value) {
  
  total_time_ns_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.StatDuration.total_time_ns)
}

// -------------------------------------------------------------------

// StatusRequestStats

// .nvidia.inferenceserver.StatDuration success = 1;
inline bool StatusRequestStats::has_success() const {
  return this != internal_default_instance() && success_ != NULL;
}
inline void StatusRequestStats::clear_success() {
  if (GetArenaNoVirtual() == NULL && success_ != NULL) {
    delete success_;
  }
  success_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& StatusRequestStats::_internal_success() const {
  return *success_;
}
inline const ::nvidia::inferenceserver::StatDuration& StatusRequestStats::success() const {
  const ::nvidia::inferenceserver::StatDuration* p = success_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.StatusRequestStats.success)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* StatusRequestStats::release_success() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.StatusRequestStats.success)
  
  ::nvidia::inferenceserver::StatDuration* temp = success_;
  success_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* StatusRequestStats::mutable_success() {
  
  if (success_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    success_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.StatusRequestStats.success)
  return success_;
}
inline void StatusRequestStats::set_allocated_success(::nvidia::inferenceserver::StatDuration* success) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete success_;
  }
  if (success) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      success = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, success, submessage_arena);
    }
    
  } else {
    
  }
  success_ = success;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.StatusRequestStats.success)
}

// -------------------------------------------------------------------

// HealthRequestStats

// .nvidia.inferenceserver.StatDuration success = 1;
inline bool HealthRequestStats::has_success() const {
  return this != internal_default_instance() && success_ != NULL;
}
inline void HealthRequestStats::clear_success() {
  if (GetArenaNoVirtual() == NULL && success_ != NULL) {
    delete success_;
  }
  success_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& HealthRequestStats::_internal_success() const {
  return *success_;
}
inline const ::nvidia::inferenceserver::StatDuration& HealthRequestStats::success() const {
  const ::nvidia::inferenceserver::StatDuration* p = success_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.HealthRequestStats.success)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* HealthRequestStats::release_success() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.HealthRequestStats.success)
  
  ::nvidia::inferenceserver::StatDuration* temp = success_;
  success_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* HealthRequestStats::mutable_success() {
  
  if (success_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    success_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.HealthRequestStats.success)
  return success_;
}
inline void HealthRequestStats::set_allocated_success(::nvidia::inferenceserver::StatDuration* success) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete success_;
  }
  if (success) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      success = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, success, submessage_arena);
    }
    
  } else {
    
  }
  success_ = success;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.HealthRequestStats.success)
}

// -------------------------------------------------------------------

// ModelControlRequestStats

// .nvidia.inferenceserver.StatDuration success = 1;
inline bool ModelControlRequestStats::has_success() const {
  return this != internal_default_instance() && success_ != NULL;
}
inline void ModelControlRequestStats::clear_success() {
  if (GetArenaNoVirtual() == NULL && success_ != NULL) {
    delete success_;
  }
  success_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& ModelControlRequestStats::_internal_success() const {
  return *success_;
}
inline const ::nvidia::inferenceserver::StatDuration& ModelControlRequestStats::success() const {
  const ::nvidia::inferenceserver::StatDuration* p = success_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelControlRequestStats.success)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* ModelControlRequestStats::release_success() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelControlRequestStats.success)
  
  ::nvidia::inferenceserver::StatDuration* temp = success_;
  success_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* ModelControlRequestStats::mutable_success() {
  
  if (success_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    success_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelControlRequestStats.success)
  return success_;
}
inline void ModelControlRequestStats::set_allocated_success(::nvidia::inferenceserver::StatDuration* success) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete success_;
  }
  if (success) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      success = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, success, submessage_arena);
    }
    
  } else {
    
  }
  success_ = success;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelControlRequestStats.success)
}

// -------------------------------------------------------------------

// SharedMemoryControlRequestStats

// .nvidia.inferenceserver.StatDuration success = 1;
inline bool SharedMemoryControlRequestStats::has_success() const {
  return this != internal_default_instance() && success_ != NULL;
}
inline void SharedMemoryControlRequestStats::clear_success() {
  if (GetArenaNoVirtual() == NULL && success_ != NULL) {
    delete success_;
  }
  success_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& SharedMemoryControlRequestStats::_internal_success() const {
  return *success_;
}
inline const ::nvidia::inferenceserver::StatDuration& SharedMemoryControlRequestStats::success() const {
  const ::nvidia::inferenceserver::StatDuration* p = success_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryControlRequestStats.success)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* SharedMemoryControlRequestStats::release_success() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.SharedMemoryControlRequestStats.success)
  
  ::nvidia::inferenceserver::StatDuration* temp = success_;
  success_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* SharedMemoryControlRequestStats::mutable_success() {
  
  if (success_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    success_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.SharedMemoryControlRequestStats.success)
  return success_;
}
inline void SharedMemoryControlRequestStats::set_allocated_success(::nvidia::inferenceserver::StatDuration* success) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete success_;
  }
  if (success) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      success = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, success, submessage_arena);
    }
    
  } else {
    
  }
  success_ = success;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.SharedMemoryControlRequestStats.success)
}

// -------------------------------------------------------------------

// RepositoryRequestStats

// .nvidia.inferenceserver.StatDuration success = 1;
inline bool RepositoryRequestStats::has_success() const {
  return this != internal_default_instance() && success_ != NULL;
}
inline void RepositoryRequestStats::clear_success() {
  if (GetArenaNoVirtual() == NULL && success_ != NULL) {
    delete success_;
  }
  success_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& RepositoryRequestStats::_internal_success() const {
  return *success_;
}
inline const ::nvidia::inferenceserver::StatDuration& RepositoryRequestStats::success() const {
  const ::nvidia::inferenceserver::StatDuration* p = success_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.RepositoryRequestStats.success)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* RepositoryRequestStats::release_success() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.RepositoryRequestStats.success)
  
  ::nvidia::inferenceserver::StatDuration* temp = success_;
  success_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* RepositoryRequestStats::mutable_success() {
  
  if (success_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    success_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.RepositoryRequestStats.success)
  return success_;
}
inline void RepositoryRequestStats::set_allocated_success(::nvidia::inferenceserver::StatDuration* success) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete success_;
  }
  if (success) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      success = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, success, submessage_arena);
    }
    
  } else {
    
  }
  success_ = success;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.RepositoryRequestStats.success)
}

// -------------------------------------------------------------------

// InferRequestStats

// .nvidia.inferenceserver.StatDuration success = 1;
inline bool InferRequestStats::has_success() const {
  return this != internal_default_instance() && success_ != NULL;
}
inline void InferRequestStats::clear_success() {
  if (GetArenaNoVirtual() == NULL && success_ != NULL) {
    delete success_;
  }
  success_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::_internal_success() const {
  return *success_;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::success() const {
  const ::nvidia::inferenceserver::StatDuration* p = success_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.InferRequestStats.success)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::release_success() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.InferRequestStats.success)
  
  ::nvidia::inferenceserver::StatDuration* temp = success_;
  success_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::mutable_success() {
  
  if (success_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    success_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.InferRequestStats.success)
  return success_;
}
inline void InferRequestStats::set_allocated_success(::nvidia::inferenceserver::StatDuration* success) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete success_;
  }
  if (success) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      success = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, success, submessage_arena);
    }
    
  } else {
    
  }
  success_ = success;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.InferRequestStats.success)
}

// .nvidia.inferenceserver.StatDuration failed = 2;
inline bool InferRequestStats::has_failed() const {
  return this != internal_default_instance() && failed_ != NULL;
}
inline void InferRequestStats::clear_failed() {
  if (GetArenaNoVirtual() == NULL && failed_ != NULL) {
    delete failed_;
  }
  failed_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::_internal_failed() const {
  return *failed_;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::failed() const {
  const ::nvidia::inferenceserver::StatDuration* p = failed_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.InferRequestStats.failed)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::release_failed() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.InferRequestStats.failed)
  
  ::nvidia::inferenceserver::StatDuration* temp = failed_;
  failed_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::mutable_failed() {
  
  if (failed_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    failed_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.InferRequestStats.failed)
  return failed_;
}
inline void InferRequestStats::set_allocated_failed(::nvidia::inferenceserver::StatDuration* failed) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete failed_;
  }
  if (failed) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      failed = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, failed, submessage_arena);
    }
    
  } else {
    
  }
  failed_ = failed;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.InferRequestStats.failed)
}

// .nvidia.inferenceserver.StatDuration compute = 3;
inline bool InferRequestStats::has_compute() const {
  return this != internal_default_instance() && compute_ != NULL;
}
inline void InferRequestStats::clear_compute() {
  if (GetArenaNoVirtual() == NULL && compute_ != NULL) {
    delete compute_;
  }
  compute_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::_internal_compute() const {
  return *compute_;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::compute() const {
  const ::nvidia::inferenceserver::StatDuration* p = compute_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.InferRequestStats.compute)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::release_compute() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.InferRequestStats.compute)
  
  ::nvidia::inferenceserver::StatDuration* temp = compute_;
  compute_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::mutable_compute() {
  
  if (compute_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    compute_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.InferRequestStats.compute)
  return compute_;
}
inline void InferRequestStats::set_allocated_compute(::nvidia::inferenceserver::StatDuration* compute) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete compute_;
  }
  if (compute) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      compute = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, compute, submessage_arena);
    }
    
  } else {
    
  }
  compute_ = compute;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.InferRequestStats.compute)
}

// .nvidia.inferenceserver.StatDuration queue = 4;
inline bool InferRequestStats::has_queue() const {
  return this != internal_default_instance() && queue_ != NULL;
}
inline void InferRequestStats::clear_queue() {
  if (GetArenaNoVirtual() == NULL && queue_ != NULL) {
    delete queue_;
  }
  queue_ = NULL;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::_internal_queue() const {
  return *queue_;
}
inline const ::nvidia::inferenceserver::StatDuration& InferRequestStats::queue() const {
  const ::nvidia::inferenceserver::StatDuration* p = queue_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.InferRequestStats.queue)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatDuration*>(
      &::nvidia::inferenceserver::_StatDuration_default_instance_);
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::release_queue() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.InferRequestStats.queue)
  
  ::nvidia::inferenceserver::StatDuration* temp = queue_;
  queue_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatDuration* InferRequestStats::mutable_queue() {
  
  if (queue_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatDuration>(GetArenaNoVirtual());
    queue_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.InferRequestStats.queue)
  return queue_;
}
inline void InferRequestStats::set_allocated_queue(::nvidia::inferenceserver::StatDuration* queue) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete queue_;
  }
  if (queue) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      queue = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, queue, submessage_arena);
    }
    
  } else {
    
  }
  queue_ = queue;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.InferRequestStats.queue)
}

// -------------------------------------------------------------------

// ModelReadyStateReason

// string message = 1;
inline void ModelReadyStateReason::clear_message() {
  message_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelReadyStateReason::message() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelReadyStateReason.message)
  return message_.GetNoArena();
}
inline void ModelReadyStateReason::set_message(const ::std::string& value) {
  
  message_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelReadyStateReason.message)
}
#if LANG_CXX11
inline void ModelReadyStateReason::set_message(::std::string&& value) {
  
  message_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelReadyStateReason.message)
}
#endif
inline void ModelReadyStateReason::set_message(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  message_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelReadyStateReason.message)
}
inline void ModelReadyStateReason::set_message(const char* value, size_t size) {
  
  message_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelReadyStateReason.message)
}
inline ::std::string* ModelReadyStateReason::mutable_message() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelReadyStateReason.message)
  return message_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelReadyStateReason::release_message() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelReadyStateReason.message)
  
  return message_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelReadyStateReason::set_allocated_message(::std::string* message) {
  if (message != NULL) {
    
  } else {
    
  }
  message_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), message);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelReadyStateReason.message)
}

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelVersionStatus

// .nvidia.inferenceserver.ModelReadyState ready_state = 1;
inline void ModelVersionStatus::clear_ready_state() {
  ready_state_ = 0;
}
inline ::nvidia::inferenceserver::ModelReadyState ModelVersionStatus::ready_state() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionStatus.ready_state)
  return static_cast< ::nvidia::inferenceserver::ModelReadyState >(ready_state_);
}
inline void ModelVersionStatus::set_ready_state(::nvidia::inferenceserver::ModelReadyState value) {
  
  ready_state_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionStatus.ready_state)
}

// .nvidia.inferenceserver.ModelReadyStateReason ready_state_reason = 5;
inline bool ModelVersionStatus::has_ready_state_reason() const {
  return this != internal_default_instance() && ready_state_reason_ != NULL;
}
inline void ModelVersionStatus::clear_ready_state_reason() {
  if (GetArenaNoVirtual() == NULL && ready_state_reason_ != NULL) {
    delete ready_state_reason_;
  }
  ready_state_reason_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelReadyStateReason& ModelVersionStatus::_internal_ready_state_reason() const {
  return *ready_state_reason_;
}
inline const ::nvidia::inferenceserver::ModelReadyStateReason& ModelVersionStatus::ready_state_reason() const {
  const ::nvidia::inferenceserver::ModelReadyStateReason* p = ready_state_reason_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionStatus.ready_state_reason)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelReadyStateReason*>(
      &::nvidia::inferenceserver::_ModelReadyStateReason_default_instance_);
}
inline ::nvidia::inferenceserver::ModelReadyStateReason* ModelVersionStatus::release_ready_state_reason() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelVersionStatus.ready_state_reason)
  
  ::nvidia::inferenceserver::ModelReadyStateReason* temp = ready_state_reason_;
  ready_state_reason_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelReadyStateReason* ModelVersionStatus::mutable_ready_state_reason() {
  
  if (ready_state_reason_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelReadyStateReason>(GetArenaNoVirtual());
    ready_state_reason_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelVersionStatus.ready_state_reason)
  return ready_state_reason_;
}
inline void ModelVersionStatus::set_allocated_ready_state_reason(::nvidia::inferenceserver::ModelReadyStateReason* ready_state_reason) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete ready_state_reason_;
  }
  if (ready_state_reason) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      ready_state_reason = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, ready_state_reason, submessage_arena);
    }
    
  } else {
    
  }
  ready_state_reason_ = ready_state_reason;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelVersionStatus.ready_state_reason)
}

// map<uint32, .nvidia.inferenceserver.InferRequestStats> infer_stats = 2;
inline int ModelVersionStatus::infer_stats_size() const {
  return infer_stats_.size();
}
inline void ModelVersionStatus::clear_infer_stats() {
  infer_stats_.Clear();
}
inline const ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::InferRequestStats >&
ModelVersionStatus::infer_stats() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelVersionStatus.infer_stats)
  return infer_stats_.GetMap();
}
inline ::google::protobuf::Map< ::google::protobuf::uint32, ::nvidia::inferenceserver::InferRequestStats >*
ModelVersionStatus::mutable_infer_stats() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelVersionStatus.infer_stats)
  return infer_stats_.MutableMap();
}

// uint64 model_execution_count = 3;
inline void ModelVersionStatus::clear_model_execution_count() {
  model_execution_count_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ModelVersionStatus::model_execution_count() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionStatus.model_execution_count)
  return model_execution_count_;
}
inline void ModelVersionStatus::set_model_execution_count(::google::protobuf::uint64 value) {
  
  model_execution_count_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionStatus.model_execution_count)
}

// uint64 model_inference_count = 4;
inline void ModelVersionStatus::clear_model_inference_count() {
  model_inference_count_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ModelVersionStatus::model_inference_count() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionStatus.model_inference_count)
  return model_inference_count_;
}
inline void ModelVersionStatus::set_model_inference_count(::google::protobuf::uint64 value) {
  
  model_inference_count_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionStatus.model_inference_count)
}

// uint64 last_inference_timestamp_milliseconds = 6;
inline void ModelVersionStatus::clear_last_inference_timestamp_milliseconds() {
  last_inference_timestamp_milliseconds_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ModelVersionStatus::last_inference_timestamp_milliseconds() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionStatus.last_inference_timestamp_milliseconds)
  return last_inference_timestamp_milliseconds_;
}
inline void ModelVersionStatus::set_last_inference_timestamp_milliseconds(::google::protobuf::uint64 value) {
  
  last_inference_timestamp_milliseconds_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionStatus.last_inference_timestamp_milliseconds)
}

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelStatus

// .nvidia.inferenceserver.ModelConfig config = 1;
inline bool ModelStatus::has_config() const {
  return this != internal_default_instance() && config_ != NULL;
}
inline const ::nvidia::inferenceserver::ModelConfig& ModelStatus::_internal_config() const {
  return *config_;
}
inline const ::nvidia::inferenceserver::ModelConfig& ModelStatus::config() const {
  const ::nvidia::inferenceserver::ModelConfig* p = config_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelStatus.config)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelConfig*>(
      &::nvidia::inferenceserver::_ModelConfig_default_instance_);
}
inline ::nvidia::inferenceserver::ModelConfig* ModelStatus::release_config() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelStatus.config)
  
  ::nvidia::inferenceserver::ModelConfig* temp = config_;
  config_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelConfig* ModelStatus::mutable_config() {
  
  if (config_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig>(GetArenaNoVirtual());
    config_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelStatus.config)
  return config_;
}
inline void ModelStatus::set_allocated_config(::nvidia::inferenceserver::ModelConfig* config) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(config_);
  }
  if (config) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      config = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, config, submessage_arena);
    }
    
  } else {
    
  }
  config_ = config;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelStatus.config)
}

// map<int64, .nvidia.inferenceserver.ModelVersionStatus> version_status = 2;
inline int ModelStatus::version_status_size() const {
  return version_status_.size();
}
inline void ModelStatus::clear_version_status() {
  version_status_.Clear();
}
inline const ::google::protobuf::Map< ::google::protobuf::int64, ::nvidia::inferenceserver::ModelVersionStatus >&
ModelStatus::version_status() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelStatus.version_status)
  return version_status_.GetMap();
}
inline ::google::protobuf::Map< ::google::protobuf::int64, ::nvidia::inferenceserver::ModelVersionStatus >*
ModelStatus::mutable_version_status() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelStatus.version_status)
  return version_status_.MutableMap();
}

// -------------------------------------------------------------------

// SharedMemoryRegion_SystemSharedMemory

// string shared_memory_key = 1;
inline void SharedMemoryRegion_SystemSharedMemory::clear_shared_memory_key() {
  shared_memory_key_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& SharedMemoryRegion_SystemSharedMemory::shared_memory_key() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
  return shared_memory_key_.GetNoArena();
}
inline void SharedMemoryRegion_SystemSharedMemory::set_shared_memory_key(const ::std::string& value) {
  
  shared_memory_key_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
}
#if LANG_CXX11
inline void SharedMemoryRegion_SystemSharedMemory::set_shared_memory_key(::std::string&& value) {
  
  shared_memory_key_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
}
#endif
inline void SharedMemoryRegion_SystemSharedMemory::set_shared_memory_key(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  shared_memory_key_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
}
inline void SharedMemoryRegion_SystemSharedMemory::set_shared_memory_key(const char* value, size_t size) {
  
  shared_memory_key_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
}
inline ::std::string* SharedMemoryRegion_SystemSharedMemory::mutable_shared_memory_key() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
  return shared_memory_key_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* SharedMemoryRegion_SystemSharedMemory::release_shared_memory_key() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
  
  return shared_memory_key_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void SharedMemoryRegion_SystemSharedMemory::set_allocated_shared_memory_key(::std::string* shared_memory_key) {
  if (shared_memory_key != NULL) {
    
  } else {
    
  }
  shared_memory_key_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), shared_memory_key);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.shared_memory_key)
}

// uint64 offset = 2;
inline void SharedMemoryRegion_SystemSharedMemory::clear_offset() {
  offset_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SharedMemoryRegion_SystemSharedMemory::offset() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.offset)
  return offset_;
}
inline void SharedMemoryRegion_SystemSharedMemory::set_offset(::google::protobuf::uint64 value) {
  
  offset_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory.offset)
}

// -------------------------------------------------------------------

// SharedMemoryRegion_CudaSharedMemory

// int64 device_id = 1;
inline void SharedMemoryRegion_CudaSharedMemory::clear_device_id() {
  device_id_ = GOOGLE_LONGLONG(0);
}
inline ::google::protobuf::int64 SharedMemoryRegion_CudaSharedMemory::device_id() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryRegion.CudaSharedMemory.device_id)
  return device_id_;
}
inline void SharedMemoryRegion_CudaSharedMemory::set_device_id(::google::protobuf::int64 value) {
  
  device_id_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.SharedMemoryRegion.CudaSharedMemory.device_id)
}

// -------------------------------------------------------------------

// SharedMemoryRegion

// string name = 1;
inline void SharedMemoryRegion::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& SharedMemoryRegion::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryRegion.name)
  return name_.GetNoArena();
}
inline void SharedMemoryRegion::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.SharedMemoryRegion.name)
}
#if LANG_CXX11
inline void SharedMemoryRegion::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.SharedMemoryRegion.name)
}
#endif
inline void SharedMemoryRegion::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.SharedMemoryRegion.name)
}
inline void SharedMemoryRegion::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.SharedMemoryRegion.name)
}
inline ::std::string* SharedMemoryRegion::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.SharedMemoryRegion.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* SharedMemoryRegion::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.SharedMemoryRegion.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void SharedMemoryRegion::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.SharedMemoryRegion.name)
}

// .nvidia.inferenceserver.SharedMemoryRegion.SystemSharedMemory system_shared_memory = 2;
inline bool SharedMemoryRegion::has_system_shared_memory() const {
  return shared_memory_types_case() == kSystemSharedMemory;
}
inline void SharedMemoryRegion::set_has_system_shared_memory() {
  _oneof_case_[0] = kSystemSharedMemory;
}
inline void SharedMemoryRegion::clear_system_shared_memory() {
  if (has_system_shared_memory()) {
    delete shared_memory_types_.system_shared_memory_;
    clear_has_shared_memory_types();
  }
}
inline const ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory& SharedMemoryRegion::_internal_system_shared_memory() const {
  return *shared_memory_types_.system_shared_memory_;
}
inline ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* SharedMemoryRegion::release_system_shared_memory() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.SharedMemoryRegion.system_shared_memory)
  if (has_system_shared_memory()) {
    clear_has_shared_memory_types();
      ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* temp = shared_memory_types_.system_shared_memory_;
    shared_memory_types_.system_shared_memory_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory& SharedMemoryRegion::system_shared_memory() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryRegion.system_shared_memory)
  return has_system_shared_memory()
      ? *shared_memory_types_.system_shared_memory_
      : *reinterpret_cast< ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory*>(&::nvidia::inferenceserver::_SharedMemoryRegion_SystemSharedMemory_default_instance_);
}
inline ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory* SharedMemoryRegion::mutable_system_shared_memory() {
  if (!has_system_shared_memory()) {
    clear_shared_memory_types();
    set_has_system_shared_memory();
    shared_memory_types_.system_shared_memory_ = CreateMaybeMessage< ::nvidia::inferenceserver::SharedMemoryRegion_SystemSharedMemory >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.SharedMemoryRegion.system_shared_memory)
  return shared_memory_types_.system_shared_memory_;
}

// .nvidia.inferenceserver.SharedMemoryRegion.CudaSharedMemory cuda_shared_memory = 3;
inline bool SharedMemoryRegion::has_cuda_shared_memory() const {
  return shared_memory_types_case() == kCudaSharedMemory;
}
inline void SharedMemoryRegion::set_has_cuda_shared_memory() {
  _oneof_case_[0] = kCudaSharedMemory;
}
inline void SharedMemoryRegion::clear_cuda_shared_memory() {
  if (has_cuda_shared_memory()) {
    delete shared_memory_types_.cuda_shared_memory_;
    clear_has_shared_memory_types();
  }
}
inline const ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory& SharedMemoryRegion::_internal_cuda_shared_memory() const {
  return *shared_memory_types_.cuda_shared_memory_;
}
inline ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* SharedMemoryRegion::release_cuda_shared_memory() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.SharedMemoryRegion.cuda_shared_memory)
  if (has_cuda_shared_memory()) {
    clear_has_shared_memory_types();
      ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* temp = shared_memory_types_.cuda_shared_memory_;
    shared_memory_types_.cuda_shared_memory_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory& SharedMemoryRegion::cuda_shared_memory() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryRegion.cuda_shared_memory)
  return has_cuda_shared_memory()
      ? *shared_memory_types_.cuda_shared_memory_
      : *reinterpret_cast< ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory*>(&::nvidia::inferenceserver::_SharedMemoryRegion_CudaSharedMemory_default_instance_);
}
inline ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory* SharedMemoryRegion::mutable_cuda_shared_memory() {
  if (!has_cuda_shared_memory()) {
    clear_shared_memory_types();
    set_has_cuda_shared_memory();
    shared_memory_types_.cuda_shared_memory_ = CreateMaybeMessage< ::nvidia::inferenceserver::SharedMemoryRegion_CudaSharedMemory >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.SharedMemoryRegion.cuda_shared_memory)
  return shared_memory_types_.cuda_shared_memory_;
}

// uint64 byte_size = 5;
inline void SharedMemoryRegion::clear_byte_size() {
  byte_size_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SharedMemoryRegion::byte_size() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryRegion.byte_size)
  return byte_size_;
}
inline void SharedMemoryRegion::set_byte_size(::google::protobuf::uint64 value) {
  
  byte_size_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.SharedMemoryRegion.byte_size)
}

inline bool SharedMemoryRegion::has_shared_memory_types() const {
  return shared_memory_types_case() != SHARED_MEMORY_TYPES_NOT_SET;
}
inline void SharedMemoryRegion::clear_has_shared_memory_types() {
  _oneof_case_[0] = SHARED_MEMORY_TYPES_NOT_SET;
}
inline SharedMemoryRegion::SharedMemoryTypesCase SharedMemoryRegion::shared_memory_types_case() const {
  return SharedMemoryRegion::SharedMemoryTypesCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ServerStatus

// string id = 1;
inline void ServerStatus::clear_id() {
  id_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ServerStatus::id() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.id)
  return id_.GetNoArena();
}
inline void ServerStatus::set_id(const ::std::string& value) {
  
  id_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ServerStatus.id)
}
#if LANG_CXX11
inline void ServerStatus::set_id(::std::string&& value) {
  
  id_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ServerStatus.id)
}
#endif
inline void ServerStatus::set_id(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  id_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ServerStatus.id)
}
inline void ServerStatus::set_id(const char* value, size_t size) {
  
  id_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ServerStatus.id)
}
inline ::std::string* ServerStatus::mutable_id() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ServerStatus.id)
  return id_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ServerStatus::release_id() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ServerStatus.id)
  
  return id_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ServerStatus::set_allocated_id(::std::string* id) {
  if (id != NULL) {
    
  } else {
    
  }
  id_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), id);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ServerStatus.id)
}

// string version = 2;
inline void ServerStatus::clear_version() {
  version_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ServerStatus::version() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.version)
  return version_.GetNoArena();
}
inline void ServerStatus::set_version(const ::std::string& value) {
  
  version_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ServerStatus.version)
}
#if LANG_CXX11
inline void ServerStatus::set_version(::std::string&& value) {
  
  version_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ServerStatus.version)
}
#endif
inline void ServerStatus::set_version(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  version_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ServerStatus.version)
}
inline void ServerStatus::set_version(const char* value, size_t size) {
  
  version_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ServerStatus.version)
}
inline ::std::string* ServerStatus::mutable_version() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ServerStatus.version)
  return version_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ServerStatus::release_version() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ServerStatus.version)
  
  return version_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ServerStatus::set_allocated_version(::std::string* version) {
  if (version != NULL) {
    
  } else {
    
  }
  version_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), version);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ServerStatus.version)
}

// .nvidia.inferenceserver.ServerReadyState ready_state = 7;
inline void ServerStatus::clear_ready_state() {
  ready_state_ = 0;
}
inline ::nvidia::inferenceserver::ServerReadyState ServerStatus::ready_state() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.ready_state)
  return static_cast< ::nvidia::inferenceserver::ServerReadyState >(ready_state_);
}
inline void ServerStatus::set_ready_state(::nvidia::inferenceserver::ServerReadyState value) {
  
  ready_state_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ServerStatus.ready_state)
}

// uint64 uptime_ns = 3;
inline void ServerStatus::clear_uptime_ns() {
  uptime_ns_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ServerStatus::uptime_ns() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.uptime_ns)
  return uptime_ns_;
}
inline void ServerStatus::set_uptime_ns(::google::protobuf::uint64 value) {
  
  uptime_ns_ = value;
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ServerStatus.uptime_ns)
}

// map<string, .nvidia.inferenceserver.ModelStatus> model_status = 4;
inline int ServerStatus::model_status_size() const {
  return model_status_.size();
}
inline void ServerStatus::clear_model_status() {
  model_status_.Clear();
}
inline const ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelStatus >&
ServerStatus::model_status() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ServerStatus.model_status)
  return model_status_.GetMap();
}
inline ::google::protobuf::Map< ::std::string, ::nvidia::inferenceserver::ModelStatus >*
ServerStatus::mutable_model_status() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ServerStatus.model_status)
  return model_status_.MutableMap();
}

// .nvidia.inferenceserver.StatusRequestStats status_stats = 5;
inline bool ServerStatus::has_status_stats() const {
  return this != internal_default_instance() && status_stats_ != NULL;
}
inline void ServerStatus::clear_status_stats() {
  if (GetArenaNoVirtual() == NULL && status_stats_ != NULL) {
    delete status_stats_;
  }
  status_stats_ = NULL;
}
inline const ::nvidia::inferenceserver::StatusRequestStats& ServerStatus::_internal_status_stats() const {
  return *status_stats_;
}
inline const ::nvidia::inferenceserver::StatusRequestStats& ServerStatus::status_stats() const {
  const ::nvidia::inferenceserver::StatusRequestStats* p = status_stats_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.status_stats)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::StatusRequestStats*>(
      &::nvidia::inferenceserver::_StatusRequestStats_default_instance_);
}
inline ::nvidia::inferenceserver::StatusRequestStats* ServerStatus::release_status_stats() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ServerStatus.status_stats)
  
  ::nvidia::inferenceserver::StatusRequestStats* temp = status_stats_;
  status_stats_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::StatusRequestStats* ServerStatus::mutable_status_stats() {
  
  if (status_stats_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::StatusRequestStats>(GetArenaNoVirtual());
    status_stats_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ServerStatus.status_stats)
  return status_stats_;
}
inline void ServerStatus::set_allocated_status_stats(::nvidia::inferenceserver::StatusRequestStats* status_stats) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete status_stats_;
  }
  if (status_stats) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      status_stats = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, status_stats, submessage_arena);
    }
    
  } else {
    
  }
  status_stats_ = status_stats;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ServerStatus.status_stats)
}

// .nvidia.inferenceserver.HealthRequestStats health_stats = 8;
inline bool ServerStatus::has_health_stats() const {
  return this != internal_default_instance() && health_stats_ != NULL;
}
inline void ServerStatus::clear_health_stats() {
  if (GetArenaNoVirtual() == NULL && health_stats_ != NULL) {
    delete health_stats_;
  }
  health_stats_ = NULL;
}
inline const ::nvidia::inferenceserver::HealthRequestStats& ServerStatus::_internal_health_stats() const {
  return *health_stats_;
}
inline const ::nvidia::inferenceserver::HealthRequestStats& ServerStatus::health_stats() const {
  const ::nvidia::inferenceserver::HealthRequestStats* p = health_stats_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.health_stats)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::HealthRequestStats*>(
      &::nvidia::inferenceserver::_HealthRequestStats_default_instance_);
}
inline ::nvidia::inferenceserver::HealthRequestStats* ServerStatus::release_health_stats() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ServerStatus.health_stats)
  
  ::nvidia::inferenceserver::HealthRequestStats* temp = health_stats_;
  health_stats_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::HealthRequestStats* ServerStatus::mutable_health_stats() {
  
  if (health_stats_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::HealthRequestStats>(GetArenaNoVirtual());
    health_stats_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ServerStatus.health_stats)
  return health_stats_;
}
inline void ServerStatus::set_allocated_health_stats(::nvidia::inferenceserver::HealthRequestStats* health_stats) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete health_stats_;
  }
  if (health_stats) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      health_stats = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, health_stats, submessage_arena);
    }
    
  } else {
    
  }
  health_stats_ = health_stats;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ServerStatus.health_stats)
}

// .nvidia.inferenceserver.ModelControlRequestStats model_control_stats = 9;
inline bool ServerStatus::has_model_control_stats() const {
  return this != internal_default_instance() && model_control_stats_ != NULL;
}
inline void ServerStatus::clear_model_control_stats() {
  if (GetArenaNoVirtual() == NULL && model_control_stats_ != NULL) {
    delete model_control_stats_;
  }
  model_control_stats_ = NULL;
}
inline const ::nvidia::inferenceserver::ModelControlRequestStats& ServerStatus::_internal_model_control_stats() const {
  return *model_control_stats_;
}
inline const ::nvidia::inferenceserver::ModelControlRequestStats& ServerStatus::model_control_stats() const {
  const ::nvidia::inferenceserver::ModelControlRequestStats* p = model_control_stats_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.model_control_stats)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::ModelControlRequestStats*>(
      &::nvidia::inferenceserver::_ModelControlRequestStats_default_instance_);
}
inline ::nvidia::inferenceserver::ModelControlRequestStats* ServerStatus::release_model_control_stats() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ServerStatus.model_control_stats)
  
  ::nvidia::inferenceserver::ModelControlRequestStats* temp = model_control_stats_;
  model_control_stats_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::ModelControlRequestStats* ServerStatus::mutable_model_control_stats() {
  
  if (model_control_stats_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelControlRequestStats>(GetArenaNoVirtual());
    model_control_stats_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ServerStatus.model_control_stats)
  return model_control_stats_;
}
inline void ServerStatus::set_allocated_model_control_stats(::nvidia::inferenceserver::ModelControlRequestStats* model_control_stats) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_control_stats_;
  }
  if (model_control_stats) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      model_control_stats = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, model_control_stats, submessage_arena);
    }
    
  } else {
    
  }
  model_control_stats_ = model_control_stats;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ServerStatus.model_control_stats)
}

// .nvidia.inferenceserver.SharedMemoryControlRequestStats shm_control_stats = 10;
inline bool ServerStatus::has_shm_control_stats() const {
  return this != internal_default_instance() && shm_control_stats_ != NULL;
}
inline void ServerStatus::clear_shm_control_stats() {
  if (GetArenaNoVirtual() == NULL && shm_control_stats_ != NULL) {
    delete shm_control_stats_;
  }
  shm_control_stats_ = NULL;
}
inline const ::nvidia::inferenceserver::SharedMemoryControlRequestStats& ServerStatus::_internal_shm_control_stats() const {
  return *shm_control_stats_;
}
inline const ::nvidia::inferenceserver::SharedMemoryControlRequestStats& ServerStatus::shm_control_stats() const {
  const ::nvidia::inferenceserver::SharedMemoryControlRequestStats* p = shm_control_stats_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.shm_control_stats)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::SharedMemoryControlRequestStats*>(
      &::nvidia::inferenceserver::_SharedMemoryControlRequestStats_default_instance_);
}
inline ::nvidia::inferenceserver::SharedMemoryControlRequestStats* ServerStatus::release_shm_control_stats() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ServerStatus.shm_control_stats)
  
  ::nvidia::inferenceserver::SharedMemoryControlRequestStats* temp = shm_control_stats_;
  shm_control_stats_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::SharedMemoryControlRequestStats* ServerStatus::mutable_shm_control_stats() {
  
  if (shm_control_stats_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::SharedMemoryControlRequestStats>(GetArenaNoVirtual());
    shm_control_stats_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ServerStatus.shm_control_stats)
  return shm_control_stats_;
}
inline void ServerStatus::set_allocated_shm_control_stats(::nvidia::inferenceserver::SharedMemoryControlRequestStats* shm_control_stats) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete shm_control_stats_;
  }
  if (shm_control_stats) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      shm_control_stats = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, shm_control_stats, submessage_arena);
    }
    
  } else {
    
  }
  shm_control_stats_ = shm_control_stats;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ServerStatus.shm_control_stats)
}

// .nvidia.inferenceserver.RepositoryRequestStats repository_stats = 11;
inline bool ServerStatus::has_repository_stats() const {
  return this != internal_default_instance() && repository_stats_ != NULL;
}
inline void ServerStatus::clear_repository_stats() {
  if (GetArenaNoVirtual() == NULL && repository_stats_ != NULL) {
    delete repository_stats_;
  }
  repository_stats_ = NULL;
}
inline const ::nvidia::inferenceserver::RepositoryRequestStats& ServerStatus::_internal_repository_stats() const {
  return *repository_stats_;
}
inline const ::nvidia::inferenceserver::RepositoryRequestStats& ServerStatus::repository_stats() const {
  const ::nvidia::inferenceserver::RepositoryRequestStats* p = repository_stats_;
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ServerStatus.repository_stats)
  return p != NULL ? *p : *reinterpret_cast<const ::nvidia::inferenceserver::RepositoryRequestStats*>(
      &::nvidia::inferenceserver::_RepositoryRequestStats_default_instance_);
}
inline ::nvidia::inferenceserver::RepositoryRequestStats* ServerStatus::release_repository_stats() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ServerStatus.repository_stats)
  
  ::nvidia::inferenceserver::RepositoryRequestStats* temp = repository_stats_;
  repository_stats_ = NULL;
  return temp;
}
inline ::nvidia::inferenceserver::RepositoryRequestStats* ServerStatus::mutable_repository_stats() {
  
  if (repository_stats_ == NULL) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::RepositoryRequestStats>(GetArenaNoVirtual());
    repository_stats_ = p;
  }
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ServerStatus.repository_stats)
  return repository_stats_;
}
inline void ServerStatus::set_allocated_repository_stats(::nvidia::inferenceserver::RepositoryRequestStats* repository_stats) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete repository_stats_;
  }
  if (repository_stats) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      repository_stats = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, repository_stats, submessage_arena);
    }
    
  } else {
    
  }
  repository_stats_ = repository_stats;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ServerStatus.repository_stats)
}

// -------------------------------------------------------------------

// SharedMemoryStatus

// repeated .nvidia.inferenceserver.SharedMemoryRegion shared_memory_region = 2;
inline int SharedMemoryStatus::shared_memory_region_size() const {
  return shared_memory_region_.size();
}
inline void SharedMemoryStatus::clear_shared_memory_region() {
  shared_memory_region_.Clear();
}
inline ::nvidia::inferenceserver::SharedMemoryRegion* SharedMemoryStatus::mutable_shared_memory_region(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.SharedMemoryStatus.shared_memory_region)
  return shared_memory_region_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::SharedMemoryRegion >*
SharedMemoryStatus::mutable_shared_memory_region() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.SharedMemoryStatus.shared_memory_region)
  return &shared_memory_region_;
}
inline const ::nvidia::inferenceserver::SharedMemoryRegion& SharedMemoryStatus::shared_memory_region(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.SharedMemoryStatus.shared_memory_region)
  return shared_memory_region_.Get(index);
}
inline ::nvidia::inferenceserver::SharedMemoryRegion* SharedMemoryStatus::add_shared_memory_region() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.SharedMemoryStatus.shared_memory_region)
  return shared_memory_region_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::SharedMemoryRegion >&
SharedMemoryStatus::shared_memory_region() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.SharedMemoryStatus.shared_memory_region)
  return shared_memory_region_;
}

// -------------------------------------------------------------------

// ModelRepositoryIndex_ModelEntry

// string name = 1;
inline void ModelRepositoryIndex_ModelEntry::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& ModelRepositoryIndex_ModelEntry::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
  return name_.GetNoArena();
}
inline void ModelRepositoryIndex_ModelEntry::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
}
#if LANG_CXX11
inline void ModelRepositoryIndex_ModelEntry::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
}
#endif
inline void ModelRepositoryIndex_ModelEntry::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
}
inline void ModelRepositoryIndex_ModelEntry::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
}
inline ::std::string* ModelRepositoryIndex_ModelEntry::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ModelRepositoryIndex_ModelEntry::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ModelRepositoryIndex_ModelEntry::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry.name)
}

// -------------------------------------------------------------------

// ModelRepositoryIndex

// repeated .nvidia.inferenceserver.ModelRepositoryIndex.ModelEntry models = 1;
inline int ModelRepositoryIndex::models_size() const {
  return models_.size();
}
inline void ModelRepositoryIndex::clear_models() {
  models_.Clear();
}
inline ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry* ModelRepositoryIndex::mutable_models(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelRepositoryIndex.models)
  return models_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry >*
ModelRepositoryIndex::mutable_models() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelRepositoryIndex.models)
  return &models_;
}
inline const ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry& ModelRepositoryIndex::models(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelRepositoryIndex.models)
  return models_.Get(index);
}
inline ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry* ModelRepositoryIndex::add_models() {
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelRepositoryIndex.models)
  return models_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::nvidia::inferenceserver::ModelRepositoryIndex_ModelEntry >&
ModelRepositoryIndex::models() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelRepositoryIndex.models)
  return models_;
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace inferenceserver
}  // namespace nvidia

namespace google {
namespace protobuf {

template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelReadyState> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelReadyState>() {
  return ::nvidia::inferenceserver::ModelReadyState_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::ServerReadyState> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ServerReadyState>() {
  return ::nvidia::inferenceserver::ServerReadyState_descriptor();
}

}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_INCLUDED_server_5fstatus_2eproto
